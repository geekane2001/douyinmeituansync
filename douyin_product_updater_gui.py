import requests
import json
import os
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import threading
import time
import traceback
import pandas as pd
from openai import OpenAI
import hashlib
import re
import base64
from PIL import Image
import boto3
from botocore.client import Config
from io import BytesIO
import datetime
import logging

# --- å…¨å±€æ—¥å¿—é…ç½® ---
log_file_path = 'update_products_log.txt'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path, encoding='utf-8'),
        # logging.StreamHandler() # å¦‚æœä¹Ÿæƒ³åœ¨æ§åˆ¶å°çœ‹åˆ°æ—¥å¿—ï¼Œå¯ä»¥å–æ¶ˆè¿™è¡Œæ³¨é‡Š
    ]
)

# --- 1. é…ç½®ä¿¡æ¯ ---

# Cloudflare R2 é…ç½®
CLOUDFLARE_ACCOUNT_ID = "67a7569d0cd89aafb7499f3cf3bc9f73"
CLOUDFLARE_R2_ACCESS_KEY_ID = "6684b2a5b8f947ba4f6f3ba943d22439"
CLOUDFLARE_R2_SECRET_ACCESS_KEY = "bd3dce5ac2df30ae34377c9ca5af26fd845abe5fa6ea179ec6810552856ca27f"
R2_BUCKET_NAME = "0926taocantoutu"
R2_ENDPOINT_URL = f"https://{CLOUDFLARE_ACCOUNT_ID}.r2.cloudflarestorage.com"
R2_PUBLIC_URL_PREFIX = "https://pub-c92931353257460eb0beccbf59ef2ad0.r2.dev"

# ModelScope LLM (DeepSeek) é…ç½®
MS_BASE_URL = 'https://api-inference.modelscope.cn/v1'
MS_API_KEY = 'ms-871a8344-b18d-4fb5-b96e-d4123fbbb0f0'
LLM_MODEL_ID = 'deepseek-ai/DeepSeek-V3.2-Exp'
VISION_MODEL_IDS = [
    'Qwen/Qwen3-VL-8B-Instruct',
    'Qwen/Qwen3-VL-235B-A22B-Instruct',
    'Qwen/Qwen3-VL-30B-A3B-Instruct'
]

# LLM Client with timeout
try:
    llm_client = OpenAI(base_url=MS_BASE_URL, api_key=MS_API_KEY, timeout=30.0)
except Exception as e:
    llm_client = None
    print(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")

# æŠ–éŸ³å¼€æ”¾å¹³å°å¯†é’¥
CLIENT_KEY = "awbeykzyos7kbidv"
CLIENT_SECRET = "4575440b156ecbe144284e4f69d284a2"
DOUYIN_ACCOUNT_ID = "7241078611527075855"

# æŠ–éŸ³ç½‘é¡µç«¯é…ç½®ï¼ˆç”¨äºé‡åˆ›æ¨¡å¼ï¼‰
DOUYIN_WEB_CSRF_TOKEN = "000100000001ae8a406b9344d0cc4e30ceaf542c505dbbabca5a3842c450a93e0787a4d2f8991880c8ea9d2d1372"
DOUYIN_ROOT_LIFE_ACCOUNT_ID = "7241078611527075855"  # æ ¹è´¦å·IDï¼ˆä¸DOUYIN_ACCOUNT_IDç›¸åŒï¼‰

# ä»æ–‡ä»¶è¯»å–Cookie
def load_cookie_from_file():
    """ä»cookie.txtæ–‡ä»¶è¯»å–Cookie"""
    cookie_file = os.path.join(os.path.dirname(__file__), 'cookie.txt')
    try:
        with open(cookie_file, 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
            if cookie:
                logging.info(f"æˆåŠŸä» {cookie_file} åŠ è½½Cookie")
                return cookie
    except FileNotFoundError:
        logging.warning(f"æœªæ‰¾åˆ°Cookieæ–‡ä»¶: {cookie_file}")
    except Exception as e:
        logging.error(f"è¯»å–Cookieæ–‡ä»¶å¤±è´¥: {e}")
    return ""

DOUYIN_WEB_COOKIE = load_cookie_from_file()

# é£ä¹¦å¤šç»´è¡¨æ ¼é…ç½®
FEISHU_APP_ID = "cli_a6672cae343ad00e"
FEISHU_APP_SECRET = "0J4SpfBMeIxJEOXDJMNbofMipRgwkMpV"
FEISHU_APP_TOKEN = "MslRbdwPca7P6qsqbqgcvpBGnRh"
FEISHU_TABLE_ID = "tbluVbrXLRUmfouv"

# --- 2. API å’Œ URL åœ°å€ ---
DOUYIN_TOKEN_URL = "https://open.douyin.com/oauth/client_token/"
DOUYIN_PRODUCT_QUERY_URL = "https://open.douyin.com/goodlife/v1/goods/product/online/query/"
DOUYIN_PRODUCT_GET_URL = "https://open.douyin.com/goodlife/v1/goods/product/online/get/"
DOUYIN_PRODUCT_SAVE_URL = "https://open.douyin.com/goodlife/v1/goods/product/save/"
DOUYIN_PRODUCT_OPERATE_URL = "https://open.douyin.com/goodlife/v1/goods/product/operate/"
FEISHU_TENANT_ACCESS_TOKEN_URL = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal/"
FEISHU_BITABLE_RECORDS_SEARCH_URL = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables/{FEISHU_TABLE_ID}/records/search"

# --- 3. åç«¯ä¸šåŠ¡é€»è¾‘ ---

def get_feishu_tenant_access_token(log_func):
    log_func("--- æ­£åœ¨è·å–é£ä¹¦ Access Token ---")
    try:
        response = requests.post(FEISHU_TENANT_ACCESS_TOKEN_URL, json={"app_id": FEISHU_APP_ID, "app_secret": FEISHU_APP_SECRET}, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("code") == 0:
            log_func("[Success] é£ä¹¦ Token è·å–æˆåŠŸ!")
            return data.get("tenant_access_token")
    except Exception as e:
        log_func(f"[Error] è·å–é£ä¹¦ Token å¤±è´¥: {e}")
    return None

def get_feishu_bitable_records(feishu_token, log_func):
    log_func("--- æ­£åœ¨ä»é£ä¹¦è·å–é—¨åº—åˆ—è¡¨ ---")
    headers = {"Authorization": f"Bearer {feishu_token}"}
    all_records = {}
    page_token = ""
    while True:
        params = {"page_size": 500, "page_token": page_token}
        try:
            response = requests.post(FEISHU_BITABLE_RECORDS_SEARCH_URL, headers=headers, params=params, json={"field_names": ["é—¨åº—åç§°", "é—¨åº—ID"]}, timeout=15)
            response.raise_for_status()
            data = response.json()
            if data.get("code") != 0:
                log_func(f"[Error] æŸ¥è¯¢é£ä¹¦è®°å½•å¤±è´¥: {data.get('msg')}")
                return {}
            items = data.get("data", {}).get("items", [])
            for item in items:
                fields = item.get("fields", {})
                store_name = fields.get("é—¨åº—åç§°", [{}])[0].get('text')
                store_id = fields.get("é—¨åº—ID", [{}])[0].get('text')
                if store_name and store_id: all_records[store_name] = store_id
            page_token = data.get("data", {}).get("page_token")
            if not data.get("data", {}).get("has_more", False): break
        except Exception as e:
            log_func(f"[Error] æŸ¥è¯¢é£ä¹¦è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    log_func(f"[Success] æˆåŠŸä»é£ä¹¦è·å–åˆ° {len(all_records)} ä¸ªé—¨åº—ã€‚")
    return all_records

def get_douyin_access_token(log_func):
    log_func("--- æ­£åœ¨è·å–æŠ–éŸ³ Access Token ---")
    try:
        response = requests.post(DOUYIN_TOKEN_URL, json={"grant_type": "client_credential", "client_key": CLIENT_KEY, "client_secret": CLIENT_SECRET}, timeout=10)
        response.raise_for_status()
        data = response.json().get("data", {})
        if data.get("error_code") == 0 and data.get("access_token"):
            log_func("[Success] æŠ–éŸ³ Token è·å–æˆåŠŸ!")
            return data["access_token"]
    except Exception as e:
        log_func(f"[Error] è·å–æŠ–éŸ³ Token å¤±è´¥: {e}")
    return None

def get_douyin_products_by_store(access_token, poi_id, log_func):
    log_func(f"--- æ­£åœ¨ä½¿ç”¨ POI ID: {poi_id} æŸ¥è¯¢æŠ–éŸ³å•†å“åˆ—è¡¨ ---")
    headers = {"Content-Type": "application/json", "access-token": access_token}
    params = {"account_id": str(DOUYIN_ACCOUNT_ID), "poi_ids": f'[{poi_id}]', "count": 50, "status": 1}
    try:
        response = requests.get(DOUYIN_PRODUCT_QUERY_URL, headers=headers, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()
        if data.get("data", {}).get("error_code") != 0: return []
        product_list = data.get("data", {}).get("products", [])
        detailed_products = []
        for p in product_list:
            product_info = p.get("product", {})
            sku_info = p.get("sku", {})
            if product_info and sku_info:
                detailed_products.append({
                    "id": product_info.get('product_id'),
                    "name": product_info.get('product_name'),
                    "price": f"{sku_info.get('actual_amount', 0) / 100:.2f}",
                    "origin_price": f"{sku_info.get('origin_amount', 0) / 100:.2f}"
                })
        log_func(f"[Success] æŸ¥è¯¢åˆ° {len(detailed_products)} ä¸ªåœ¨çº¿å•†å“ã€‚")
        return detailed_products
    except Exception as e:
        log_func(f"[Error] æŸ¥è¯¢æŠ–éŸ³å•†å“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    return []

def get_douyin_product_details(access_token, product_id, log_func):
    log_func(f"--- æ­£åœ¨è·å–å•†å“ '{product_id}' çš„è¯¦ç»†ä¿¡æ¯ ---")
    headers = {"Content-Type": "application/json", "access-token": access_token}
    params = {"account_id": DOUYIN_ACCOUNT_ID, "product_ids": [product_id]}
    try:
        response = requests.get(DOUYIN_PRODUCT_GET_URL, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        response_data = response.json()
        if response_data.get("data", {}).get("error_code") == 0:
            product_details = response_data.get("data", {}).get("product_onlines", [])
            if product_details:
                log_func(f"[Success] å•†å“ '{product_id}' è¯¦æƒ…è·å–æˆåŠŸã€‚")
                return product_details[0]
    except Exception as e:
        log_func(f"[Error] è·å–å•†å“ '{product_id}' è¯¦æƒ…æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
    return None

def operate_douyin_product(access_token, product_id, log_func, offline=True):
    op_type = 2 if offline else 1
    action_text = "ä¸‹æ¶" if offline else "ä¸Šæ¶"
    log_func(f"========== å¼€å§‹ {action_text} å•†å“ ID: {product_id} ==========")
    headers = {"Content-Type": "application/json", "access-token": access_token}
    payload = { "account_id": DOUYIN_ACCOUNT_ID, "product_id": product_id, "op_type": op_type }
    try:
        response = requests.post(DOUYIN_PRODUCT_OPERATE_URL, headers=headers, json=payload, timeout=20)
        response.raise_for_status()
        response_data = response.json()
        if response_data.get('data', {}).get('error_code') == 0:
            log_func(f"[SUCCESS] å•†å“ {product_id} {action_text}æˆåŠŸ!")
            return True, ""
        else:
            reason = response_data.get('data', {}).get('description', 'APIè¿”å›æœªçŸ¥é”™è¯¯')
            log_func(f"[FAILURE] å•†å“ {product_id} {action_text}å¤±è´¥: {reason}")
            return False, reason
    except Exception as e:
        log_func(f"å•†å“ {product_id} {action_text}æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return False, str(e)

# --- ç¾å›¢åŒæ­¥ç›¸å…³å‡½æ•° ---
def process_store_name_for_meituan(store_name, log_func):
    """å¤„ç†åº—åç”¨äºç¾å›¢æœç´¢ï¼ˆç§»é™¤'ç«æ½®ç©'ï¼‰"""
    cleaned_name = store_name.replace("ç«æ½®ç©", "").strip()
    log_func(f"å¤„ç†åº—å: '{store_name}' -> '{cleaned_name}'")
    return cleaned_name

def get_meituan_packages(store_name, city, log_func):
    """è·å–ç¾å›¢å¥—é¤ä¿¡æ¯"""
    from bs4 import BeautifulSoup
    import time
    import re
    
    url = f"https://i.meituan.com/s/{city}-{store_name}"
    log_func(f"--- æ­£åœ¨è¯·æ±‚ç¾å›¢URL: {url} ---")
    
    # ç¾å›¢Cookieæ¨¡æ¿
    current_timestamp_ms = int(time.time() * 1000)
    base_cookie = (
        f"__mta=176011805.1756208359328.{current_timestamp_ms-5000}.{current_timestamp_ms}.30; "
        "iuuid=BB0697D3630DED2F82ADB96105EC195EB173E4FFD90723B66428C9829840A7AA; "
        "_lxsdk_cuid=199985447cbc8-0682d08c20b0dd-4c657b58-1fa400-199985447cbc8; "
        "_lxsdk=BB0697D3630DED2F82ADB96105EC195EB173E4FFD90723B66428C9829840A7AA; "
        "uuid=15efb5c50ad74159b2cd.1759197288.1.0.0; "
        "webp=1; "
        "_hc.v=17500b8b-5eb7-fe17-d4e8-ffc997c5aeda.1762150863; "
        "token=AgE9Jw3iB0xS0K4Dvg5h7_SFKSHEFNG3l3ns5orAsvwiPjQSJEe4ONv8nXX8acfUcNWMJhCiWxrpXgAAAAD1LgAAJsMNtP1gv1zb-teZ_5_kWSrKWuemK26NdJ5W9PXcqThYpPwqaiFNJIXoQXyCW92I; "
        "userId=4976202507; "
        f"latlng=30.602421,104.09746,{current_timestamp_ms}; "
        f"_lxsdk_s=199869550cd-0fd-14f-716%7C%7C46"
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Host': 'i.meituan.com',
        'Cookie': base_cookie
    }
    
    proxies = {'http': 'http://127.0.0.1:10808', 'https': 'http://127.0.0.1:10808'}
    
    try:
        response = requests.get(url, headers=headers, proxies=proxies, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        log_func(f"è¯·æ±‚æˆåŠŸ! çŠ¶æ€ç : {response.status_code}")
        
        if "è®¿é—®å¼‚å¸¸" in response.text:
            log_func("[Error] ç¾å›¢é¡µé¢è®¿é—®å¼‚å¸¸ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookie")
            log_func(f"[Debug] é¡µé¢éƒ¨åˆ†å†…å®¹: {response.text[:500]}")
            return []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åº—å
        if store_name not in response.text:
            log_func(f"[Warning] é¡µé¢ä¸­æœªæ‰¾åˆ°åº—å'{store_name}'ï¼Œå¯èƒ½æœç´¢å¤±è´¥")
            log_func(f"[Debug] é¡µé¢éƒ¨åˆ†å†…å®¹: {response.text[:500]}")
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # æå–åº—åç”¨äºç¡®è®¤
        shop_name_tag = soup.find('span', class_='poiname')
        shop_name = shop_name_tag.text.strip() if shop_name_tag else "æœªçŸ¥åº—å"
        log_func(f"--- æˆåŠŸæå–åˆ°ã€{shop_name}ã€‘çš„ç¾å›¢å¥—é¤ä¿¡æ¯ ---")
        
        deal_items = soup.select('dl.bd-deal-list dd a.react')
        
        if not deal_items:
            log_func("[Warning] æœªæ‰¾åˆ°ç¾å›¢å¥—é¤ä¿¡æ¯ (CSSé€‰æ‹©å™¨: dl.bd-deal-list dd a.react)")
            log_func(f"[Debug] é¡µé¢å†…å®¹å‰1000å­—ç¬¦: {response.text[:1000]}")
            # ä¿å­˜å®Œæ•´HTMLç”¨äºè°ƒè¯•
            debug_file = f"meituan_debug_{int(time.time())}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            log_func(f"[Debug] å®Œæ•´HTMLå·²ä¿å­˜åˆ°: {debug_file}")
            return []
        
        log_func(f"æ‰¾åˆ° {len(deal_items)} ä¸ªå¥—é¤é¡¹")
        
        packages = []
        for idx, item in enumerate(deal_items):
            log_func(f"\n========== å¼€å§‹è§£æå¥—é¤ {idx+1} ==========")
            
            # æ‰“å°è¯¥å¥—é¤é¡¹çš„å®Œæ•´HTMLç»“æ„ï¼ˆå‰500å­—ç¬¦ï¼‰
            item_html = str(item)[:500]
            log_func(f"[HTMLç»“æ„] å¥—é¤{idx+1}çš„HTMLå‰500å­—ç¬¦:\n{item_html}")
            
            # æå–æ ‡é¢˜
            title_tag = item.find('div', class_='title')
            log_func(f"[æ­¥éª¤1-æ ‡é¢˜] title_tagæ˜¯å¦æ‰¾åˆ°: {title_tag is not None}")
            if title_tag:
                log_func(f"[æ­¥éª¤1-æ ‡é¢˜] title_tagå†…å®¹: {title_tag}")
                title = title_tag.text.strip()
                log_func(f"[æ­¥éª¤1-æ ‡é¢˜] æå–çš„æ ‡é¢˜: '{title}'")
            else:
                title = "æ— æ ‡é¢˜"
                log_func(f"[æ­¥éª¤1-æ ‡é¢˜] æœªæ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤å€¼: '{title}'")
            
            # æå–ç°ä»· - è¯¦ç»†åˆ†æ
            log_func(f"[æ­¥éª¤2-ç°ä»·] å¼€å§‹æŸ¥æ‰¾ç°ä»·...")
            price_tag = item.find('span', class_='strong')
            log_func(f"[æ­¥éª¤2-ç°ä»·] ä½¿ç”¨é€‰æ‹©å™¨'span.strong'æŸ¥æ‰¾ç»“æœ: {price_tag is not None}")
            
            if price_tag:
                log_func(f"[æ­¥éª¤2-ç°ä»·] æ‰¾åˆ°price_tag: {price_tag}")
                price_str = price_tag.text.strip()
                log_func(f"[æ­¥éª¤2-ç°ä»·] price_tag.text.strip() = '{price_str}'")
            else:
                # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                log_func(f"[æ­¥éª¤2-ç°ä»·] 'span.strong'æœªæ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
                
                # å°è¯•æ‰€æœ‰spanæ ‡ç­¾
                all_spans = item.find_all('span')
                log_func(f"[æ­¥éª¤2-ç°ä»·] è¯¥å¥—é¤é¡¹ä¸­å…±æœ‰ {len(all_spans)} ä¸ªspanæ ‡ç­¾")
                for span_idx, span in enumerate(all_spans):
                    log_func(f"[æ­¥éª¤2-ç°ä»·]   span[{span_idx}]: class={span.get('class')}, text='{span.text.strip()}'")
                
                price_str = "æ— ä»·æ ¼"
                log_func(f"[æ­¥éª¤2-ç°ä»·] æ‰€æœ‰é€‰æ‹©å™¨å‡æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼: '{price_str}'")
            
            # æå–åŸä»· - è¯¦ç»†åˆ†æ
            log_func(f"[æ­¥éª¤3-åŸä»·] å¼€å§‹æŸ¥æ‰¾åŸä»·...")
            original_price_tag = item.find('del')
            log_func(f"[æ­¥éª¤3-åŸä»·] ä½¿ç”¨é€‰æ‹©å™¨'del'æŸ¥æ‰¾ç»“æœ: {original_price_tag is not None}")
            
            if original_price_tag:
                log_func(f"[æ­¥éª¤3-åŸä»·] æ‰¾åˆ°original_price_tag: {original_price_tag}")
                original_price_str = original_price_tag.text.strip()
                log_func(f"[æ­¥éª¤3-åŸä»·] original_price_tag.text.strip() = '{original_price_str}'")
            else:
                log_func(f"[æ­¥éª¤3-åŸä»·] 'del'æ ‡ç­¾æœªæ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
                
                # å°è¯•sæ ‡ç­¾
                s_tag = item.find('s')
                log_func(f"[æ­¥éª¤3-åŸä»·] ä½¿ç”¨é€‰æ‹©å™¨'s'æŸ¥æ‰¾ç»“æœ: {s_tag is not None}")
                if s_tag:
                    original_price_str = s_tag.text.strip()
                    log_func(f"[æ­¥éª¤3-åŸä»·] ä»'s'æ ‡ç­¾æå–: '{original_price_str}'")
                else:
                    original_price_str = ""
                    log_func(f"[æ­¥éª¤3-åŸä»·] æ‰€æœ‰é€‰æ‹©å™¨å‡æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")
            
            log_func(f"[æ­¥éª¤4-æ±‡æ€»] åŸå§‹æå–ç»“æœ: title='{title}', price_str='{price_str}', original_price_str='{original_price_str}'")
            
            # è½¬æ¢ä¸ºæ•°å­—
            log_func(f"[æ­¥éª¤5-è½¬æ¢] å¼€å§‹å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—...")
            try:
                # æ¸…ç†ä»·æ ¼å­—ç¬¦ä¸²
                log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†ç°ä»·å­—ç¬¦ä¸²: '{price_str}'")
                price_clean = re.sub(r'[^\d.]', '', price_str)
                log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†åçš„ç°ä»·: '{price_clean}'")
                
                if price_clean:
                    price = float(price_clean)
                    log_func(f"[æ­¥éª¤5-è½¬æ¢] ç°ä»·è½¬æ¢æˆåŠŸ: {price}")
                else:
                    price = 0.0
                    log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†åä¸ºç©ºï¼Œç°ä»·è®¾ä¸º: {price}")
                
                if original_price_str:
                    log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†åŸä»·å­—ç¬¦ä¸²: '{original_price_str}'")
                    original_price_clean = re.sub(r'[^\d.]', '', original_price_str)
                    log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†åçš„åŸä»·: '{original_price_clean}'")
                    
                    if original_price_clean:
                        original_price = float(original_price_clean)
                        log_func(f"[æ­¥éª¤5-è½¬æ¢] åŸä»·è½¬æ¢æˆåŠŸ: {original_price}")
                    else:
                        original_price = price
                        log_func(f"[æ­¥éª¤5-è½¬æ¢] æ¸…ç†åä¸ºç©ºï¼ŒåŸä»·ä½¿ç”¨ç°ä»·: {original_price}")
                else:
                    original_price = price
                    log_func(f"[æ­¥éª¤5-è½¬æ¢] åŸä»·å­—ç¬¦ä¸²ä¸ºç©ºï¼Œä½¿ç”¨ç°ä»·: {original_price}")
                
                log_func(f"[æ­¥éª¤5-è½¬æ¢] âœ… è½¬æ¢å®Œæˆ: price={price}, original_price={original_price}")
                
            except (ValueError, AttributeError) as e:
                log_func(f"[æ­¥éª¤5-è½¬æ¢] âŒ è½¬æ¢å¤±è´¥: {e}")
                log_func(f"[æ­¥éª¤5-è½¬æ¢] å¤±è´¥æ—¶çš„åŸå§‹å€¼: price_str='{price_str}', original_price_str='{original_price_str}'")
                price = 0.0
                original_price = 0.0
                log_func(f"[æ­¥éª¤5-è½¬æ¢] è®¾ç½®é»˜è®¤å€¼: price={price}, original_price={original_price}")
            
            packages.append({
                "title": title,
                "price": price,
                "original_price": original_price
            })
            log_func(f"[æ­¥éª¤6-å®Œæˆ] âœ… å¥—é¤{idx+1}è§£æå®Œæˆ: {title} | ç°ä»·: {price}å…ƒ | åŸä»·: {original_price}å…ƒ")
            log_func(f"========== å¥—é¤ {idx+1} è§£æç»“æŸ ==========\n")
        
        log_func(f"æˆåŠŸè·å– {len(packages)} ä¸ªç¾å›¢å¥—é¤")
        return packages
        
    except Exception as e:
        log_func(f"[Error] è·å–ç¾å›¢å¥—é¤å¤±è´¥: {e}")
        import traceback
        log_func(f"[Debug] è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def match_packages_smart(douyin_packages, meituan_packages, log_func):
    """æ™ºèƒ½åŒ¹é…æŠ–éŸ³å’Œç¾å›¢å¥—é¤ï¼ˆåŸºäºç°ä»·å’ŒåŸä»·çš„ç›¸ä¼¼åº¦ï¼‰"""
    log_func("\n" + "="*80)
    log_func("å¼€å§‹æ™ºèƒ½åŒ¹é…æŠ–éŸ³å’Œç¾å›¢å¥—é¤")
    log_func("="*80)
    
    # æ‰“å°è¾“å…¥æ•°æ®æ¦‚è§ˆ
    log_func(f"\n[è¾“å…¥æ•°æ®] æŠ–éŸ³å¥—é¤æ•°é‡: {len(douyin_packages)}")
    log_func(f"[è¾“å…¥æ•°æ®] ç¾å›¢å¥—é¤æ•°é‡: {len(meituan_packages)}")
    
    # è¯¦ç»†æ‰“å°æŠ–éŸ³å¥—é¤ä¿¡æ¯
    log_func("\n--- æŠ–éŸ³å¥—é¤è¯¦ç»†ä¿¡æ¯ ---")
    for idx, dy_pkg in enumerate(douyin_packages):
        log_func(f"æŠ–éŸ³å¥—é¤[{idx+1}]:")
        log_func(f"  ID: {dy_pkg.get('id')}")
        log_func(f"  åç§°: {dy_pkg.get('name')}")
        log_func(f"  ç°ä»·(price): {dy_pkg.get('price')} (ç±»å‹: {type(dy_pkg.get('price'))})")
        log_func(f"  åŸä»·(origin_price): {dy_pkg.get('origin_price')} (ç±»å‹: {type(dy_pkg.get('origin_price'))})")
    
    # è¯¦ç»†æ‰“å°ç¾å›¢å¥—é¤ä¿¡æ¯
    log_func("\n--- ç¾å›¢å¥—é¤è¯¦ç»†ä¿¡æ¯ ---")
    for idx, mt_pkg in enumerate(meituan_packages):
        log_func(f"ç¾å›¢å¥—é¤[{idx+1}]:")
        log_func(f"  æ ‡é¢˜: {mt_pkg.get('title')}")
        log_func(f"  ç°ä»·(price): {mt_pkg.get('price')} (ç±»å‹: {type(mt_pkg.get('price'))})")
        log_func(f"  åŸä»·(original_price): {mt_pkg.get('original_price')} (ç±»å‹: {type(mt_pkg.get('original_price'))})")
    
    matches = []  # åŒ¹é…æˆåŠŸçš„
    meituan_only = []  # ç¾å›¢ç‹¬æœ‰çš„ï¼ˆéœ€è¦æ–°å»ºï¼‰
    douyin_only = []  # æŠ–éŸ³ç‹¬æœ‰çš„ï¼ˆéœ€è¦ä¸‹æ¶ï¼Œé™¤äº†ç‰¹æ®Šå¥—é¤ï¼‰
    
    # ç‰¹æ®Šå¥—é¤åˆ—è¡¨ï¼ˆä¸ä¸‹æ¶ï¼‰
    special_packages = ["ã€æ–°è€ä¼šå‘˜ã€‘28å¾—30ç½‘è´¹", "28å¾—30ç½‘è´¹"]
    
    # ä¸ºæ¯ä¸ªç¾å›¢å¥—é¤å¯»æ‰¾åŒ¹é…çš„æŠ–éŸ³å¥—é¤
    matched_douyin_ids = set()
    
    log_func("\n" + "-"*80)
    log_func("ã€ç¬¬ä¸€è½®åŒ¹é…ã€‘ä¼˜å…ˆåŒ¹é…ä»·æ ¼å®Œå…¨ç›¸åŒçš„å¥—é¤")
    log_func("-"*80)
    
    # ç¬¬ä¸€è½®ï¼šåªåŒ¹é…ä»·æ ¼å®Œå…¨ç›¸åŒçš„å¥—é¤ï¼ˆç°ä»·å’ŒåŸä»·å·®å¼‚éƒ½<0.01å…ƒï¼‰
    for mt_idx, mt_pkg in enumerate(meituan_packages):
        mt_price = mt_pkg['price']
        mt_orig_price = mt_pkg['original_price']
        
        log_func(f"\n>>> ç¬¬ä¸€è½® - ç¾å›¢å¥—é¤ [{mt_idx+1}/{len(meituan_packages)}]: {mt_pkg['title']} <<<")
        log_func(f"  ç°ä»·: {mt_price}, åŸä»·: {mt_orig_price}")
        
        best_match = None
        best_score = 0
        
        for dy_idx, dy_pkg in enumerate(douyin_packages):
            if dy_pkg['id'] in matched_douyin_ids:
                continue
            
            try:
                dy_price = float(dy_pkg['price'])
                dy_orig_price = float(dy_pkg['origin_price'])
            except Exception as e:
                log_func(f"  âŒ æŠ–éŸ³å¥—é¤ä»·æ ¼è½¬æ¢å¤±è´¥: {dy_pkg['name']}, é”™è¯¯: {e}")
                continue
            
            # è®¡ç®—ä»·æ ¼å·®å¼‚
            price_diff = abs(mt_price - dy_price)
            orig_price_diff = abs(mt_orig_price - dy_orig_price)
            
            # ç¬¬ä¸€è½®åªåŒ¹é…ä»·æ ¼å®Œå…¨ç›¸åŒçš„
            if price_diff < 0.01 and orig_price_diff < 0.01:
                score = 100  # å®Œå…¨åŒ¹é…å¾—æ»¡åˆ†
                log_func(f"  âœ“ æ‰¾åˆ°å®Œå…¨åŒ¹é…: {dy_pkg['name']} (ç°ä»·:{dy_price}, åŸä»·:{dy_orig_price})")
                
                if score > best_score:
                    best_score = score
                    best_match = dy_pkg
        
        if best_match:
            matches.append({
                "douyin": best_match,
                "meituan": mt_pkg,
                "action": "keep"  # ä»·æ ¼å®Œå…¨ç›¸åŒï¼Œä¿æŒåŸæ ·
            })
            matched_douyin_ids.add(best_match['id'])
            log_func(f"  âœ… ç¬¬ä¸€è½®åŒ¹é…æˆåŠŸ - ä»·æ ¼å®Œå…¨ç›¸åŒï¼Œä¿æŒåŸæ ·")
            log_func(f"     æŠ–éŸ³: {best_match['name']}")
            log_func(f"     ç¾å›¢: {mt_pkg['title']}")
    
    log_func("\n" + "-"*80)
    log_func(f"ã€ç¬¬ä¸€è½®åŒ¹é…å®Œæˆã€‘æˆåŠŸåŒ¹é… {len(matches)} ä¸ªå®Œå…¨ç›¸åŒçš„å¥—é¤")
    log_func("-"*80)
    
    log_func("\n" + "-"*80)
    log_func("ã€ç¬¬äºŒè½®åŒ¹é…ã€‘åŒ¹é…ä»·æ ¼ç›¸ä¼¼çš„å¥—é¤")
    log_func("-"*80)
    
    # ç¬¬äºŒè½®ï¼šåŒ¹é…ä»·æ ¼ç›¸ä¼¼çš„å¥—é¤ï¼ˆç°ä»·å·®å¼‚â‰¤2å…ƒä¸”åŸä»·å·®å¼‚â‰¤30å…ƒï¼‰
    for mt_idx, mt_pkg in enumerate(meituan_packages):
        # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¬¬ä¸€è½®åŒ¹é…
        already_matched = any(m['meituan']['title'] == mt_pkg['title'] for m in matches)
        if already_matched:
            log_func(f"\n>>> ç¬¬äºŒè½® - ç¾å›¢å¥—é¤ [{mt_idx+1}]: {mt_pkg['title']} - å·²åœ¨ç¬¬ä¸€è½®åŒ¹é…ï¼Œè·³è¿‡ <<<")
            continue
        
        mt_price = mt_pkg['price']
        mt_orig_price = mt_pkg['original_price']
        
        log_func(f"\n>>> ç¬¬äºŒè½® - ç¾å›¢å¥—é¤ [{mt_idx+1}/{len(meituan_packages)}]: {mt_pkg['title']} <<<")
        log_func(f"  ç°ä»·: {mt_price}, åŸä»·: {mt_orig_price}")
        
        best_match = None
        best_score = 0
        
        for dy_idx, dy_pkg in enumerate(douyin_packages):
            if dy_pkg['id'] in matched_douyin_ids:
                continue
            
            log_func(f"  æ¯”å¯¹æŠ–éŸ³å¥—é¤: {dy_pkg['name']}")
            
            try:
                dy_price = float(dy_pkg['price'])
                dy_orig_price = float(dy_pkg['origin_price'])
                log_func(f"    æŠ–éŸ³ç°ä»·: {dy_price}, æŠ–éŸ³åŸä»·: {dy_orig_price}")
            except Exception as e:
                log_func(f"    âŒ ä»·æ ¼è½¬æ¢å¤±è´¥: {e}")
                continue
            
            # è®¡ç®—ä»·æ ¼å·®å¼‚
            price_diff = abs(mt_price - dy_price)
            orig_price_diff = abs(mt_orig_price - dy_orig_price)
            
            log_func(f"    ç°ä»·å·®å¼‚: {price_diff:.2f}å…ƒ, åŸä»·å·®å¼‚: {orig_price_diff:.2f}å…ƒ")
            
            # ç¬¬äºŒè½®åŒ¹é…é€»è¾‘ï¼šä»·æ ¼ç›¸ä¼¼
            is_match = False
            match_reason = ""
            
            if price_diff <= 0.5:
                # ç°ä»·å‡ ä¹ç›¸åŒ
                is_match = True
                match_reason = "ç°ä»·å‡ ä¹ç›¸åŒ"
                score = 100 - (price_diff * 20) - (orig_price_diff * 0.5)
            elif price_diff <= 2.0 and orig_price_diff <= 30.0:
                # ç°ä»·å’ŒåŸä»·éƒ½åœ¨å®¹å¿èŒƒå›´å†…
                is_match = True
                match_reason = "ç°ä»·å’ŒåŸä»·éƒ½ç›¸ä¼¼"
                score = 100 - (price_diff * 10) - (orig_price_diff * 1)
            
            if is_match:
                log_func(f"    âœ“ ç¬¦åˆåŒ¹é…æ¡ä»¶ï¼åŸå› : {match_reason}, åˆ†æ•°: {score:.1f}")
                
                if score > best_score:
                    best_score = score
                    best_match = dy_pkg
                    log_func(f"    â˜… å½“å‰æœ€ä½³åŒ¹é…ï¼")
            else:
                log_func(f"    âœ— ä¸ç¬¦åˆåŒ¹é…æ¡ä»¶")
        
        if best_match:
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°ä»·æ ¼
            dy_price = float(best_match['price'])
            dy_orig_price = float(best_match['origin_price'])
            price_diff = abs(mt_price - dy_price)
            orig_price_diff = abs(mt_orig_price - dy_orig_price)
            
            # åˆ¤æ–­æ“ä½œç±»å‹
            if price_diff <= 2.0 and orig_price_diff < 0.01:
                action = "keep"  # ç°ä»·å·®å¼‚å°ä¸”åŸä»·ç›¸åŒï¼Œä¿æŒåŸæ ·
                log_func(f"  âœ… ç¬¬äºŒè½®åŒ¹é…æˆåŠŸ - ç°ä»·å·®å¼‚å°ï¼ˆ{price_diff:.1f}å…ƒï¼‰ä¸”åŸä»·ç›¸åŒï¼Œä¿æŒåŸæ ·")
            else:
                action = "update"  # éœ€è¦æ›´æ–°ä»·æ ¼
                log_func(f"  âœ… ç¬¬äºŒè½®åŒ¹é…æˆåŠŸ - ä»·æ ¼å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦æ›´æ–°")
            
            matches.append({
                "douyin": best_match,
                "meituan": mt_pkg,
                "action": action
            })
            matched_douyin_ids.add(best_match['id'])
            log_func(f"     æŠ–éŸ³: {best_match['name']} (ç°ä»·:{dy_price}, åŸä»·:{dy_orig_price})")
            log_func(f"     ç¾å›¢: {mt_pkg['title']} (ç°ä»·:{mt_price}, åŸä»·:{mt_orig_price})")
            log_func(f"     åŒ¹é…åˆ†æ•°: {best_score:.1f}")
            log_func(f"     æ“ä½œ: {action}")
        else:
            meituan_only.append(mt_pkg)
            log_func(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æŠ–éŸ³å¥—é¤")
            log_func(f"     ç¾å›¢å¥—é¤: {mt_pkg['title']} (ç°ä»·:{mt_price}, åŸä»·:{mt_orig_price})")
            log_func(f"     â†’ æ ‡è®°ä¸ºéœ€è¦æ–°å»º")
    
    log_func("\n" + "-"*80)
    log_func(f"ã€ç¬¬äºŒè½®åŒ¹é…å®Œæˆã€‘")
    log_func("-"*80)
    
    # æ‰¾å‡ºæŠ–éŸ³ç‹¬æœ‰çš„å¥—é¤
    log_func("\n" + "-"*80)
    log_func("æ£€æŸ¥æŠ–éŸ³ç‹¬æœ‰å¥—é¤")
    log_func("-"*80)
    
    for dy_pkg in douyin_packages:
        if dy_pkg['id'] not in matched_douyin_ids:
            if dy_pkg['name'] not in special_packages:
                douyin_only.append(dy_pkg)
                log_func(f"âš ï¸ æŠ–éŸ³ç‹¬æœ‰å¥—é¤ï¼ˆéœ€ä¸‹æ¶ï¼‰: {dy_pkg['name']}")
            else:
                log_func(f"ğŸ”’ ç‰¹æ®Šå¥—é¤ï¼ˆä¿ç•™ä¸ä¸‹æ¶ï¼‰: {dy_pkg['name']}")
    
    log_func("\n" + "="*80)
    log_func("åŒ¹é…ç»“æœæ±‡æ€»")
    log_func("="*80)
    
    # ç»Ÿè®¡ä¸åŒæ“ä½œç±»å‹çš„æ•°é‡
    keep_count = sum(1 for m in matches if m["action"] == "keep")
    update_count = sum(1 for m in matches if m["action"] == "update")
    
    log_func(f"âœ… æˆåŠŸåŒ¹é…: {len(matches)} ä¸ª")
    log_func(f"   - ä¿æŒåŸæ ·: {keep_count} ä¸ª")
    log_func(f"   - éœ€è¦æ›´æ–°: {update_count} ä¸ª")
    log_func(f"â• éœ€è¦æ–°å»º: {len(meituan_only)} ä¸ª")
    log_func(f"â– éœ€è¦ä¸‹æ¶: {len(douyin_only)} ä¸ª")
    log_func("="*80 + "\n")
    
    return {
        "matches": matches,
        "meituan_only": meituan_only,
        "douyin_only": douyin_only
    }

# --- ç½‘é¡µç«¯APIåˆ›å»ºå•†å“ï¼ˆç”¨äºé‡åˆ›æ¨¡å¼ï¼‰---
def _get_product_template_web(session, product_id, root_life_account_id, log_func):
    """ä»ç½‘é¡µç«¯è·å–å•†å“æ¨¡æ¿"""
    log_func(f"--- [ç½‘é¡µç«¯] æ­£åœ¨è·å–å•†å“æ¨¡æ¿ (ID: {product_id})... ---")
    url = "https://life.douyin.com/life/tobias/product/get/"
    params = {
        'product_type': '1',
        'category_id': '4007001',
        'scene': '2',
        'product_id': product_id,
        'list_tab': '9',
        'source': '1',
        'is_lite_req': 'false',
        'root_life_account_id': root_life_account_id
    }
    
    try:
        response = session.get(url, params=params, timeout=20)
        response.raise_for_status()
        result = response.json()
        
        if result.get('status_code') == 0 and result.get('product_detail'):
            log_func("âœ… æˆåŠŸè·å–å•†å“æ¨¡æ¿ï¼")
            return result['product_detail'], None
        else:
            return None, f"è·å–æ¨¡æ¿å¤±è´¥: {result.get('status_msg', 'æœªçŸ¥é”™è¯¯')}"
    except requests.exceptions.RequestException as e:
        return None, f"è·å–æ¨¡æ¿è¯·æ±‚å¤±è´¥: {e}"

def _build_web_product_payload_from_template(product_detail_template, new_data, log_func):
    """åŸºäºæ¨¡æ¿æ„å»ºç½‘é¡µç«¯å•†å“åˆ›å»ºpayloadï¼ˆå¤ç”¨å›¾ç‰‡ï¼‰"""
    log_func("--- [ç½‘é¡µç«¯] æ­£åœ¨åŸºäºæ¨¡æ¿æ„å»ºå•†å“è´Ÿè½½... ---")
    
    # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
    product_detail_template.pop('product_permission_list', None)
    
    if 'product' not in product_detail_template:
        return None
    
    product_object = product_detail_template['product']
    product_object.pop('product_id', None)  # ç§»é™¤product_idä»¥åˆ›å»ºæ–°å•†å“
    
    if 'comp_key_value_map' not in product_object:
        return None
    
    comp_map = product_object['comp_key_value_map']
    
    # æ›´æ–°å•†å“åç§°ã€ä»·æ ¼å’Œæ—¶é—´
    current_timestamp = int(time.time())
    comp_map['productName'] = new_data["å›¢è´­æ ‡é¢˜"]
    
    # æ›´æ–°å”®ä»·å’ŒåŸä»·
    actual_amount = int(new_data["å”®ä»·"] * 100)  # è½¬æ¢ä¸ºåˆ†
    origin_amount = int(new_data["åŸä»·"] * 100)  # è½¬æ¢ä¸ºåˆ†
    comp_map['actualAmount'] = str(actual_amount)
    comp_map['originAmount'] = str(origin_amount)
    
    sold_start_time = str(current_timestamp)
    sold_end_time = str(current_timestamp + 90 * 24 * 3600)
    comp_map['auto_renew-sold_end_time-sold_start_time'] = json.dumps({
        "soldStartTime": sold_start_time,
        "soldEndTime": sold_end_time,
        "autoRenew": True,
        "soldTimeType": 1
    })
    
    log_func(f"âœ… å•†å“åç§°å·²æ›´æ–°ä¸º: {comp_map['productName']}")
    log_func(f"âœ… å”®ä»·å·²æ›´æ–°ä¸º: {new_data['å”®ä»·']}å…ƒ (åŸä»·: {new_data['åŸä»·']}å…ƒ)")
    
    # å¼ºåˆ¶è®¾ç½®ä¸º"ä¸éœ€è¦"é¡¾å®¢ä¿¡æ¯
    comp_map['customer_reserved_info-real_name_info'] = '{"customerReservedInfo":{"allow":false},"realNameInfo":{"enable":false}}'
    log_func("âœ… å·²å¼ºåˆ¶å°† 'é¡¾å®¢ä¿¡æ¯è®¾ç½®' ä¿®æ”¹ä¸º 'ä¸éœ€è¦'ã€‚")
    
    # å›¾ç‰‡ä¿æŒä¸å˜ï¼ˆå¤ç”¨æ¨¡æ¿å•†å“çš„å›¾ç‰‡ï¼‰
    log_func("âœ… å›¾ç‰‡é“¾æ¥ä¿æŒåŸæ ·ï¼ˆå¤ç”¨æ¨¡æ¿å•†å“å›¾ç‰‡ï¼‰ã€‚")
    
    # æ›´æ–° commodity å­—æ®µä¸­çš„ä»·æ ¼ï¼ˆè¿™æ˜¯ç½‘é¡µç«¯APIçœŸæ­£è¯»å–çš„åŸä»·ï¼‰
    # é‡è¦ï¼šæ ¹æ®commodity_typeæ„å»ºæ­£ç¡®çš„ç»“æ„
    try:
        commodity_type = new_data.get("commodity_type", "ç½‘è´¹")
        log_func(f"--- [Commodityæ›´æ–°] ç›®æ ‡ç±»å‹: {commodity_type}")
        
        # æ ¹æ®ç±»å‹æ„å»ºcommodityç»“æ„
        if commodity_type == "ç½‘è´¹":
            # ç½‘è´¹ç±»å‹ï¼šç®€å•ç»“æ„ï¼Œä¸éœ€è¦æœåŠ¡æ—¶é•¿ç­‰å­—æ®µ
            # æ³¨æ„ï¼špriceå¿…é¡»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼
            
            # ç¡®å®šé€‚ç”¨äººç¾¤
            member_type = new_data.get("member_type", "ä¸é™åˆ¶")
            if member_type == "æ–°å®¢":
                suitable_group_key = 2
                suitable_group_value = "æœ¬åº—æ–°ä¼šå‘˜"
            elif member_type == "è€å®¢":
                suitable_group_key = 3
                suitable_group_value = "æœ¬åº—è€ä¼šå‘˜"
            else:
                suitable_group_key = 1
                suitable_group_value = "ä¸é™åˆ¶"
            
            commodity_obj = [{
                "group_name": "ç½‘è´¹",
                "total_count": 1,
                "option_count": 1,
                "item_list": [{
                    "count": "1",
                    "count-unit": json.dumps({"count": 1, "unit": "FEN"}, ensure_ascii=False),
                    "includeMeal": json.dumps({"value": False}, ensure_ascii=False),
                    "itemOpticalItemClassify": json.dumps({"value": 1, "label": "ç½‘è´¹æœåŠ¡", "isCustom": None}, ensure_ascii=False),
                    "itemSuitableGroup": json.dumps({"key": suitable_group_key, "value": suitable_group_value}, ensure_ascii=False),
                    "name": "ç½‘è´¹",
                    "price": str(origin_amount),  # å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼
                    "unit": "FEN"
                }]
            }]
            log_func(f"âœ… å·²æ„å»ºç½‘è´¹ç±»å‹commodityç»“æ„ï¼ŒåŸä»·: {origin_amount/100}å…ƒï¼Œé€‚ç”¨äººç¾¤: {suitable_group_value}")
        else:
            # åŒ…æ—¶ç±»å‹ï¼šéœ€è¦ä¿ç•™æœåŠ¡æ—¶é•¿ç­‰å­—æ®µ
            commodity_str = comp_map.get('commodity')
            if commodity_str:
                log_func(f"--- [Commodityæ›´æ–°] åŸå§‹commodityé•¿åº¦: {len(commodity_str)} å­—ç¬¦")
                commodity_obj = json.loads(commodity_str)
                log_func(f"--- [Commodityæ›´æ–°] è§£æåæœ‰ {len(commodity_obj)} ä¸ªgroup")
                
                if commodity_obj and len(commodity_obj) > 0:
                    # åªä¿ç•™ç¬¬ä¸€ä¸ªgroup
                    first_group = commodity_obj[0]
                    log_func(f"--- [Commodityæ›´æ–°] ç¬¬ä¸€ä¸ªgroupæœ‰ {len(first_group.get('item_list', []))} ä¸ªitem")
                    
                    if 'item_list' in first_group and len(first_group['item_list']) > 0:
                        # åªä¿ç•™ç¬¬ä¸€ä¸ªitem
                        first_item = first_group['item_list'][0]
                        
                        # æ›´æ–°åŸä»·
                        old_price = first_item.get('price')
                        first_item['price'] = origin_amount
                        
                        # æ›´æ–°åç§°
                        first_group['group_name'] = commodity_type
                        first_item['name'] = commodity_type
                        
                        # åªä¿ç•™ç¬¬ä¸€ä¸ªitem
                        first_group['item_list'] = [first_item]
                        first_group['total_count'] = 1
                        first_group['option_count'] = 1
                        
                        log_func(f"âœ… å·²ç®€åŒ–å¹¶æ›´æ–° commodity: price {old_price} â†’ {origin_amount} ({origin_amount/100}å…ƒ)")
                    
                    # åªä¿ç•™ç¬¬ä¸€ä¸ªgroup
                    commodity_obj = [first_group]
            else:
                log_func("[Warning] æ¨¡æ¿ä¸­æ²¡æœ‰æ‰¾åˆ° commodity å­—æ®µï¼")
                commodity_obj = None
        
        if commodity_obj:
            comp_map['commodity'] = json.dumps(commodity_obj, ensure_ascii=False)
            log_func(f"--- [Commodityæ›´æ–°] æœ€ç»ˆcommodityé•¿åº¦: {len(comp_map['commodity'])} å­—ç¬¦")
    except Exception as e:
        log_func(f"[Warning] æ›´æ–° commodity ä»·æ ¼æ—¶å‡ºé”™: {e}")
        import traceback
        log_func(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    # å¼ºåˆ¶ä½¿ç”¨å›ºå®šçš„ poi_set_id
    fixed_poi_set_id = "7585041807923316776"
    product_object['extra_map'] = {
        "poi_set_id": fixed_poi_set_id,
        "poi_check_result": "",
        "boost_strategy": '{"ai_recommend_title":"","ai_recommend_title_source":""}'
    }
    log_func(f"âœ… å·²å¼ºåˆ¶å°† 'extra_map' è®¾ç½®ä¸ºå›ºå®šå€¼ï¼Œpoi_set_id ä¸º: {fixed_poi_set_id}")
    
    # æ›´æ–° SKU ä»·æ ¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'sku' in product_detail_template:
        sku_object = product_detail_template['sku']
        log_func(f"--- [SKUæ›´æ–°å‰] actual_amount: {sku_object.get('actual_amount')}, origin_amount: {sku_object.get('origin_amount')}")
        sku_object['actual_amount'] = actual_amount
        sku_object['origin_amount'] = origin_amount
        sku_object['sku_name'] = new_data["å›¢è´­æ ‡é¢˜"]
        log_func(f"--- [SKUæ›´æ–°å] actual_amount: {sku_object.get('actual_amount')}, origin_amount: {sku_object.get('origin_amount')}")
        log_func(f"âœ… SKU ä»·æ ¼å·²åŒæ­¥æ›´æ–°: å”®ä»·={actual_amount/100}å…ƒ, åŸä»·={origin_amount/100}å…ƒ")
    else:
        log_func("[Warning] product_detail_template ä¸­æ²¡æœ‰ 'sku' å­—æ®µ")
    
    # æ„å»ºæœ€ç»ˆpayload
    final_payload = {
        "product_detail": product_detail_template,
        "save_product_draft_cache_type": 4,
        "product_cache_scene": 1,
        "version_info": {
            "Enable": True,
            "VersionName": "1.0.8"
        }
    }
    
    # æ‰“å°å…³é”®ä»·æ ¼ä¿¡æ¯ç”¨äºè°ƒè¯•
    log_func("--- [ä»·æ ¼ä¿¡æ¯æ£€æŸ¥] ---")
    log_func(f"Product actualAmount: {comp_map.get('actualAmount')}")
    log_func(f"Product originAmount: {comp_map.get('originAmount')}")
    
    # æ‰“å°commodityå­—æ®µä¸­çš„ä»·æ ¼
    try:
        commodity_str = comp_map.get('commodity')
        if commodity_str:
            commodity_obj = json.loads(commodity_str)
            log_func(f"Commodity ç»“æ„: {len(commodity_obj)} ä¸ªgroup")
            for idx, group in enumerate(commodity_obj):
                if 'item_list' in group:
                    for item_idx, item in enumerate(group['item_list']):
                        log_func(f"  Group[{idx}].item[{item_idx}].price = {item.get('price')}")
    except:
        pass
    
    if 'sku' in product_detail_template:
        log_func(f"SKU actual_amount: {product_detail_template['sku'].get('actual_amount')}")
        log_func(f"SKU origin_amount: {product_detail_template['sku'].get('origin_amount')}")
    
    return final_payload

def _create_product_web(session, product_payload, root_life_account_id, log_func):
    """é€šè¿‡ç½‘é¡µç«¯APIåˆ›å»ºå•†å“"""
    log_func("--- [ç½‘é¡µç«¯] æ­£åœ¨å‘é€åˆ›å»ºå•†å“è¯·æ±‚... ---")
    url = "https://life.douyin.com/life/tobias/product/save/"
    params = {'root_life_account_id': root_life_account_id}
    
    # æ‰“å°å®Œæ•´çš„è¯·æ±‚payloadç”¨äºè°ƒè¯•
    log_func("--- [å®Œæ•´è¯·æ±‚Payload] ---")
    payload_str = json.dumps(product_payload, ensure_ascii=False, indent=2)
    # æ‰“å°å®Œæ•´payloadï¼Œä¸æˆªæ–­ï¼ˆç”¨äºè°ƒè¯•åŸä»·é—®é¢˜ï¼‰
    log_func(payload_str)
    log_func("-" * 60)
    
    try:
        response = session.post(url, params=params, data=json.dumps(product_payload), timeout=20)
        response.raise_for_status()
        result = response.json()
        log_func(f"æœåŠ¡å™¨å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        if result.get('status_code') == 0:
            product_id = result.get('product_id') or result.get('product', {}).get('product_id')
            if product_id and product_id != "0":
                log_func(f"[SUCCESS] å•†å“åˆ›å»ºæˆåŠŸï¼Product ID: {product_id}")
                return product_id, None
        
        return None, result.get('status_msg', 'æœªçŸ¥APIé”™è¯¯')
    except requests.exceptions.RequestException as e:
        return None, f"åˆ›å»ºå•†å“è¯·æ±‚å¤±è´¥: {e}"

def _wait_for_product_approval(access_token, product_id, log_func, max_wait_time=60, check_interval=5):
    """ç­‰å¾…å•†å“å®¡æ ¸é€šè¿‡"""
    log_func(f"--- ç­‰å¾…å•†å“å®¡æ ¸é€šè¿‡ï¼ˆæœ€å¤šç­‰å¾…{max_wait_time}ç§’ï¼‰... ---")
    
    start_time = time.time()
    attempt = 0
    
    while time.time() - start_time < max_wait_time:
        attempt += 1
        log_func(f"ç¬¬{attempt}æ¬¡æ£€æŸ¥å®¡æ ¸çŠ¶æ€...")
        
        # å°è¯•è·å–å•†å“è¯¦æƒ…
        product_details = get_douyin_product_details(access_token, product_id, log_func)
        
        if product_details:
            log_func(f"âœ… å•†å“å®¡æ ¸å·²é€šè¿‡ï¼å¯ä»¥è¿›è¡Œåç»­æ“ä½œã€‚")
            return True, product_details
        
        # ç­‰å¾…åå†æ¬¡æ£€æŸ¥
        if time.time() - start_time < max_wait_time:
            log_func(f"å•†å“ä»åœ¨å®¡æ ¸ä¸­ï¼Œ{check_interval}ç§’åå†æ¬¡æ£€æŸ¥...")
            time.sleep(check_interval)
    
    log_func(f"[Warning] ç­‰å¾…è¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰ï¼Œå•†å“å¯èƒ½ä»åœ¨å®¡æ ¸ä¸­ã€‚")
    return False, None

def create_product_via_web(cookie, csrf_token, root_life_account_id, template_product_id, new_data, target_poi_id, access_token, log_func):
    """ä½¿ç”¨ç½‘é¡µç«¯APIåˆ›å»ºå•†å“ï¼ˆé‡åˆ›æ¨¡å¼ä¸“ç”¨ï¼‰- å¤ç”¨æ¨¡æ¿å›¾ç‰‡ï¼Œåˆ›å»ºåè‡ªåŠ¨ä¿®æ”¹POI ID"""
    log_func("========== å¼€å§‹ é‡åˆ› å•†å“ï¼ˆç½‘é¡µç«¯æ¨¡å¼ï¼‰==========")
    
    if not cookie or not csrf_token:
        log_func("[Error] ç½‘é¡µç«¯Cookieæˆ–CSRF Tokenæœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨é‡åˆ›æ¨¡å¼ã€‚")
        return None, "ç¼ºå°‘ç½‘é¡µç«¯è®¤è¯ä¿¡æ¯"
    
    if not template_product_id:
        log_func("[Error] é‡åˆ›æ¨¡å¼éœ€è¦ä¸€ä¸ªæ¨¡æ¿å•†å“IDã€‚")
        return None, "ç¼ºå°‘æ¨¡æ¿å•†å“ID"
    
    session = requests.Session()
    session.headers.update({
        'Accept': 'application/json, text/plain, */*',
        'Cookie': cookie,
        'Origin': 'https://life.douyin.com',
        'Referer': 'https://life.douyin.com/p/product/create',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0',
        'x-secsdk-csrf-token': csrf_token,
        'Content-Type': 'application/json;charset=UTF-8'
    })

    # æ­¥éª¤1: è·å–æ¨¡æ¿å•†å“
    log_func(f"--- æ­¥éª¤1: è·å–æ¨¡æ¿å•†å“ (ID: {template_product_id}) ---")
    product_detail_template, error = _get_product_template_web(session, template_product_id, root_life_account_id, log_func)
    if error:
        log_func(f"[FAILURE] è·å–æ¨¡æ¿å¤±è´¥: {error}")
        return None, error

    # æ­¥éª¤2: æ„å»ºpayloadï¼ˆå¤ç”¨æ¨¡æ¿å›¾ç‰‡ï¼Œä½¿ç”¨å›ºå®šPOI IDï¼‰
    log_func("--- æ­¥éª¤2: æ„å»ºå•†å“payloadï¼ˆå¤ç”¨å›¾ç‰‡ï¼Œå›ºå®šPOI IDï¼‰ ---")
    product_payload = _build_web_product_payload_from_template(product_detail_template, new_data, log_func)
    if not product_payload:
        log_func("[FAILURE] æ„å»ºpayloadå¤±è´¥")
        return None, "æ„å»ºpayloadå¤±è´¥"
    
    # æ­¥éª¤3: åˆ›å»ºå•†å“
    log_func("--- æ­¥éª¤3: åˆ›å»ºæ–°å•†å“ ---")
    new_product_id, error = _create_product_web(session, product_payload, root_life_account_id, log_func)
    if error:
        log_func(f"[FAILURE] å•†å“ '{new_data['å›¢è´­æ ‡é¢˜']}' åˆ›å»ºå¤±è´¥: {error}")
        return None, error
    
    log_func(f"âœ… å•†å“åˆ›å»ºæˆåŠŸï¼æ–°å•†å“ID: {new_product_id}")
    
    # æ­¥éª¤4: ç­‰å¾…å•†å“å®¡æ ¸é€šè¿‡
    log_func(f"--- æ­¥éª¤4: ç­‰å¾…å•†å“å®¡æ ¸é€šè¿‡ ---")
    approval_success, full_product_data = _wait_for_product_approval(access_token, new_product_id, log_func, max_wait_time=60, check_interval=5)
    
    if not approval_success or not full_product_data:
        log_func("[Warning] å•†å“å¯èƒ½ä»åœ¨å®¡æ ¸ä¸­ï¼Œæ— æ³•ç«‹å³ä¿®æ”¹POI IDã€‚")
        log_func("[Info] å•†å“å·²åˆ›å»ºæˆåŠŸï¼Œä½†POI IDä¸ºå›ºå®šå€¼ã€‚è¯·ç¨åæ‰‹åŠ¨ä¿®æ”¹æˆ–ç­‰å¾…å®¡æ ¸é€šè¿‡åé‡æ–°è¿è¡Œã€‚")
        return new_product_id, None
    
    # æ­¥éª¤5: ä½¿ç”¨å¼€æ”¾å¹³å°APIä¿®æ”¹POI IDåˆ°ç›®æ ‡é—¨åº—
    log_func(f"--- æ­¥éª¤5: ä¿®æ”¹POI IDåˆ°ç›®æ ‡é—¨åº— (POI ID: {target_poi_id}) ---")
    
    try:
        product_to_save = full_product_data.get('product')
        sku_to_save = full_product_data.get('skus', [{}])[0] if full_product_data.get('skus') else full_product_data.get('sku')
        
        if not product_to_save or not sku_to_save:
            log_func("[Warning] å•†å“æ•°æ®ä¸å®Œæ•´ï¼ŒPOI IDå¯èƒ½æœªæ›´æ–°ã€‚")
            return new_product_id, None
        
        # æ›´æ–°POI IDåˆ°ç›®æ ‡é—¨åº—
        product_to_save['pois'] = [{"poi_id": str(target_poi_id)}]
        extra_obj = json.loads(product_to_save.get("extra", "{}"))
        extra_obj['poi_set_id'] = str(target_poi_id)
        product_to_save['extra'] = json.dumps(extra_obj)
        
        # ç¡®ä¿æ‰€æœ‰å¿…å¡«å­—æ®µå­˜åœ¨
        log_func("æ­£åœ¨æ£€æŸ¥å¹¶è¡¥å……å¿…å¡«å­—æ®µ...")
        
        # 1. product å¿…å¡«å­—æ®µ
        if "attr_key_value_map" not in product_to_save:
            product_to_save["attr_key_value_map"] = {}
        
        # RefundPolicyï¼ˆé€€æ¬¾æ”¿ç­–ï¼‰
        if "RefundPolicy" not in product_to_save["attr_key_value_map"]:
            product_to_save["attr_key_value_map"]["RefundPolicy"] = "2"
            log_func("å·²æ·»åŠ ç¼ºå¤±çš„ RefundPolicy å­—æ®µ")
        
        # Notificationï¼ˆä½¿ç”¨é¡»çŸ¥ï¼‰
        if "Notification" not in product_to_save["attr_key_value_map"]:
            notification_content = [
                {"title": "ä½¿ç”¨é¡»çŸ¥", "content": "è¯·æŒ‰ç…§å•†å®¶è§„å®šä½¿ç”¨"},
                {"title": "é™è´­è¯´æ˜", "content": "æ¯äººé™è´­1ä»½"},
                {"title": "æœ‰æ•ˆæœŸ", "content": "è´­ä¹°å30æ—¥å†…æœ‰æ•ˆ"}
            ]
            product_to_save["attr_key_value_map"]["Notification"] = json.dumps(notification_content, ensure_ascii=False)
            log_func("å·²æ·»åŠ ç¼ºå¤±çš„ Notification å­—æ®µ")
        
        # Descriptionï¼ˆå•†å“æè¿°ï¼‰
        if "Description" not in product_to_save["attr_key_value_map"]:
            product_to_save["attr_key_value_map"]["Description"] = json.dumps(["é€‚ç”¨åŒºåŸŸ: å…¨åœºé€šç”¨"], ensure_ascii=False)
            log_func("å·²æ·»åŠ ç¼ºå¤±çš„ Description å­—æ®µ")
        
        # 2. sku å¿…å¡«å­—æ®µ
        if "attr_key_value_map" not in sku_to_save:
            sku_to_save["attr_key_value_map"] = {}
        
        # use_typeï¼ˆä½¿ç”¨ç±»å‹ï¼‰
        if "use_type" not in sku_to_save.get("attr_key_value_map", {}):
            sku_to_save["attr_key_value_map"]["use_type"] = "1"
            log_func("å·²æ·»åŠ ç¼ºå¤±çš„ use_type å­—æ®µ")
        
        log_func(f"æ­£åœ¨å°†POI IDä»å›ºå®šå€¼æ›´æ–°ä¸ºç›®æ ‡é—¨åº—: {target_poi_id}")
        
        # æ„å»ºä¿å­˜è¯·æ±‚
        save_payload = {
            "account_id": str(DOUYIN_ACCOUNT_ID),
            "product": product_to_save,
            "sku": sku_to_save,
            "poi_ids": [str(target_poi_id)],
            "supplier_ext_ids": [str(target_poi_id)]
        }
        
        headers = {"Content-Type": "application/json", "access-token": access_token}
        response = requests.post(DOUYIN_PRODUCT_SAVE_URL, headers=headers, json=save_payload, timeout=20)
        response.raise_for_status()
        response_data = response.json()
        
        if response_data.get('data', {}).get('error_code') == 0:
            log_func(f"âœ… POI IDå·²æˆåŠŸæ›´æ–°åˆ°ç›®æ ‡é—¨åº—ï¼")
        else:
            log_func(f"[Warning] POI IDæ›´æ–°å¤±è´¥: {response_data.get('data', {}).get('description', 'æœªçŸ¥é”™è¯¯')}")
    
    except Exception as e:
        log_func(f"[Warning] æ›´æ–°POI IDæ—¶å‡ºé”™: {e}")
    
    log_func(f"[SUCCESS] å•†å“ '{new_data['å›¢è´­æ ‡é¢˜']}' é‡åˆ›å®Œæˆï¼Product ID: {new_product_id}")
    return new_product_id, None

def update_douyin_product(access_token, template_product_id, new_data, log_func, mode="ä¿®æ”¹", image_dir=None, target_poi_id=None):
    log_func(f"========== å¼€å§‹ {mode} å•†å“ ==========")

    if mode == "é‡åˆ›":
        if not target_poi_id:
            log_func("[Error] é‡åˆ›æ¨¡å¼ä¸‹éœ€è¦ä¸€ä¸ªç›®æ ‡é—¨åº—ID(target_poi_id)ï¼Œä½†æœªæä¾›ã€‚")
            return False, "é‡åˆ›æ¨¡å¼ç¼ºå°‘ç›®æ ‡é—¨åº—ID"
        try:
            with open('å•†å“å®Œæ•´å†…å®¹.json', 'r', encoding='utf-8') as f:
                template_data = json.load(f)
            full_product_data = template_data['data']['product_onlines'][0]
            log_func("æˆåŠŸä» 'å•†å“å®Œæ•´å†…å®¹.json' åŠ è½½æ¨¡æ¿ã€‚")
        except Exception as e:
            log_func(f"[Error] åŠ è½½æ¨¡æ¿æ–‡ä»¶ 'å•†å“å®Œæ•´å†…å®¹.json' å¤±è´¥: {e}")
            return False, "åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥"
    else: # ä¿®æ”¹æ¨¡å¼
        if not template_product_id:
             log_func("[Error] ä¿®æ”¹æ¨¡å¼ä¸‹éœ€è¦ä¸€ä¸ªæ¨¡æ¿å•†å“IDï¼Œä½†æœªæä¾›ã€‚")
             return False, "ä¿®æ”¹æ¨¡å¼ç¼ºå°‘æ¨¡æ¿ID"
        full_product_data = get_douyin_product_details(access_token, template_product_id, log_func)

    if not full_product_data: return False, "è·å–æ¨¡æ¿å•†å“è¯¦æƒ…å¤±è´¥"

    try:
        product_to_save = full_product_data.get('product')
        sku_to_save = full_product_data.get('skus')[0] if full_product_data.get('skus') else full_product_data.get('sku')
        if not product_to_save or not sku_to_save: return False, f"æ¨¡æ¿å•†å“æ•°æ®ä¸å®Œæ•´"

        log_func(f"ä½¿ç”¨æ¨¡æ¿è¿›è¡Œ{mode}æ“ä½œ...")
        
        poi_ids_for_saving = []

        if mode == "é‡åˆ›":
            # --- åŠ¨æ€æ›´æ–°POI ID ---
            product_to_save['pois'] = [{"poi_id": str(target_poi_id)}]
            extra_obj = json.loads(product_to_save.get("extra", "{}"))
            extra_obj['poi_set_id'] = str(target_poi_id)
            product_to_save['extra'] = json.dumps(extra_obj)
            poi_ids_for_saving.append(str(target_poi_id))
            log_func(f"å•†å“POI IDå·²æ›´æ–°ä¸º: {target_poi_id}")

            # --- åŠ¨æ€æ›´æ–°ç”¨æˆ·ç±»å‹ (commodity) ---
            member_type = new_data.get("member_type")
            if member_type:
                try:
                    commodity_str = sku_to_save['attr_key_value_map']['commodity']
                    commodity_obj = json.loads(commodity_str)
                    
                    # æ›´æ–°é€‚ç”¨äººç¾¤
                    member_type_map = {
                        "æ–°å®¢": '{"key":2,"value":"ä»…é™æ–°å®¢"}',
                        "è€å®¢": '{"key":3,"value":"ä»…é™è€å®¢"}',
                        "ä¸é™åˆ¶": '{"key":1,"value":"ä¸é™åˆ¶"}'
                    }
                    if member_type in member_type_map and commodity_obj and commodity_obj[0].get('item_list'):
                        for item in commodity_obj[0]['item_list']:
                            for attr in item.get('attr_list', []):
                                if attr.get('attr_key') == 'item_suitable_group':
                                    attr['attr_value'] = member_type_map[member_type]
                                    log_func(f"å•†å“é€‚ç”¨äººç¾¤å·²æ›´æ–°ä¸º: {member_type}")
                                    break
                    
                    # æ›´æ–°å¥—é¤ç±»å‹å’ŒåŸä»·
                    commodity_type = new_data.get("commodity_type")
                    origin_price = new_data.get("åŸä»·")
                    applicable_location = new_data.get("applicable_location")

                    if commodity_obj and commodity_obj[0].get('item_list'):
                        commodity_group = commodity_obj[0]
                        item_list_inner = commodity_group['item_list'][0]
                        
                        # æ›´æ–°å¥—é¤ç±»å‹å’ŒåŸä»·
                        if commodity_type and origin_price is not None:
                            log_func(f"å‡†å¤‡æ ¹æ®ç±»å‹ '{commodity_type}' å’ŒåŸä»· '{origin_price}' æ›´æ–°commodityå­—æ®µ...")
                            commodity_group['group_name'] = commodity_type
                            item_list_inner['name'] = commodity_type
                            item_list_inner['price'] = int(origin_price * 100)
                            log_func(f"commodityå†…éƒ¨çš„group_nameå’Œnameå·²æ›´æ–°ä¸º'{commodity_type}'ï¼Œpriceå·²æ›´æ–°ä¸º'{item_list_inner['price']}'")
                        
                        # æ›´æ–°é€‚ç”¨ä½ç½®
                        if applicable_location:
                            for attr in item_list_inner.get('attr_list', []):
                                if attr.get('attr_key') == 'applicable_location':
                                    location_value = json.loads(attr['attr_value'])
                                    location_value['value'] = applicable_location
                                    attr['attr_value'] = json.dumps(location_value, ensure_ascii=False)
                                    log_func(f"å•†å“é€‚ç”¨ä½ç½®å·²æ›´æ–°ä¸º: {applicable_location}")
                                    break
                        
                        # æ›´æ–°é¡¹ç›®åˆ†ç±»
                        optical_classify_map = {
                            "åŒ…æ—¶": '{"key":2,"value":"ä¸Šç½‘åŒ…æ—¶ç±»æœåŠ¡"}',
                            "ç½‘è´¹": '{"key":1,"value":"ç½‘è´¹æœåŠ¡"}'
                        }
                        if commodity_type in optical_classify_map:
                             for attr in item_list_inner.get('attr_list', []):
                                if attr.get('attr_key') == 'item_optical_item_classify':
                                    attr['attr_value'] = optical_classify_map[commodity_type]
                                    log_func(f"é¡¹ç›®åˆ†ç±»å·²æ ¹æ®å¥—é¤ç±»å‹ '{commodity_type}' æ›´æ–°ã€‚")
                                    break

                    sku_to_save['attr_key_value_map']['commodity'] = json.dumps(commodity_obj, ensure_ascii=False)
                except Exception as e:
                    log_func(f"[Warning] æ›´æ–°å•†å“ 'commodity' å­—æ®µå¤±è´¥: {e}")
        else: # ä¿®æ”¹æ¨¡å¼
             extra_obj = json.loads(product_to_save.get("extra", "{}"))
             poi_set_id = extra_obj.get("poi_set_id")
             if not poi_set_id: return False, f"åœ¨ extra å­—æ®µä¸­æœªæ‰¾åˆ° poi_set_id"
             poi_ids_for_saving.append(str(poi_set_id))

        if mode == "é‡åˆ›" and image_dir and new_data.get("matched_image"):
            image_filename = new_data["matched_image"]
            found_image_path = os.path.join(image_dir, image_filename)
            log_func(f"--- [æ–°å¢å¥—é¤] å‡†å¤‡ä½¿ç”¨å¤´å›¾: {image_filename} ---")

            if os.path.exists(found_image_path):
                try:
                    original_image = Image.open(found_image_path)
                    poi_id_for_filename = str(json.loads(product_to_save.get("extra", "{}")).get("poi_set_id", "unknown_poi"))

                    log_func("--- å¼€å§‹å¤„ç†å’Œä¸Šä¼ å›¾ç‰‡ ---")
                    img_1_1 = center_crop_image(original_image, 1/1)
                    img_4_3 = center_crop_image(original_image, 4/3)
                    
                    url_1_1 = upload_to_r2(img_1_1, poi_id_for_filename, "1:1", log_func)
                    url_4_3 = upload_to_r2(img_4_3, poi_id_for_filename, "4:3", log_func)

                    if url_1_1 and url_4_3:
                        log_func("å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨æ›´æ–°å•†å“å¤´å›¾ä¿¡æ¯...")
                        image_list_4_3 = [{"url": url_4_3}, {"url": url_4_3}]
                        product_to_save['attr_key_value_map']['image_list'] = json.dumps(image_list_4_3, ensure_ascii=False)
                        image_list_1v1 = [{"url": url_1_1}, {"url": url_4_3}]
                        product_to_save['attr_key_value_map']['image_1v1_list'] = json.dumps(image_list_1v1, ensure_ascii=False)
                        log_func("å•†å“å¤´å›¾ä¿¡æ¯å·²æ›´æ–°ã€‚")
                    else:
                        log_func("[Warning] å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œå°†ä¸å¸¦å¤´å›¾åˆ›å»ºå¥—é¤ã€‚")
                except Exception as img_e:
                    log_func(f"[Error] å¤„ç†æˆ–ä¸Šä¼ å›¾ç‰‡ '{image_filename}' æ—¶å‡ºé”™: {img_e}ï¼Œå°†ä¸å¸¦å¤´å›¾åˆ›å»ºå¥—é¤ã€‚")
            else:
                log_func(f"[Error] åœ¨ç›®å½• '{image_dir}' ä¸­æœªæ‰¾åˆ°æŒ‡å®šçš„å›¾ç‰‡æ–‡ä»¶ '{image_filename}'ï¼Œå°†ä¸å¸¦å¤´å›¾åˆ›å»ºå¥—é¤ã€‚")

        product_to_save["product_name"] = new_data["å›¢è´­æ ‡é¢˜"]
        notification_content = [{"title": "ä½¿ç”¨é¡»çŸ¥", "content": new_data['å›¢å•å¤‡æ³¨']}, {"title": "é™è´­è¯´æ˜", "content": new_data['é™è´­']}, {"title": "æœ‰æ•ˆæœŸ", "content": f"è´­ä¹°å{new_data['æœ‰æ•ˆæœŸ']}å†…æœ‰æ•ˆ"}]
        product_to_save['attr_key_value_map']['Notification'] = json.dumps(notification_content, ensure_ascii=False)
        product_to_save['attr_key_value_map']['Description'] = json.dumps([f"é€‚ç”¨åŒºåŸŸ: {new_data['å¯ç”¨åŒºåŸŸ']}"], ensure_ascii=False)
        if "RefundPolicy" not in product_to_save["attr_key_value_map"]: product_to_save["attr_key_value_map"]["RefundPolicy"] = "2"

        sku_to_save["sku_name"] = new_data["å›¢è´­æ ‡é¢˜"]
        sku_to_save["actual_amount"] = int(new_data["å”®ä»·"] * 100)
        # åŒæ—¶æ›´æ–°åŸä»·
        if new_data.get("åŸä»·"):
            sku_to_save["origin_amount"] = int(new_data["åŸä»·"] * 100)
        elif "origin_amount" not in sku_to_save:
             sku_to_save["origin_amount"] = int(new_data["å”®ä»·"] * 100) # å¦‚æœæ²¡æœ‰æä¾›åŸä»·ï¼Œé»˜è®¤ç­‰äºå”®ä»·

        if "use_type" not in sku_to_save.get("attr_key_value_map", {}):
            if "attr_key_value_map" not in sku_to_save: sku_to_save["attr_key_value_map"] = {}
            sku_to_save["attr_key_value_map"]["use_type"] = "1"
        
        if mode == "é‡åˆ›":
            if "product_id" in product_to_save: del product_to_save["product_id"]
            if "sku_id" in sku_to_save: del sku_to_save["sku_id"]
            log_func("æ“ä½œæ¨¡å¼ä¸ºâ€œé‡åˆ›â€ï¼Œå·²ç§»é™¤ product_id å’Œ sku_idã€‚")

        save_payload = {"account_id": str(DOUYIN_ACCOUNT_ID), "product": product_to_save, "sku": sku_to_save, "poi_ids": poi_ids_for_saving, "supplier_ext_ids": poi_ids_for_saving}
        log_func(f"å‡†å¤‡å‘é€ {mode} è¯·æ±‚...")
        log_func(f"--- [API Request Payload] ---\n{json.dumps(save_payload, ensure_ascii=False, indent=2)}\n" + "-"*30)
        
        headers = {"Content-Type": "application/json", "access-token": access_token}
        response = requests.post(DOUYIN_PRODUCT_SAVE_URL, headers=headers, json=save_payload, timeout=20)
        response.raise_for_status()
        response_data = response.json()

        if response_data.get('data', {}).get('error_code') == 0:
            log_func(f"[SUCCESS] å•†å“ '{new_data['å›¢è´­æ ‡é¢˜']}' {mode}æˆåŠŸ!")
            return True, ""
        else:
            reason = response_data.get('data', {}).get('description', 'APIè¿”å›æœªçŸ¥é”™è¯¯')
            log_func(f"[FAILURE] å•†å“ '{new_data['å›¢è´­æ ‡é¢˜']}' {mode}å¤±è´¥: {reason}")
            return False, reason
    except Exception as e:
        log_func(f"å¤„ç†å•†å“æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}\n{traceback.format_exc()}")
        return False, f"æ„å¤–é”™è¯¯: {e}"

def extract_cells_with_formatting(file_path, log_func):
    """æå–Excelå•å…ƒæ ¼å†…å®¹å’Œæ ¼å¼åŒ–ä¿¡æ¯ï¼ˆé¢œè‰²ç­‰ï¼‰"""
    try:
        from openpyxl import load_workbook
        from openpyxl.cell.cell import MergedCell
        
        log_func("æ­£åœ¨æå–Excelå•å…ƒæ ¼æ ¼å¼åŒ–ä¿¡æ¯...")
        wb = load_workbook(file_path, data_only=False)
        ws = wb.active
        
        cells_data = []
        for row_idx in range(1, ws.max_row + 1):
            row_data = {}
            for col_idx in range(1, ws.max_column + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                
                # è·³è¿‡åˆå¹¶å•å…ƒæ ¼
                if isinstance(cell, MergedCell):
                    continue
                    
                value = cell.value
                
                # è·å–èƒŒæ™¯é¢œè‰²
                fill = cell.fill
                bg_color = None
                try:
                    if hasattr(fill, 'start_color') and fill.start_color.rgb and str(fill.start_color.rgb) != '00000000':
                        bg_color = f"#{fill.start_color.rgb}"
                except:
                    pass
                
                # å°†åˆ—ç´¢å¼•è½¬æ¢ä¸ºåˆ—å­—æ¯
                col_letter = chr(64 + col_idx)  # A=65, B=66, etc.
                col_index = col_idx
                
                row_data[f"col_{col_index}"] = {
                    "value": value,
                    "bg_color": bg_color,
                    "position": f"R{row_idx}C{col_index}",
                    "col_letter": col_letter
                }
            
            if any(cell_info["value"] for cell_info in row_data.values() if cell_info["value"] is not None):
                cells_data.append({
                    "row": row_idx,
                    "cells": row_data
                })
        
        log_func(f"æˆåŠŸæå– {len(cells_data)} è¡Œæ•°æ®")
        return cells_data
    
    except Exception as e:
        log_func(f"[Error] æå–Excelæ ¼å¼åŒ–ä¿¡æ¯å¤±è´¥: {e}")
        import traceback
        log_func(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def build_llm_prompt_for_table_parsing(cells_data, log_func):
    """æ„å»ºç”¨äºæ™ºèƒ½è¡¨æ ¼è§£æçš„LLM Prompt"""
    
    # å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼ŒåŒ…å«é¢œè‰²ä¿¡æ¯
    table_text = "è¡¨æ ¼æ•°æ®ï¼ˆåŒ…å«ä½ç½®ã€è¡Œå·ã€åˆ—å·ã€å€¼å’ŒèƒŒæ™¯é¢œè‰²ï¼‰:\n\n"
    
    for row_info in cells_data:
        row_num = row_info["row"]
        table_text += f"ç¬¬{row_num}è¡Œ: "
        row_parts = []
        
        for col_key in sorted(row_info["cells"].keys()):
            cell_info = row_info["cells"][col_key]
            value = cell_info["value"]
            bg_color = cell_info["bg_color"]
            position = cell_info["position"]
            
            if value is not None and str(value).strip():
                if bg_color:
                    row_parts.append(f"[{position}]'{value}'({bg_color})")
                else:
                    row_parts.append(f"[{position}]'{value}'")
        
        if row_parts:
            table_text += " | ".join(row_parts) + "\n"
    
    prompt = f"""
# ä»»åŠ¡ï¼šæ™ºèƒ½è§£æExcelè¡¨æ ¼æ•°æ®

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Excelè¡¨æ ¼è§£æä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„è¡¨æ ¼æ•°æ®ï¼Œæ™ºèƒ½è¯†åˆ«è¡¨å¤´å’Œæ•°æ®ï¼Œå¹¶ç†è§£ä¸šåŠ¡æ„å›¾ã€‚

## è¡¨æ ¼æ•°æ®ï¼š
{table_text}

## è§£æè¦æ±‚ï¼š

1. **æ™ºèƒ½è¡¨å¤´è¯†åˆ«**ï¼š
   - åˆ†æè¡¨æ ¼ç»“æ„ï¼Œç¡®å®šå“ªä¸€è¡Œæ˜¯è¡¨å¤´è¡Œ
   - è¡¨å¤´é€šå¸¸åŒ…å«"æ ‡é¢˜"ã€"ä»·æ ¼"ã€"åŒºåŸŸ"ã€"é™è´­"ã€"æœ‰æ•ˆæœŸ"ã€"å¤‡æ³¨"ç­‰å…³é”®è¯
   - å¦‚æœè¡¨å¤´è¢«æ ‡è®°äº†ç‰¹æ®Šé¢œè‰²ï¼ˆå¦‚é»„è‰²#FFFF00ï¼‰ï¼Œä¼˜å…ˆè€ƒè™‘è¯¥è¡Œä¸ºè¡¨å¤´

2. **æ•°æ®è¡Œè§£æ**ï¼š
   - è·³è¿‡è¡¨å¤´ï¼Œè§£æå®é™…æ•°æ®è¡Œ
   - å°†æ¯è¡Œæ•°æ®æ˜ å°„åˆ°æ ‡å‡†å­—æ®µï¼šå›¢è´­æ ‡é¢˜ã€å”®ä»·ã€å¯ç”¨åŒºåŸŸã€é™è´­ã€æœ‰æ•ˆæœŸã€å›¢å•å¤‡æ³¨

3. **é¢œè‰²æ„å›¾è¯†åˆ«**ï¼š
   - é‡ç‚¹å…³æ³¨èƒŒæ™¯é¢œè‰²ä¸ºé»„è‰²çš„å•å…ƒæ ¼ï¼Œè¿™é€šå¸¸è¡¨ç¤º**ä¿®æ”¹æ„å›¾**ï¼ˆintent: "modify"ï¼‰
   - å¦‚æœæ•´è¡Œéƒ½æœ‰é»„è‰²èƒŒæ™¯ï¼Œåˆ™è¯¥è¡Œæ•°æ®ä¸ºä¿®æ”¹æ“ä½œ
   - å¦‚æœæ²¡æœ‰ç‰¹æ®Šé¢œè‰²æ ‡è®°ï¼Œé»˜è®¤ä¸ºæ–°å¢æ“ä½œï¼ˆintent: "add"ï¼‰

4. **æ•°æ®æ ‡å‡†åŒ–**ï¼š
   - ç¡®ä¿ä»·æ ¼å­—æ®µä¸ºæ•°å­—æ ¼å¼
   - æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬å†…å®¹
   - å¡«å……ç¼ºå¤±çš„å­—æ®µï¼ˆé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰

## è¿”å›æ ¼å¼ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šï¼š

```json
{{
  "header_row": è¡¨å¤´è¡Œå·,
  "data": [
    {{
      "å›¢è´­æ ‡é¢˜": "å•†å“æ ‡é¢˜",
      "å”®ä»·": 0.00,
      "å¯ç”¨åŒºåŸŸ": "åŒºåŸŸæè¿°",
      "é™è´­": "é™è´­è¯´æ˜",
      "æœ‰æ•ˆæœŸ": "æœ‰æ•ˆæœŸ",
      "å›¢å•å¤‡æ³¨": "å¤‡æ³¨å†…å®¹",
      "intent": "modify"
    }}
  ],
  "analysis": {{
    "total_rows": æ€»è¡Œæ•°,
    "header_columns": è¡¨å¤´åˆ—æ˜ å°„,
    "color_analysis": "é¢œè‰²åˆ†æç»“æœ"
  }}
}}
```

**æ³¨æ„äº‹é¡¹**ï¼š
- å¦‚æœæ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œheader_rowè®¾ä¸ºnull
- intentå­—æ®µï¼šå¦‚æœå•å…ƒæ ¼/è¡Œæœ‰ç‰¹æ®Šé¢œè‰²æ ‡è®°åˆ™ä¸º"modify"ï¼Œå¦åˆ™ä¸º"add"
- ä»·æ ¼å¿…é¡»æ˜¯æ•°å­—æ ¼å¼ï¼Œæ— æ³•è½¬æ¢æ—¶è®¾ä¸º0.0
"""
    
    return prompt

def intelligent_load_excel_data(file_path, log_func, cache):
    """ä½¿ç”¨LLMæ™ºèƒ½è§£æExcelæ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    try:
        log_func("æ­£åœ¨ä½¿ç”¨LLMæ™ºèƒ½è§£æExcelè¡¨æ ¼...")
        
        # 1. é¦–å…ˆç”¨pandasè¯»å–Excelï¼Œæ™ºèƒ½è·³è¿‡ç©ºè¡Œ
        df_list = []
        best_df_info = None
        
        # å…ˆè¯»å–å®Œæ•´çš„Excelæ–‡ä»¶æ¥åˆ†æç»“æ„
        try:
            df_raw = pd.read_excel(file_path, engine='openpyxl', header=None)
            log_func(f"åŸå§‹Excelæ–‡ä»¶å½¢çŠ¶: {df_raw.shape}")
            
            # æ™ºèƒ½å¯»æ‰¾çœŸæ­£çš„æ•°æ®å¼€å§‹ä½ç½®
            data_start_row = None
            for i in range(min(10, len(df_raw))):  # æ£€æŸ¥å‰10è¡Œ
                row_data = df_raw.iloc[i]
                # è®¡ç®—éç©ºå•å…ƒæ ¼æ•°é‡
                non_empty_count = sum(1 for val in row_data if pd.notna(val) and str(val).strip())
                
                # å¦‚æœè¿™ä¸€è¡Œæœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œè®¤ä¸ºæ˜¯æ•°æ®å¼€å§‹è¡Œ
                if non_empty_count >= 3:  # è‡³å°‘3åˆ—æœ‰æ•°æ®
                    data_start_row = i
                    log_func(f"æ£€æµ‹åˆ°æ•°æ®å¼€å§‹è¡Œ: {i} (éç©ºå•å…ƒæ ¼: {non_empty_count})")
                    break
            
            if data_start_row is None:
                log_func("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®è¡Œï¼Œä½¿ç”¨é»˜è®¤header=1")
                data_start_row = 1
            
            # ä»æ‰¾åˆ°çš„æ•°æ®å¼€å§‹è¡Œè¯»å–å‡ æ¬¡ï¼Œç¡®å®šæœ€ä½³è¡¨å¤´ä½ç½®
            for header_offset in [0, 1, 2]:  # åœ¨æ•°æ®å¼€å§‹ä½ç½®åŸºç¡€ä¸Šåç§»
                actual_header_row = data_start_row + header_offset
                if actual_header_row >= len(df_raw):
                    continue
                    
                try:
                    df = pd.read_excel(file_path, engine='openpyxl', header=actual_header_row)
                    if not df.empty and len(df.columns) > 3:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åˆ—åï¼ˆéUnnamedä¸”æœ‰æ„ä¹‰ï¼‰
                        valid_columns = [col for col in df.columns if not str(col).startswith('Unnamed')]
                        
                        df_info = {
                            'header_row': actual_header_row,
                            'data_start_row': data_start_row,
                            'dataframe': df,
                            'columns': list(df.columns),
                            'valid_column_count': len(valid_columns),
                            'valid_columns': valid_columns
                        }
                        df_list.append(df_info)
                        
                        log_func(f"è¯»å–Excelï¼ˆheader={actual_header_row}ï¼Œæ•°æ®å¼€å§‹={data_start_row}ï¼‰ï¼Œå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—ï¼Œæœ‰æ•ˆåˆ—å{len(valid_columns)}ä¸ª")
                        
                        # å¦‚æœæ‰¾åˆ°æœ‰æ„ä¹‰çš„åˆ—åï¼Œä¼˜å…ˆé€‰æ‹©
                        if valid_columns and len(valid_columns) >= 3:
                            best_df_info = df_info
                            log_func(f"æ‰¾åˆ°æœ‰æ•ˆåˆ—åï¼Œè·³è¿‡å…¶ä»–é€‰é¡¹")
                            break
                            
                except Exception as e:
                    log_func(f"è¯»å–Excelï¼ˆheader={actual_header_row}ï¼‰å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ„ä¹‰çš„åˆ—åï¼Œä½¿ç”¨æ•°æ®è¡Œæ•°æœ€å¤šçš„
            if not best_df_info and df_list:
                best_df_info = max(df_list, key=lambda x: len(x['dataframe']))
                log_func(f"æœªæ‰¾åˆ°æœ‰æ•ˆåˆ—åï¼Œé€‰æ‹©æ•°æ®æœ€å¤šçš„é€‰é¡¹ï¼ˆheader={best_df_info['header_row']}ï¼‰")
            
        except Exception as e:
            log_func(f"åˆ†æExcelç»“æ„å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            for header_row in [0, 1, 2, 3]:
                try:
                    df = pd.read_excel(file_path, engine='openpyxl', header=header_row)
                    if not df.empty and len(df.columns) > 3:
                        df_list.append({
                            'header_row': header_row,
                            'dataframe': df,
                            'columns': list(df.columns)
                        })
                except:
                    continue
        
        if not df_list:
            log_func("æ— æ³•è¯»å–Excelæ–‡ä»¶ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼")
            return load_excel_data_fallback(file_path, log_func)
        
        # 2. å¦‚æœå·²ç»æœ‰best_df_infoï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä»åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä½³çš„
        if not best_df_info:
            if len(df_list) > 1:
                log_func("æ£€æµ‹åˆ°å¤šä¸ªå¯èƒ½çš„è¡¨å¤´ä½ç½®ï¼Œä½¿ç”¨LLMæ™ºèƒ½é€‰æ‹©...")
                
                # æ„å»ºå¤šä¸ªé€‰é¡¹çš„æè¿°
                options_text = ""
                for i, df_info in enumerate(df_list):
                    valid_cols = len([col for col in df_info['columns'] if not str(col).startswith('Unnamed')])
                    options_text += f"é€‰é¡¹{i+1}ï¼ˆheader={df_info['header_row']}ï¼Œæœ‰æ•ˆåˆ—å={valid_cols}ä¸ªï¼‰: {df_info['columns']}\n"
                
                prompt = f"""
# ä»»åŠ¡ï¼šæ™ºèƒ½è¯†åˆ«Excelè¡¨æ ¼çš„æœ€ä½³æ•°æ®ç»“æ„

Excelæ–‡ä»¶ä¸­æœ‰å¤šä¸ªå¯èƒ½çš„è¡¨å¤´ä½ç½®ï¼Œè¯·é€‰æ‹©æœ€åˆç†çš„ä¸€ä¸ªã€‚

## å¯èƒ½çš„è¡¨å¤´é€‰é¡¹ï¼š
{options_text}

## ä»»åŠ¡è¦æ±‚ï¼š
1. ä¼˜å…ˆé€‰æ‹©æœ‰æœ‰æ•ˆåˆ—åçš„é€‰é¡¹ï¼ˆéUnnamedåˆ—ï¼‰
2. å¦‚æœéƒ½æœ‰Unnamedåˆ—ï¼Œé€‰æ‹©æ•°æ®è¡Œæ•°æœ€å¤šçš„é€‰é¡¹
3. å¯»æ‰¾åŒ…å«ä»¥ä¸‹å…³é”®è¯çš„åˆ—ï¼šæ ‡é¢˜ã€å•†å“ã€åç§°ã€ä»·æ ¼ã€åŒºåŸŸã€é™è´­ã€æœ‰æ•ˆæœŸã€å¤‡æ³¨

## è¿”å›æ ¼å¼ï¼š
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼ˆ1-{len(df_list)}ï¼‰ï¼Œè¡¨ç¤ºé€‰æ‹©çš„é€‰é¡¹ç¼–å·ã€‚

ä¾‹å¦‚ï¼šå¦‚æœé€‰é¡¹2æœ€åˆç†ï¼Œè¿”å›"2"
"""
                
                if llm_client:
                    try:
                        response = llm_client.chat.completions.create(
                            model=LLM_MODEL_ID,
                            messages=[{'role': 'user', 'content': prompt}],
                            stream=False
                        )
                        
                        choice_text = response.choices[0].message.content.strip()
                        log_func(f"--- [LLM Raw Response for Table Structure Selection] ---\n{choice_text}\n" + "-"*30)
                        
                        # å°è¯•æå–æ•°å­—
                        numbers = re.findall(r'\d+', choice_text)
                        if numbers:
                            choice_num = int(numbers[0]) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
                            
                            if 0 <= choice_num < len(df_list):
                                best_df_info = df_list[choice_num]
                                log_func(f"LLMé€‰æ‹©é€‰é¡¹{choice_num+1}ï¼ˆheader={best_df_info['header_row']}ï¼‰")
                            else:
                                log_func(f"LLMè¿”å›æ•°å­—è¶…å‡ºèŒƒå›´ {numbers[0]}ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹")
                        else:
                            log_func(f"LLMè¿”å›ä¸­æœªæ‰¾åˆ°æ•°å­— '{choice_text}'ï¼Œåˆ†æå†…å®¹...")
                            # å¦‚æœæ‰€æœ‰è¡¨å¤´éƒ½æ˜¯Unnamedï¼Œé€‰æ‹©æ•°æ®è¡Œæ•°æœ€å¤šçš„
                            best_df_info = max(df_list, key=lambda x: len(x['dataframe']))
                            log_func(f"åŸºäºæ•°æ®è¡Œæ•°é€‰æ‹©é€‰é¡¹{df_list.index(best_df_info)+1}ï¼ˆheader={best_df_info['header_row']}ï¼‰")
                            
                    except Exception as e:
                        log_func(f"LLMé€‰æ‹©å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹")
                else:
                    log_func("LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹")
            else:
                best_df_info = df_list[0]  # åªæœ‰ä¸€ä¸ªé€‰é¡¹æ—¶ç›´æ¥ä½¿ç”¨
        
        # 3. ä½¿ç”¨é€‰å®šçš„DataFrameè¿›è¡Œå­—æ®µæ˜ å°„
        selected_df = best_df_info['dataframe']
        columns = best_df_info['columns']
        
        log_func(f"é€‰å®šæ•°æ®ç»“æ„ï¼šheader={best_df_info['header_row']}, åˆ—æ•°={len(columns)}")
        
        # 4. æ„å»ºå­—æ®µæ˜ å°„Prompt
        mapping_prompt = f"""
# ä»»åŠ¡ï¼šæ™ºèƒ½æ˜ å°„Excelåˆ—åˆ°æ ‡å‡†å­—æ®µ

Excelåˆ—å: {columns}

éœ€è¦æ˜ å°„åˆ°çš„æ ‡å‡†å­—æ®µï¼š
- å›¢è´­æ ‡é¢˜ï¼ˆå•†å“åç§°ï¼‰
- å”®ä»·ï¼ˆæ•°å­—ä»·æ ¼ï¼‰
- å¯ç”¨åŒºåŸŸï¼ˆé€‚ç”¨åŒºåŸŸï¼‰
- é™è´­ï¼ˆè´­ä¹°é™åˆ¶ï¼‰
- æœ‰æ•ˆæœŸï¼ˆä½¿ç”¨æœŸé™ï¼‰
- å›¢å•å¤‡æ³¨ï¼ˆè¯´æ˜å¤‡æ³¨ï¼‰

## ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†ææ¯ä¸ªExcelåˆ—çš„å†…å®¹ï¼Œåˆ¤æ–­å®ƒå¯¹åº”å“ªä¸ªæ ‡å‡†å­—æ®µ
2. å¦‚æœæŸä¸ªæ ‡å‡†å­—æ®µåœ¨Excelä¸­æ‰¾ä¸åˆ°å¯¹åº”åˆ—ï¼Œè®¾ä¸ºnull
3. è¿”å›ä¸¥æ ¼çš„JSONæ˜ å°„å¯¹è±¡

## è¿”å›æ ¼å¼ï¼š
```json
{{
  "å›¢è´­æ ‡é¢˜": "å¯¹åº”çš„Excelåˆ—åæˆ–null",
  "å”®ä»·": "å¯¹åº”çš„Excelåˆ—åæˆ–null",
  "å¯ç”¨åŒºåŸŸ": "å¯¹åº”çš„Excelåˆ—åæˆ–null",
  "é™è´­": "å¯¹åº”çš„Excelåˆ—åæˆ–null",
  "æœ‰æ•ˆæœŸ": "å¯¹åº”çš„Excelåˆ—åæˆ–null",
  "å›¢å•å¤‡æ³¨": "å¯¹åº”çš„Excelåˆ—åæˆ–null"
}}
```
"""
        
        column_mapping = None
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ—åéƒ½æ˜¯"Unnamed"
        unnamed_count = sum(1 for col in columns if str(col).startswith('Unnamed'))
        if unnamed_count == len(columns):
            log_func("æ£€æµ‹åˆ°æ‰€æœ‰åˆ—å¤´éƒ½æ˜¯Unnamedï¼Œä½¿ç”¨æ•°æ®å†…å®¹åˆ†æ...")
            
            # è·å–å‰3è¡Œæ•°æ®æ ·æœ¬è¿›è¡Œåˆ†æ
            sample_data = []
            for i in range(min(3, len(selected_df))):
                row_data = [str(selected_df.iloc[i, j]) if j < len(selected_df.columns) else "" for j in range(len(columns))]
                sample_data.append(row_data)
            
            # æ„å»ºåŸºäºæ•°æ®å†…å®¹çš„åˆ†æPrompt
            content_mapping_prompt = f"""
# ä»»åŠ¡ï¼šåŸºäºæ•°æ®å†…å®¹æ™ºèƒ½æ˜ å°„Excelåˆ—

Excelåˆ—: {columns}

å‰3è¡Œæ•°æ®æ ·æœ¬:
{chr(10).join([f'ç¬¬{i+1}è¡Œ: {row}' for i, row in enumerate(sample_data)])}

## ä»»åŠ¡è¦æ±‚ï¼š
åŸºäºæ•°æ®å†…å®¹åˆ¤æ–­æ¯åˆ—çš„å«ä¹‰ï¼Œå¹¶æ˜ å°„åˆ°ä»¥ä¸‹æ ‡å‡†å­—æ®µï¼š
- å›¢è´­æ ‡é¢˜ï¼ˆå•†å“åç§°ï¼Œé€šå¸¸åŒ…å«å•†å“ã€å¥—é¤ã€æ ‡é¢˜ç­‰å…³é”®è¯ï¼‰
- å”®ä»·ï¼ˆæ•°å­—ä»·æ ¼ï¼Œå¯èƒ½æ˜¯å°æ•°æˆ–æ•´æ•°ï¼‰
- å¯ç”¨åŒºåŸŸï¼ˆæè¿°æ€§æ–‡æœ¬ï¼Œä½ç½®ã€åŒºåŸŸç›¸å…³ï¼‰
- é™è´­ï¼ˆè´­ä¹°é™åˆ¶è¯´æ˜ï¼‰
- æœ‰æ•ˆæœŸï¼ˆæ—¶é—´æœŸé™æè¿°ï¼‰
- å›¢å•å¤‡æ³¨ï¼ˆè¡¥å……è¯´æ˜ï¼‰

## è¿”å›æ ¼å¼ï¼š
åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼š
```json
{{
  "å›¢è´­æ ‡é¢˜": "åˆ—ç´¢å¼•(0-11)",
  "å”®ä»·": "åˆ—ç´¢å¼•æˆ–null",
  "å¯ç”¨åŒºåŸŸ": "åˆ—ç´¢å¼•æˆ–null",
  "é™è´­": "åˆ—ç´¢å¼•æˆ–null",
  "æœ‰æ•ˆæœŸ": "åˆ—ç´¢å¼•æˆ–null",
  "å›¢å•å¤‡æ³¨": "åˆ—ç´¢å¼•æˆ–null"
}}
```
"""
            
            if llm_client:
                try:
                    response = llm_client.chat.completions.create(
                        model=LLM_MODEL_ID,
                        messages=[{'role': 'user', 'content': content_mapping_prompt}],
                        stream=False
                    )
                    
                    response_text = response.choices[0].message.content.strip()
                    log_func(f"--- [LLM Raw Response for Content-Based Mapping] ---\n{response_text}\n" + "-"*30)
                    json_match = re.search(r'\{[\s\S]*\}', response_text)
                    if json_match:
                        cleaned_response = json_match.group(0)
                        column_mapping = json.loads(cleaned_response)
                    else:
                        raise json.JSONDecodeError("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONå¯¹è±¡", response_text, 0)
                    log_func(f"åŸºäºæ•°æ®å†…å®¹çš„LLMæ˜ å°„ç»“æœ: {column_mapping}")
                    
                    # è½¬æ¢åˆ—ç´¢å¼•ä¸ºå®é™…åˆ—å
                    index_mapping = {}
                    for field, idx_str in column_mapping.items():
                        if idx_str and str(idx_str).isdigit():
                            idx = int(idx_str)
                            if 0 <= idx < len(columns):
                                index_mapping[field] = columns[idx]
                            else:
                                index_mapping[field] = None
                        else:
                            index_mapping[field] = None
                    
                    column_mapping = index_mapping
                    log_func(f"è½¬æ¢ä¸ºåˆ—åæ˜ å°„: {column_mapping}")
                    
                except Exception as e:
                    log_func(f"åŸºäºæ•°æ®å†…å®¹çš„LLMæ˜ å°„å¤±è´¥: {e}")
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•åŸå§‹çš„åˆ—åæ˜ å°„
        if not column_mapping and llm_client:
            try:
                response = llm_client.chat.completions.create(
                    model=LLM_MODEL_ID,
                    messages=[{'role': 'user', 'content': mapping_prompt}],
                    stream=False
                )
                
                response_text = response.choices[0].message.content.strip()
                log_func(f"--- [LLM Raw Response for Column Name Mapping] ---\n{response_text}\n" + "-"*30)
                json_match = re.search(r'\{[\s\S]*\}', response_text)
                if json_match:
                    cleaned_response = json_match.group(0)
                    column_mapping = json.loads(cleaned_response)
                else:
                    raise json.JSONDecodeError("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONå¯¹è±¡", response_text, 0)
                log_func(f"LLMå­—æ®µæ˜ å°„ç»“æœ: {column_mapping}")
                
            except Exception as e:
                log_func(f"LLMå­—æ®µæ˜ å°„å¤±è´¥: {e}")
        
        # 5. å¦‚æœLLMæ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ˜ å°„
        if not column_mapping:
            log_func("LLMå®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨å¼ºåˆ¶å†…å®¹åˆ†æ...")
            # å¼ºåˆ¶åŸºäºæ•°æ®å†…å®¹è¿›è¡Œæ™ºèƒ½æ˜ å°„
            column_mapping = force_content_based_mapping(selected_df, columns, log_func)
            log_func(f"å¼ºåˆ¶å†…å®¹æ˜ å°„ç»“æœ: {column_mapping}")
        
        # 6. åº”ç”¨æ˜ å°„å¹¶æ¸…ç†æ•°æ®
        mapped_data = apply_column_mapping(selected_df, column_mapping, log_func)
        
        log_func(f"æ™ºèƒ½è§£æå®Œæˆï¼Œå…±{mapped_data['record_count']}æ¡è®°å½•")
        return mapped_data['records']
        
    except Exception as e:
        log_func(f"[Error] æ™ºèƒ½è§£æå¤±è´¥: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼")
        return load_excel_data_fallback(file_path, log_func)

def force_content_based_mapping(df, columns, log_func):
    """å¼ºåˆ¶åŸºäºæ•°æ®å†…å®¹è¿›è¡Œå­—æ®µæ˜ å°„"""
    log_func("æ‰§è¡Œå¼ºåˆ¶å†…å®¹æ˜ å°„...")
    
    # è·å–å‰5è¡Œæ•°æ®æ ·æœ¬
    sample_data = []
    for i in range(min(5, len(df))):
        row_data = []
        for j in range(len(columns)):
            value = df.iloc[i, j] if j < len(df.columns) else ""
            row_data.append(str(value))
        sample_data.append(row_data)
    
    # åˆ†ææ¯åˆ—çš„æ•°æ®ç‰¹å¾
    column_analysis = {}
    for col_idx, col_name in enumerate(columns):
        if col_idx >= len(sample_data[0]):
            continue
            
        values = [row[col_idx] for row in sample_data if col_idx < len(row)]
        non_empty_values = [v for v in values if v and str(v).strip() and str(v) != 'nan']
        
        analysis = {
            'column_name': col_name,
            'total_values': len(values),
            'non_empty_count': len(non_empty_values),
            'sample_values': non_empty_values[:3],  # å‰3ä¸ªéç©ºå€¼
            'is_numeric': True,  # é»˜è®¤å‡è®¾æ˜¯æ•°å­—
            'is_text': True,     # é»˜è®¤å‡è®¾æ˜¯æ–‡æœ¬
            'looks_like_title': False,
            'looks_like_price': False,
            'looks_like_area': False,
            'looks_like_limit': False,
            'looks_like_validity': False,
            'looks_like_remark': False
        }
        
        # æ•°å­—ç‰¹å¾åˆ†æ
        numeric_count = 0
        for v in non_empty_values:
            try:
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                if isinstance(v, str):
                    # æ¸…ç†ä»·æ ¼æ ¼å¼
                    clean_v = v.replace('Â¥', '').replace(',', '').strip()
                    if clean_v and clean_v.replace('.', '').replace('-', '').isdigit():
                        numeric_count += 1
                elif isinstance(v, (int, float)) and not pd.isna(v):
                    numeric_count += 1
            except:
                pass
        
        analysis['is_numeric'] = numeric_count > len(non_empty_values) * 0.6  # 60%ä»¥ä¸Šæ˜¯æ•°å­—
        
        # æ–‡æœ¬ç‰¹å¾åˆ†æ
        text_samples = ' '.join(non_empty_values).lower()
        
        # æ ‡é¢˜ç‰¹å¾ï¼šåŒ…å«å•†å“ç›¸å…³è¯æ±‡
        title_keywords = ['å¥—é¤', 'å•†å“', 'ç½‘è´¹', 'åŒ…æ—¶', 'ä¼šå‘˜', 'æ–°å®¢', 'è€å®¢', 'ç‰¹ä»·', 'ä¼˜æƒ ', 'æ´»åŠ¨', 'å›¢è´­']
        analysis['looks_like_title'] = any(keyword in text_samples for keyword in title_keywords)
        
        # ä»·æ ¼ç‰¹å¾ï¼šæ•°å­—ä¸”åŒ…å«ä»·æ ¼ç›¸å…³è¯æ±‡
        price_keywords = ['å…ƒ', 'ä»·æ ¼', 'å”®ä»·', 'ä¼˜æƒ ä»·']
        analysis['looks_like_price'] = analysis['is_numeric'] or any(keyword in text_samples for keyword in price_keywords)
        
        # åŒºåŸŸç‰¹å¾
        area_keywords = ['åŒºåŸŸ', 'ä½ç½®', 'åŒºåŸŸ', 'é€‚ç”¨', 'å¤§å…', 'åŒ…é—´', 'å•äºº', 'åŒäºº']
        analysis['looks_like_area'] = any(keyword in text_samples for keyword in area_keywords)
        
        # é™è´­ç‰¹å¾
        limit_keywords = ['é™è´­', 'é™åˆ¶', 'è´­ä¹°', 'æ¬¡æ•°', 'äººå‡']
        analysis['looks_like_limit'] = any(keyword in text_samples for keyword in limit_keywords)
        
        # æœ‰æ•ˆæœŸç‰¹å¾
        validity_keywords = ['æœ‰æ•ˆæœŸ', 'æœŸé™', 'å¤©', 'æœˆ', 'æ—¥', 'å¹´']
        analysis['looks_like_validity'] = any(keyword in text_samples for keyword in validity_keywords)
        
        # å¤‡æ³¨ç‰¹å¾
        remark_keywords = ['å¤‡æ³¨', 'è¯´æ˜', 'å¤‡æ³¨', 'æ³¨æ„', 'é¡»çŸ¥', 'ä½¿ç”¨']
        analysis['looks_like_remark'] = any(keyword in text_samples for keyword in remark_keywords)
        
        column_analysis[col_idx] = analysis
    
    # æ ¹æ®ç‰¹å¾è¿›è¡Œå­—æ®µæ˜ å°„
    mapping = {
        "å›¢è´­æ ‡é¢˜": None,
        "å”®ä»·": None,
        "å¯ç”¨åŒºåŸŸ": None,
        "é™è´­": None,
        "æœ‰æ•ˆæœŸ": None,
        "å›¢å•å¤‡æ³¨": None
    }
    
    # æ˜ å°„é€»è¾‘
    for col_idx, analysis in column_analysis.items():
        col_name = analysis['column_name']
        
        # æ ‡é¢˜æ˜ å°„ - ä¼˜å…ˆæ–‡æœ¬ä¸”æœ‰æ ‡é¢˜ç‰¹å¾çš„
        if mapping["å›¢è´­æ ‡é¢˜"] is None and (analysis['looks_like_title'] or (not analysis['is_numeric'] and len(analysis['sample_values']) > 0)):
            mapping["å›¢è´­æ ‡é¢˜"] = col_name
            log_func(f"æ˜ å°„æ ‡é¢˜åˆ—: {col_name} (ç‰¹å¾: æ ‡é¢˜={analysis['looks_like_title']}, æ•°å­—={analysis['is_numeric']})")
            continue
            
        # ä»·æ ¼æ˜ å°„ - ä¼˜å…ˆæ•°å­—ç‰¹å¾
        if mapping["å”®ä»·"] is None and analysis['looks_like_price']:
            mapping["å”®ä»·"] = col_name
            log_func(f"æ˜ å°„ä»·æ ¼åˆ—: {col_name} (ç‰¹å¾: ä»·æ ¼={analysis['looks_like_price']}, æ•°å­—={analysis['is_numeric']})")
            continue
            
        # åŒºåŸŸæ˜ å°„ - åŒºåŸŸç‰¹å¾ä¼˜å…ˆ
        if mapping["å¯ç”¨åŒºåŸŸ"] is None and analysis['looks_like_area']:
            mapping["å¯ç”¨åŒºåŸŸ"] = col_name
            log_func(f"æ˜ å°„åŒºåŸŸåˆ—: {col_name}")
            continue
            
        # é™è´­æ˜ å°„ - é™è´­ç‰¹å¾ä¼˜å…ˆ
        if mapping["é™è´­"] is None and analysis['looks_like_limit']:
            mapping["é™è´­"] = col_name
            log_func(f"æ˜ å°„é™è´­åˆ—: {col_name}")
            continue
            
        # æœ‰æ•ˆæœŸæ˜ å°„ - æœ‰æ•ˆæœŸç‰¹å¾ä¼˜å…ˆ
        if mapping["æœ‰æ•ˆæœŸ"] is None and analysis['looks_like_validity']:
            mapping["æœ‰æ•ˆæœŸ"] = col_name
            log_func(f"æ˜ å°„æœ‰æ•ˆæœŸåˆ—: {col_name}")
            continue
            
        # å¤‡æ³¨æ˜ å°„ - å…¶ä»–æ–‡æœ¬åˆ—
        if mapping["å›¢å•å¤‡æ³¨"] is None and not analysis['is_numeric'] and analysis['sample_values']:
            mapping["å›¢å•å¤‡æ³¨"] = col_name
            log_func(f"æ˜ å°„å¤‡æ³¨åˆ—: {col_name}")
            continue
    
    # å¦‚æœè¿˜æœ‰æœªæ˜ å°„çš„å­—æ®µï¼Œå°è¯•æ™ºèƒ½åˆ†é…
    unmapped_fields = [field for field, col in mapping.items() if col is None]
    unmapped_cols = [col_idx for col_idx, analysis in column_analysis.items() if analysis['column_name'] not in mapping.values()]
    
    for field in unmapped_fields:
        if unmapped_cols:
            col_idx = unmapped_cols.pop(0)
            col_name = column_analysis[col_idx]['column_name']
            mapping[field] = col_name
            log_func(f"æ™ºèƒ½åˆ†é… {field} -> {col_name}")
    
    return mapping

def heuristic_column_mapping(columns):
    """å¯å‘å¼å­—æ®µæ˜ å°„"""
    mapping = {
        "å›¢è´­æ ‡é¢˜": None,
        "å”®ä»·": None,
        "å¯ç”¨åŒºåŸŸ": None,
        "é™è´­": None,
        "æœ‰æ•ˆæœŸ": None,
        "å›¢å•å¤‡æ³¨": None
    }
    
    # å…³é”®è¯æ˜ å°„
    keyword_map = {
        "å›¢è´­æ ‡é¢˜": ["æ ‡é¢˜", "å•†å“", "åç§°", "äº§å“", "å¥—é¤", "title", "name"],
        "å”®ä»·": ["å”®ä»·", "ä»·æ ¼", "é‡‘é¢", "price", "ç°ä»·"],
        "å¯ç”¨åŒºåŸŸ": ["åŒºåŸŸ", "åœ°åŒº", "é€‚ç”¨", "ä½ç½®", "area", "location"],
        "é™è´­": ["é™è´­", "é™åˆ¶", "è´­ä¹°", "limit"],
        "æœ‰æ•ˆæœŸ": ["æœ‰æ•ˆæœŸ", "æœŸé™", "å¤©æ•°", "validity", "expire"],
        "å›¢å•å¤‡æ³¨": ["å¤‡æ³¨", "è¯´æ˜", "note", "remark", "æè¿°", "description"]
    }
    
    for field, keywords in keyword_map.items():
        for col in columns:
            col_lower = str(col).lower()
            if any(keyword.lower() in col_lower for keyword in keywords):
                mapping[field] = col
                break
    
    return mapping

def apply_column_mapping(df, column_mapping, log_func):
    """åº”ç”¨åˆ—æ˜ å°„å¹¶æ¸…ç†æ•°æ®"""
    records = []
    
    # æ•°æ®æ¸…ç†
    df_filled = df.fillna('')
    
    for _, row in df_filled.iterrows():
        record = {}
        for field, excel_col in column_mapping.items():
            if excel_col and excel_col in df.columns:
                value = row[excel_col]
                # ç‰¹æ®Šå¤„ç†ä»·æ ¼å­—æ®µ
                if field == "å”®ä»·" and value:
                    try:
                        # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                        if isinstance(value, str):
                            # ç§»é™¤è´§å¸ç¬¦å·å’Œé€—å·
                            value = value.replace('Â¥', '').replace(',', '').strip()
                        record[field] = float(value)
                    except (ValueError, TypeError):
                        record[field] = 0.0
                else:
                    record[field] = str(value) if value else ""
            else:
                record[field] = "" if field != "å”®ä»·" else 0.0
        
        # åªæ·»åŠ æœ‰å®é™…å†…å®¹çš„è®°å½•
        if any(str(v).strip() for v in record.values() if v != 0.0):
            records.append(record)
    
    return {
        "records": records,
        "record_count": len(records),
        "mapping_used": column_mapping
    }

def load_excel_data_fallback(file_path, log_func):
    """ä¼ ç»ŸExcelåŠ è½½æ–¹å¼ä½œä¸ºå›é€€æ–¹æ¡ˆ"""
    try:
        df = pd.read_excel(file_path, engine='openpyxl', header=1)
        expected_columns = {df.columns[4]: 'å›¢è´­æ ‡é¢˜', df.columns[5]: 'å”®ä»·', df.columns[6]: 'å¯ç”¨åŒºåŸŸ', df.columns[7]: 'é™è´­', df.columns[8]: 'æœ‰æ•ˆæœŸ', df.columns[9]: 'å›¢å•å¤‡æ³¨'}
        df.rename(columns=expected_columns, inplace=True)
        df.fillna('', inplace=True)
        data = df.to_dict('records')
        log_func(f"[Success] ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æˆåŠŸåŠ è½½ {len(data)} æ¡Excelæ•°æ®ã€‚")
        return data
    except Exception as e:
        log_func(f"[Error] ä¼ ç»Ÿæ–¹å¼åŠ è½½Excelæ–‡ä»¶ä¹Ÿå¤±è´¥: {e}")
    return None

# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
def load_excel_data(file_path, log_func):
    """æ™ºèƒ½Excelæ•°æ®åŠ è½½å‡½æ•°ï¼ˆæ”¯æŒLLMå¢å¼ºï¼‰"""
    return intelligent_load_excel_data(file_path, log_func, {})

def match_products_with_llm(douyin_products, excel_data, log_func, cache):
    if not llm_client: return None
    log_func("--- å¼€å§‹ä½¿ç”¨LLMæ™ºèƒ½åŒ¹é…å¥—é¤ ---")
    # å‡†å¤‡æ›´è¯¦ç»†çš„æ•°æ®ç»™LLMï¼Œä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§
    douyin_product_details_for_llm = [
        {"name": p['name'], "price": p['price'], "origin_price": p.get('origin_price', '0.00')}
        for p in douyin_products
    ]
    excel_product_details_for_llm = [
        {"å›¢è´­æ ‡é¢˜": p['å›¢è´­æ ‡é¢˜'], "å”®ä»·": p.get('å”®ä»·', 0.0), "ç½‘è´¹": p.get('ç½‘è´¹', 0.0), "åŒºåŸŸ": p.get('å¯ç”¨åŒºåŸŸ', '')}
        for p in excel_data
    ]

    prompt = f"""
# ä»»åŠ¡ï¼šæ™ºèƒ½åŒ¹é…æŠ–éŸ³å•†å“ä¸Excelå•†å“

è¯·ä¸ºâ€œæŠ–éŸ³å•†å“åˆ—è¡¨â€ä¸­çš„æ¯ä¸€ä¸ªå•†å“ï¼Œåœ¨â€œExcelå•†å“åˆ—è¡¨â€ä¸­æ‰¾åˆ°å”¯ä¸€ä¸”æœ€ç²¾ç¡®çš„åŒ¹é…é¡¹ã€‚

## åŒ¹é…åŸåˆ™ (æå…¶é‡è¦):
1.  **æ ¸å¿ƒåŒ¹é…**: é¦–å…ˆå¿…é¡»æ ¹æ®å•†å“çš„æ ¸å¿ƒå†…å®¹è¿›è¡ŒåŒ¹é…ã€‚ä¾‹å¦‚ï¼ŒæŠ–éŸ³çš„â€œã€æ–°ä¼šå‘˜ã€‘108ç½‘è´¹â€åº”è¯¥åŒ¹é…Excelä¸­çš„â€œã€æ–°ä¼šå‘˜ã€‘108ç½‘è´¹â€ã€‚
2.  **ä»·æ ¼éªŒè¯**: åœ¨æ ¸å¿ƒå†…å®¹åŒ¹é…çš„åŸºç¡€ä¸Šï¼Œå¿…é¡»ä¸¥æ ¼æ¯”è¾ƒä»·æ ¼ã€‚æŠ–éŸ³å•†å“çš„ä»·æ ¼ (`price`) å¿…é¡»ä¸Excelä¸­å¯¹åº”çš„â€œå”®ä»·â€**å‡ ä¹å®Œå…¨ç›¸ç­‰**ã€‚
3.  **å†…å®¹ä¸ä»·æ ¼ç»“åˆ**: ç»¼åˆæ ¸å¿ƒå†…å®¹å’Œä»·æ ¼è¿›è¡ŒåŒé‡éªŒè¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæŠ–éŸ³å•†å“å«â€œã€ä¸ŠåˆåŒ…ã€‘æ— çƒŸåŒºâ€ï¼Œä»·æ ¼æ˜¯10.5å…ƒï¼Œé‚£ä¹ˆå®ƒåº”è¯¥åŒ¹é…åˆ°Excelä¸­â€œå›¢è´­æ ‡é¢˜â€ä¸ºâ€œã€ä¸ŠåˆåŒ…ã€‘æ— çƒŸåŒºâ€ä¸”â€œå”®ä»·â€ä¸º10.5çš„é‚£ä¸€è¡Œã€‚
4.  **å¤„ç†æ¨¡ç³Šæƒ…å†µ**: å¦‚æœå¤šä¸ªæŠ–éŸ³å•†å“ï¼ˆä¾‹å¦‚â€œã€ç¬å½±å¿…æ€åˆ¸ã€‘100å…ƒç½‘è´¹ï¼ˆæ–°ä¼šå‘˜ï¼‰â€å’Œâ€œã€æ–°é¼ é¼ åˆ¸Aã€‘100å…ƒç½‘è´¹â€ï¼‰éƒ½èƒ½æ¨¡ç³ŠåŒ¹é…åˆ°åŒä¸€ä¸ªExcelé¡¹ï¼ˆä¾‹å¦‚â€œã€æ–°ä¼šå‘˜ã€‘108ç½‘è´¹â€ï¼‰ï¼Œä½ éœ€è¦æ ¹æ®**ä»·æ ¼**æ¥åŒºåˆ†ã€‚å¦‚æœä»·æ ¼ä¹Ÿç›¸ä¼¼ï¼Œåˆ™é€‰æ‹©å†…å®¹æ›´æ¥è¿‘çš„é‚£ä¸ªã€‚å¦‚æœæ— æ³•åŒºåˆ†ï¼Œåˆ™å¯ä»¥å°†å…¶ä¸­ä¸€ä¸ªè®¾ä¸ºnullã€‚
5.  **æ‰¾ä¸åˆ°åˆ™ä¸ºnull**: å¦‚æœåœ¨Excelåˆ—è¡¨ä¸­æ‰¾ä¸åˆ°ä»»ä½•æ»¡è¶³ä¸Šè¿°æ¡ä»¶çš„åŒ¹é…é¡¹ï¼Œå¯¹åº”çš„å€¼å¿…é¡»æ˜¯ `null`ã€‚

## æ•°æ®åˆ—è¡¨:

### æŠ–éŸ³å•†å“åˆ—è¡¨ (åŒ…å«åç§°ã€ç°ä»·ã€åŸä»·):
{json.dumps(douyin_product_details_for_llm, ensure_ascii=False, indent=2)}

### Excelå•†å“åˆ—è¡¨ (åŒ…å«å›¢è´­æ ‡é¢˜ã€å”®ä»·ã€ç½‘è´¹ã€åŒºåŸŸ):
{json.dumps(excel_product_details_for_llm, ensure_ascii=False, indent=2)}

## è¿”å›æ ¼å¼:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œé”®æ˜¯æŠ–éŸ³å•†å“**å®Œæ•´çš„`name`**ï¼Œå€¼æ˜¯åŒ¹é…åˆ°çš„Excelå•†å“**å®Œæ•´çš„`å›¢è´­æ ‡é¢˜`**ã€‚ä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚

```json
{{
  "æŠ–éŸ³å•†å“åç§°1": "åŒ¹é…åˆ°çš„Excelå›¢è´­æ ‡é¢˜1",
  "æŠ–éŸ³å•†å“åç§°2": null,
  "æŠ–éŸ³å•†å“åç§°3": "åŒ¹é…åˆ°çš„Excelå›¢è´­æ ‡é¢˜3"
}}
```
"""
    
    cache_key = hashlib.sha256(prompt.encode('utf-8')).hexdigest()
    if cache_key in cache:
        log_func(f"[Cache Hit] å‘ç°ç›¸åŒè¯·æ±‚çš„ç¼“å­˜ç»“æœï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ã€‚")
        return cache[cache_key]

    log_func("[Cache Miss] æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå‘èµ·æ–°çš„LLMè¯·æ±‚ã€‚")
    try:
        log_func("æ­£åœ¨è°ƒç”¨LLM API...")
        log_func(f"--- [DEBUG] å‘é€ç»™LLMçš„Prompt ---\n{prompt[:500]}...\n" + "-"*30)
        response = llm_client.chat.completions.create(model=LLM_MODEL_ID, messages=[{'role': 'user', 'content': prompt}], stream=True)
        full_response = "".join(chunk.choices[0].delta.content for chunk in response if chunk.choices[0].delta.content)
        log_func(f"--- [LLM Raw Response for Product Matching] ---\n{full_response}\n" + "-"*30)
        json_match = re.search(r'\{[\s\S]*\}', full_response)
        if json_match:
            cleaned_response = json_match.group(0)
            match_result = json.loads(cleaned_response)
        else:
            raise json.JSONDecodeError("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONå¯¹è±¡", full_response, 0)
        log_func("[Success] LLMæ™ºèƒ½åŒ¹é…æˆåŠŸï¼")
        cache[cache_key] = match_result
        log_func(f"å·²å°†æœ¬æ¬¡ç»“æœå­˜å…¥ç¼“å­˜ï¼ŒKey: {cache_key[:10]}...")
        return match_result
    except Exception as e:
        log_func(f"[Error] LLMæ™ºèƒ½åŒ¹é…è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    return None

def analyze_text_for_actions(text_input, douyin_products, log_func, cache):
    if not llm_client:
        log_func("[Error] LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ã€‚")
        return None
        
    log_func("--- å¼€å§‹ä½¿ç”¨LLMåˆ†ææ–‡æœ¬æŒ‡ä»¤ ---")
    
    # è½¬æ¢ä»·æ ¼ä¸ºæ•°å­—ï¼Œä»¥ä¾¿AIæ›´å¥½åœ°å¤„ç†
    simple_product_list_for_llm = []
    for p in douyin_products:
        try:
            price = float(p.get('price', 0))
            origin_price = float(p.get('origin_price', 0))
        except (ValueError, TypeError):
            price = 0.0
            origin_price = 0.0
        simple_product_list_for_llm.append({
            "name": p.get('name'),
            "price": price,
            "origin_price": origin_price
        })

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ–éŸ³å›¢è´­è¿è¥åŠ©ç†ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤å’Œå½“å‰çš„æŠ–éŸ³çº¿ä¸Šå•†å“åˆ—è¡¨ï¼Œåˆ†æå‡ºéœ€è¦æ‰§è¡Œçš„æ–°å¢ã€ä¿®æ”¹ã€ä¸‹æ¶æ“ä½œã€‚

# å½“å‰çº¿ä¸Šå•†å“åˆ—è¡¨ (åŒ…å«åç§°ã€ç°ä»·å’ŒåŸä»·):
{json.dumps(simple_product_list_for_llm, ensure_ascii=False, indent=2)}

# ç”¨æˆ·æŒ‡ä»¤:
---
{text_input}
---

# ä»»åŠ¡è¦æ±‚:
1.  **åˆ†ææŒ‡ä»¤**: ä»”ç»†é˜…è¯»ç”¨æˆ·æŒ‡ä»¤ï¼Œè¯†åˆ«å‡ºä¸‰ç§æ“ä½œï¼š`add` (æ–°å¢), `update` (ä¿®æ”¹), `delete` (ä¸‹æ¶)ã€‚
    *   **ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœç”¨æˆ·æŒ‡ä»¤ä¸­æ˜ç¡®åŒ…å«â€œæ–°å»ºâ€æˆ–â€œæ–°å¢â€å…³é”®è¯ï¼Œåˆ™åº”å°†æ‰€æœ‰å†…å®¹éƒ½è§£æä¸º `add` æ“ä½œï¼Œä¸è¦å°è¯•è¿›è¡Œ `update` æˆ– `delete` åŒ¹é…ã€‚**

2.  **æ™ºèƒ½åŒ¹é…å•†å“ (æ ¸å¿ƒä»»åŠ¡)**:
    *   å¯¹äº `update` å’Œ `delete` æ“ä½œï¼ˆ**ä»…åœ¨æŒ‡ä»¤ä¸å«â€œæ–°å»ºâ€æˆ–â€œæ–°å¢â€æ—¶æ‰§è¡Œ**ï¼‰ï¼Œä½ å¿…é¡»åœ¨â€œå½“å‰çº¿ä¸Šå•†å“åˆ—è¡¨â€ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„å•†å“ã€‚åŒ¹é…ä¸åº”æ˜¯ç®€å•çš„æ–‡æœ¬ç›¸ç­‰ï¼Œè€Œåº”æ˜¯åŸºäºæ ¸å¿ƒå†…å®¹ã€ç°ä»· (`price`) å’ŒåŸä»· (`origin_price`) çš„**æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…**ã€‚
    *   **åŒ¹é…ç¤ºä¾‹**: ç”¨æˆ·æŒ‡ä»¤ `ã€9.9å¾—60ã€‘æ¢æˆã€19.9å¾—100ã€‘` åº”è¯¥èƒ½å‡†ç¡®åŒ¹é…åˆ°çº¿ä¸Šå•†å“ `{{"name": "ã€å¼€ä¸šæ–°ä¼šå‘˜ã€‘9.9å¾—60ç½‘è´¹", "price": 9.9, "origin_price": 60.0}}`ï¼Œå› ä¸ºå®ƒçš„æ ¸å¿ƒéƒ¨åˆ† `9.9å¾—60` ä¸ä»·æ ¼ `9.9` å’Œ `60.0` é«˜åº¦ç›¸å…³ã€‚
    *   å¯¹äº `delete` æ“ä½œï¼Œå¦‚æœæŒ‡ä»¤åªæœ‰ä»·æ ¼ï¼ˆå¦‚ `59.9ä¸‹æ¶`ï¼‰ï¼Œåº”æ ¹æ® `price` å­—æ®µè¿›è¡ŒåŒ¹é…ã€‚
    *   å¯¹äº `add` æ“ä½œï¼Œæ˜¯å…¨æ–°çš„å•†å“ï¼Œä¸éœ€è¦åŒ¹é…ã€‚

3.  **æå–å¹¶æ„å»ºä¿¡æ¯**:
    *   å¯¹äº `add` æ“ä½œï¼Œæ ¹æ®æŒ‡ä»¤æå–å¹¶æ„å»ºä¸€ä¸ªå®Œæ•´å•†å“ä¿¡æ¯å¯¹è±¡ã€‚
        *   **å”®ä»·æå–**: å¿…é¡»ä»æŒ‡ä»¤ä¸­æå–å‡ºæ˜ç¡®çš„â€œå”®ä»·â€ã€‚
        *   **å¥—é¤ç±»å‹æå–**: ä»æŒ‡ä»¤ä¸­è¯†åˆ«å¥—é¤çš„æ ¸å¿ƒç±»å‹ï¼Œä¾‹å¦‚â€œç½‘è´¹â€æˆ–â€œåŒ…æ—¶â€ã€‚å°†ç»“æœæ”¾å…¥ `commodity_type` å­—æ®µã€‚
        *   **åŸä»·è®¡ç®—è§„åˆ™**:
            *   å¦‚æœ `commodity_type` æ˜¯ â€œ**ç½‘è´¹**â€ï¼Œåˆ™ä»æŒ‡ä»¤ä¸­ç›´æ¥æå–â€œåŸä»·â€ã€‚ä¾‹å¦‚ï¼ŒæŒ‡ä»¤â€œ19.9å¾—50ç½‘è´¹â€ä¸­ï¼Œâ€œåŸä»·â€æ˜¯50ã€‚
            *   å¦‚æœ `commodity_type` æ˜¯ â€œ**åŒ…æ—¶**â€ï¼Œåˆ™â€œåŸä»·â€åº”æ ¹æ®â€œå”®ä»·â€**ä¼°ç®—**ï¼Œè§„åˆ™ä¸º **å”®ä»·çš„3å€**ã€‚ä¾‹å¦‚ï¼ŒæŒ‡ä»¤â€œ3å°æ—¶åŒ…æ—¶ï¼Œä»·æ ¼9.8â€ï¼Œ`å”®ä»·`æ˜¯9.8ï¼Œé‚£ä¹ˆ`åŸä»·`å°±åº”è¯¥æ˜¯ 9.8 * 3 = 29.4ã€‚**ç»å¯¹ä¸è¦**å°†â€œ3å°æ—¶â€è¿™ä¸ªæ—¶é•¿é”™è¯¯åœ°è¯†åˆ«ä¸ºåŸä»·ã€‚
        *   **ç”¨æˆ·ç±»å‹æå–**: ä»æŒ‡ä»¤ä¸­è¯†åˆ«ç”¨æˆ·ç±»å‹ï¼Œå¦‚â€œæ–°å®¢â€ã€â€œæ–°ä¼šå‘˜â€åº”æå–ä¸º "æ–°å®¢"ï¼›å¦‚â€œè€å®¢â€ã€â€œä¼šå‘˜â€åº”æå–ä¸º "è€å®¢"ï¼›å¦‚æœæœªæåŠï¼Œåˆ™ä¸º "ä¸é™åˆ¶"ã€‚å°†ç»“æœæ”¾å…¥ `member_type` å­—æ®µã€‚
        *   **é€‚ç”¨ä½ç½®æå–**: è¯¦ç»†åˆ†ææŒ‡ä»¤ä¸­æè¿°å¥—é¤çš„å…³é”®è¯ï¼Œä¾‹å¦‚â€œå•äººåŒäººåŒ…â€ã€â€œè±ªåç”µç«åŒ…é—´â€ã€â€œå¤§å…â€ç­‰ã€‚å¦‚æœæŒ‡ä»¤ä¸­æ˜ç¡®æåˆ°äº†æˆ¿é—´ç±»å‹æˆ–ä½ç½®ï¼Œå°±æå–å®ƒã€‚å¦‚æœæœªæåŠä»»ä½•å…·ä½“ä½ç½®ï¼Œåˆ™é»˜è®¤ä¸ºâ€œå¤§å…â€ã€‚å°†ç»“æœæ”¾å…¥ `applicable_location` å­—æ®µã€‚
        *   **æ ‡é¢˜ç”Ÿæˆ**: ä½ éœ€è¦æ ¹æ®æå–å‡ºçš„ä¿¡æ¯ï¼Œä¸ºâ€œå›¢è´­æ ‡é¢˜â€ç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€è§„èŒƒçš„åç§°ã€‚
            *   **ç½‘è´¹æ ‡é¢˜æ ¼å¼**: ä¿æŒ `ã€ç”¨æˆ·ç±»å‹ã€‘å”®ä»·å¾—åŸä»·å†…å®¹` æ ¼å¼ã€‚ä¾‹å¦‚ï¼š`ã€æ–°å®¢ä¸“äº«ã€‘42.9å¾—100å…ƒç½‘è´¹`ã€‚
            *   **åŒ…æ—¶æ ‡é¢˜æ ¼å¼**: ç®€åŒ–ä¸º `æ—¶é•¿ + æ›´è¯¦ç»†çš„å¥—é¤æè¿°`ã€‚ä¾‹å¦‚ï¼ŒæŒ‡ä»¤ â€œåŒ…æ—¶ å•äººåŒäººåŒ…å¥—é¤ 5 å°æ—¶ï¼Œä»·æ ¼ 39.9â€ï¼Œæ ‡é¢˜åº”ç”Ÿæˆä¸º `5å°æ—¶å•äººåŒäººåŒ…å¥—é¤`ã€‚
        *   **å…¶ä»–å­—æ®µ**: å¦‚æœæŒ‡ä»¤ä¸­åŒ…å«ï¼Œä¹Ÿè¯·æå– "å¯ç”¨åŒºåŸŸ", "é™è´­", "æœ‰æ•ˆæœŸ", "å›¢å•å¤‡æ³¨"ã€‚
    *   å¯¹äº `update` æ“ä½œï¼Œé¦–å…ˆé€šè¿‡æ™ºèƒ½åŒ¹é…æ‰¾åˆ°è¦ä¿®æ”¹çš„å•†å“ã€‚åœ¨è¿”å›ç»“æœä¸­ï¼Œ`from_name` å¿…é¡»ä½¿ç”¨â€œå½“å‰çº¿ä¸Šå•†å“åˆ—è¡¨â€ä¸­è¢«åŒ¹é…åˆ°çš„é‚£ä¸ªå•†å“**å®Œæ•´çš„ `name`**ã€‚`new_data` åˆ™æ˜¯æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆçš„ã€åŒ…å«æ‰€æœ‰å­—æ®µçš„å®Œæ•´æ–°å•†å“ä¿¡æ¯å¯¹è±¡ï¼Œå…¶ä¸­å¿…é¡»åŒ…å«`å”®ä»·`å’Œ`åŸä»·`ã€‚
    *   å¯¹äº `delete` æ“ä½œï¼Œåªéœ€æå–å¹¶è¿”å›è¦ä¸‹æ¶çš„å•†å“çš„**å®Œæ•´çš„ `name`**ã€‚
4.  **æ ¼å¼åŒ–è¾“å‡º**: å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜æ–‡å­—ã€‚

```json
{{
  "add": [
    {{
      "å›¢è´­æ ‡é¢˜": "...",
      "å”®ä»·": 0.0,
      "åŸä»·": 0.0,
      "member_type": "æ–°å®¢",
      "commodity_type": "ç½‘è´¹",
      "applicable_location": "å¤§å…",
      "å¯ç”¨åŒºåŸŸ": "...",
      "é™è´­": "...",
      "æœ‰æ•ˆæœŸ": "...",
      "å›¢å•å¤‡æ³¨": "..."
    }}
  ],
  "update": [
    {{
      "from_name": "è¦ä¿®æ”¹çš„çº¿ä¸Šå•†å“åŸåç§°",
      "new_data": {{
          "å›¢è´­æ ‡é¢˜": "ä¿®æ”¹åçš„æ–°åç§°",
          "å”®ä»·": 19.9,
          "åŸä»·": 100.0,
          "å¯ç”¨åŒºåŸŸ": "...",
          "é™è´­": "...",
          "æœ‰æ•ˆæœŸ": "...",
          "å›¢å•å¤‡æ³¨": "..."
      }}
    }}
  ],
  "delete": [
    {{
      "name": "è¦ä¸‹æ¶çš„çº¿ä¸Šå•†å“åç§°"
    }}
  ]
}}
```

**æ³¨æ„äº‹é¡¹**:
-   å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…çš„å•†å“è¿›è¡Œä¿®æ”¹æˆ–ä¸‹æ¶ï¼Œè¯·ä¸è¦å‡­ç©ºåˆ›é€ ï¼Œåœ¨ç»“æœä¸­å¿½ç•¥è¯¥é¡¹æ“ä½œã€‚
-   ä»·æ ¼å¿…é¡»æ˜¯æ•°å­—ï¼ˆæµ®ç‚¹æ•°ï¼‰ã€‚
-   è¿”å›çš„ç»“æœå¿…é¡»æ˜¯çº¯ç²¹çš„JSONå­—ç¬¦ä¸²ã€‚
"""

    cache_key = hashlib.sha256(prompt.encode('utf-8')).hexdigest()
    if cache_key in cache:
        log_func("[Cache Hit] å‘ç°ç›¸åŒåˆ†æè¯·æ±‚çš„ç¼“å­˜ç»“æœï¼Œç›´æ¥ä½¿ç”¨ã€‚")
        return cache[cache_key]

    log_func("[Cache Miss] æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå‘èµ·æ–°çš„LLMè¯·æ±‚ã€‚")
    try:
        log_func(f"--- [DEBUG] å‘é€ç»™LLMçš„Prompt ---\n{prompt[:800]}...\n" + "-"*30)
        response = llm_client.chat.completions.create(model=LLM_MODEL_ID, messages=[{'role': 'user', 'content': prompt}], stream=True)
        full_response = "".join(chunk.choices[0].delta.content for chunk in response if chunk.choices[0].delta.content)
        log_func(f"--- [LLM Raw Response for Text Analysis] ---\n{full_response}\n" + "-"*30)
        
        json_match = re.search(r'\{[\s\S]*\}', full_response)
        if json_match:
            cleaned_response = json_match.group(0)
            analysis_result = json.loads(cleaned_response)
        else:
            raise json.JSONDecodeError("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONå¯¹è±¡", full_response, 0)
        
        log_func("[Success] LLMæ–‡æœ¬æŒ‡ä»¤åˆ†ææˆåŠŸï¼")
        cache[cache_key] = analysis_result
        log_func(f"å·²å°†æœ¬æ¬¡ç»“æœå­˜å…¥ç¼“å­˜ï¼ŒKey: {cache_key[:10]}...")
        return analysis_result
    except Exception as e:
        log_func(f"[Error] LLMæ–‡æœ¬æŒ‡ä»¤åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}\n{traceback.format_exc()}")
    return None

def parse_product_details(details):
    """Parses the raw product detail JSON into a clean dictionary."""
    if not details or 'product' not in details:
        raise ValueError("æ— æ•ˆçš„å•†å“è¯¦æƒ…æ•°æ®")

    product = details.get('product', {})
    sku = details.get('skus', [{}])[0]
    attr_map = product.get('attr_key_value_map', {})

    name = product.get('product_name')
    price = sku.get('actual_amount', 0) / 100
    product_id = product.get('product_id')

    # Default values
    area, limit, validity, notes = "æœªçŸ¥", "æœªçŸ¥", "æœªçŸ¥", ""

    try:
        notification = json.loads(attr_map.get('Notification', '[]'))
        title_map = {item['title']: item['content'] for item in notification}
        
        validity_text = title_map.get('æœ‰æ•ˆæœŸ', 'è´­ä¹°å30æ—¥å†…æœ‰æ•ˆ')
        validity = validity_text.replace("è´­ä¹°å", "").replace("å†…æœ‰æ•ˆ", "")
        
        limit = title_map.get('é™è´­è¯´æ˜', 'æ— ')
        notes = title_map.get('ä½¿ç”¨é¡»çŸ¥', '')
        
        desc = json.loads(attr_map.get('Description', '[]'))
        if desc:
            area = desc[0].replace("é€‚ç”¨åŒºåŸŸ: ", "")
    except (json.JSONDecodeError, IndexError, KeyError) as e:
        # Silently ignore parsing errors, use defaults
        pass

    return {
        "id": product_id,
        "å›¢è´­æ ‡é¢˜": name,
        "å”®ä»·": price,
        "å¯ç”¨åŒºåŸŸ": area,
        "é™è´­": limit,
        "æœ‰æ•ˆæœŸ": validity,
        "å›¢å•å¤‡æ³¨": notes
    }

def center_crop_image(img, aspect_ratio):
    """å±…ä¸­è£å‰ªå›¾ç‰‡"""
    width, height = img.size
    target_width, target_height = width, width / aspect_ratio
    if target_height > height:
        target_height = height
        target_width = height * aspect_ratio
    left, top = (width - target_width) / 2, (height - target_height) / 2
    return img.crop((left, top, left + target_width, top + target_height))

def upload_to_r2(img_obj, poi_id, aspect_ratio_str, log_func):
    """ä¸Šä¼ å›¾ç‰‡åˆ°Cloudflare R2"""
    log_func(f"--- æ­£åœ¨ä¸Šä¼  {aspect_ratio_str} æ¯”ä¾‹çš„å›¾ç‰‡ ---")
    try:
        s3_client = boto3.client('s3', endpoint_url=R2_ENDPOINT_URL, aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY_ID, aws_secret_access_key=CLOUDFLARE_R2_SECRET_ACCESS_KEY, config=Config(signature_version='s3v4'))
        in_mem_file = BytesIO()
        if img_obj.mode == 'RGBA':
            img_obj = img_obj.convert('RGB')
        img_obj.save(in_mem_file, format='JPEG', quality=90)
        in_mem_file.seek(0)
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        filename = f"{poi_id}_{timestamp}_{aspect_ratio_str.replace(':', '_')}.jpg"
        s3_client.upload_fileobj(in_mem_file, R2_BUCKET_NAME, filename, ExtraArgs={'ContentType': 'image/jpeg'})
        image_url = f"{R2_PUBLIC_URL_PREFIX}/{filename}"
        log_func(f"[Success] å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {image_url}")
        return image_url
    except Exception as e:
        log_func(f"[Error] å›¾ç‰‡ä¸Šä¼ åˆ°R2å¤±è´¥: {e}")
        return None

class App(tk.Frame):
    def __init__(self, master=None):
        super().__init__(master)
        self.master = master
        self.master.title("æŠ–éŸ³å›¢è´­æ™ºèƒ½åŒæ­¥å·¥å…· v2.4")
        self.master.geometry("1350x800")
        self.pack(fill="both", expand=True)
        self.store_data, self.douyin_access_token, self.douyin_products, self.excel_data, self.excel_file_path, self.all_store_names, self.image_dir, self.current_poi_id = {}, None, [], [], "", [], None, None
        self.llm_cache = {}
        self.product_details_cache = {}
        self.hide_live_only_var = tk.BooleanVar(value=False)
        self.multi_match_mode_var = tk.BooleanVar(value=False)
        self.edit_entry = None
        self.create_widgets()
        self.init_backend()

    def log(self, message): self.master.after(0, lambda: self._log_thread_safe(message))
    def _log_thread_safe(self, message):
        # GUIæ—¥å¿—
        self.log_text.config(state="normal")
        self.log_text.insert(tk.END, f"{time.strftime('%H:%M:%S')} - {message}\n")
        self.log_text.see(tk.END)
        self.log_text.config(state="disabled")
        # æ–‡ä»¶æ—¥å¿—
        logging.info(message)

    def create_widgets(self):
        top_frame = ttk.Frame(self); top_frame.pack(fill="x", padx=10, pady=5)
        ttk.Label(top_frame, text="é€‰æ‹©é—¨åº—:").pack(side="left", padx=(0, 5))
        self.store_name_combobox = ttk.Combobox(top_frame, width=35); self.store_name_combobox.pack(side="left", padx=5)
        self.store_name_combobox.bind('<KeyRelease>', self.filter_combobox_list)
        
        ttk.Label(top_frame, text="æ’é™¤å…³é”®è¯:").pack(side="left", padx=(10, 5))
        self.exclude_keyword_var = tk.StringVar()
        self.exclude_keyword_entry = ttk.Entry(top_frame, textvariable=self.exclude_keyword_var, width=20); self.exclude_keyword_entry.pack(side="left")

        self.query_douyin_btn = ttk.Button(top_frame, text="1. æŸ¥è¯¢æŠ–éŸ³å•†å“", command=self.start_query_douyin); self.query_douyin_btn.pack(side="left", padx=5)
        self.hide_live_only_check = ttk.Checkbutton(top_frame, text="éšè—ä»…ç›´æ’­é—´å•†å“", variable=self.hide_live_only_var, command=self.filter_and_populate_products); self.hide_live_only_check.pack(side="left", padx=10)
        self.web_config_btn = ttk.Button(top_frame, text="âš™ï¸ ç½‘é¡µç«¯é…ç½®", command=self.open_web_config); self.web_config_btn.pack(side="left", padx=5)

        excel_frame = ttk.LabelFrame(self, text="Excel æ•°æ®æº"); excel_frame.pack(fill="x", padx=10, pady=5)
        self.load_excel_btn = ttk.Button(excel_frame, text="2. åŠ è½½Excelæ–‡ä»¶", command=self.start_load_excel); self.load_excel_btn.pack(side="left", padx=5, pady=5)
        self.excel_path_label = ttk.Label(excel_frame, text="æœªåŠ è½½æ–‡ä»¶"); self.excel_path_label.pack(side="left", padx=5)

        image_source_frame = ttk.LabelFrame(self, text="å›¾ç‰‡æº (ç”¨äºæ–°å¢å¥—é¤)"); image_source_frame.pack(fill="x", padx=10, pady=5)
        self.select_image_dir_btn = ttk.Button(image_source_frame, text="é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹", command=self.select_image_dir); self.select_image_dir_btn.pack(side="left", padx=5, pady=5)
        self.image_dir_label = ttk.Label(image_source_frame, text="æœªé€‰æ‹©æ–‡ä»¶å¤¹"); self.image_dir_label.pack(side="left", padx=5)
        self.auto_match_image_btn = ttk.Button(image_source_frame, text="è‡ªåŠ¨åŒ¹é…å›¾ç‰‡", command=self.start_auto_match_images, state="disabled"); self.auto_match_image_btn.pack(side="left", padx=10)
        self.multi_match_check = ttk.Checkbutton(image_source_frame, text="å›¾ç‰‡åŒ¹é…å¤šå¥—é¤æ¨¡å¼", variable=self.multi_match_mode_var); self.multi_match_check.pack(side="left", padx=10)

        analysis_frame = ttk.LabelFrame(self, text="æ™ºèƒ½åˆ†æ (æ–‡æœ¬/å›¾ç‰‡/ç¾å›¢åŒæ­¥)"); analysis_frame.pack(fill="x", padx=10, pady=5)
        self.analysis_text = tk.Text(analysis_frame, height=8, width=80); self.analysis_text.pack(side="left", fill="x", expand=True, padx=5, pady=5)
        analysis_btn_frame = ttk.Frame(analysis_frame); analysis_btn_frame.pack(side="left", fill="y", padx=5)
        self.analyze_text_btn = ttk.Button(analysis_btn_frame, text="åˆ†ææ–‡æœ¬", command=self.start_text_analysis); self.analyze_text_btn.pack(pady=5, fill="x")
        self.sync_meituan_btn = ttk.Button(analysis_btn_frame, text="åŒæ­¥ç¾å›¢å¥—é¤", command=self.start_sync_meituan); self.sync_meituan_btn.pack(pady=5, fill="x")
        # ç¾å›¢åŒæ­¥é€‰é¡¹
        self.meituan_skip_update_var = tk.BooleanVar(value=False)
        self.meituan_skip_update_check = ttk.Checkbutton(analysis_btn_frame, text="ä»…æ–°å¢/ä¸‹æ¶\n(è·³è¿‡ä»·æ ¼æ›´æ–°)", variable=self.meituan_skip_update_var); self.meituan_skip_update_check.pack(pady=2, fill="x")
        self.analyze_image_btn = ttk.Button(analysis_btn_frame, text="åˆ†æå›¾ç‰‡ (æš‚æœªå®ç°)", state="disabled"); self.analyze_image_btn.pack(pady=5, fill="x")

        main_frame = ttk.Frame(self); main_frame.pack(fill="both", expand=True, padx=10, pady=5)
        columns = ("douyin_name", "douyin_price", "douyin_origin_price", "match_status", "action_mode", "matched_image", "excel_title", "excel_price", "excel_origin_price", "commodity_type", "applicable_location", "excel_area", "excel_limit", "excel_validity", "id")
        self.product_tree = ttk.Treeview(main_frame, columns=columns, show="headings"); self.product_tree.pack(side="left", fill="both", expand=True)
        headings = {
            "douyin_name": "æŠ–éŸ³å•†å“å", "douyin_price": "æŠ–éŸ³ä»·", "douyin_origin_price": "æŠ–éŸ³åŸä»·",
            "match_status": "åŒ¹é…çŠ¶æ€", "action_mode": "æ“ä½œæ¨¡å¼", "matched_image": "åŒ¹é…å›¾ç‰‡",
            "excel_title": "åŒ¹é…Excelå•†å“", "excel_price": "Excelä»·", "excel_origin_price": "ExcelåŸä»·",
            "commodity_type": "å¥—é¤ç±»å‹", "applicable_location": "é€‚ç”¨ä½ç½®", "excel_area": "å¯ç”¨åŒºåŸŸ", "excel_limit": "é™è´­", "excel_validity": "æœ‰æ•ˆæœŸ",
            "id": "Product ID"
        }
        widths = {
            "douyin_name": 220, "douyin_price": 60, "douyin_origin_price": 60,
            "match_status": 80, "action_mode": 80, "matched_image": 120,
            "excel_title": 220, "excel_price": 60, "excel_origin_price": 60,
            "commodity_type": 80, "applicable_location": 100, "excel_area": 100, "excel_limit": 80, "excel_validity": 60
        }
        for col, text in headings.items(): self.product_tree.heading(col, text=text)
        for col, width in widths.items(): self.product_tree.column(col, width=width, anchor="center" if "price" in col or "status" in col or "validity" in col or "mode" in col else "w")
        self.product_tree["displaycolumns"] = [col for col in columns if col != "id"]
        self.product_tree.bind("<Double-1>", self.edit_cell)
        scrollbar = ttk.Scrollbar(main_frame, orient="vertical", command=self.product_tree.yview); self.product_tree.configure(yscroll=scrollbar.set); scrollbar.pack(side="right", fill="y")
        
        bottom_frame = ttk.Frame(self); bottom_frame.pack(fill="both", expand=True, padx=10, pady=5)
        action_frame = ttk.Frame(bottom_frame); action_frame.pack(fill="x", pady=5)
        self.match_btn = ttk.Button(action_frame, text="3. å¼€å§‹æ™ºèƒ½åŒ¹é…", command=self.start_match_products, state="disabled"); self.match_btn.pack(side="left", fill="x", expand=True, padx=5)
        self.delete_btn = ttk.Button(action_frame, text="åˆ é™¤é€‰ä¸­è¡Œ", command=self.delete_selected_rows); self.delete_btn.pack(side="left", fill="x", expand=True, padx=5)
        self.update_btn = ttk.Button(action_frame, text="4. ä¸€é”®æ‰¹é‡æ“ä½œ", command=self.start_batch_update, state="disabled"); self.update_btn.pack(side="left", fill="x", expand=True, padx=5)
        log_frame = ttk.LabelFrame(bottom_frame, text="æ—¥å¿—"); log_frame.pack(fill="both", expand=True)
        self.log_text = tk.Text(log_frame, wrap="word", state="disabled", height=10); self.log_text.pack(fill="both", expand=True)

    def set_ui_state(self, is_busy):
        state = "disabled" if is_busy else "normal"
        for btn in (self.query_douyin_btn, self.load_excel_btn, self.delete_btn, self.analyze_text_btn, self.select_image_dir_btn, self.auto_match_image_btn): btn.config(state=state)
        self.store_name_combobox.config(state="disabled" if is_busy else "normal")
        self.exclude_keyword_entry.config(state="disabled" if is_busy else "normal")
        self.match_btn.config(state="disabled" if is_busy or not (self.douyin_products and self.excel_data) else "normal")
        self.update_btn.config(state="disabled" if is_busy or not any(self.product_tree.set(item, "match_status") == "åŒ¹é…æˆåŠŸ" for item in self.product_tree.get_children()) else "normal")
        self.auto_match_image_btn.config(state="disabled" if is_busy or not self.image_dir else "normal")

    def open_web_config(self):
        """æ‰“å¼€ç½‘é¡µç«¯é…ç½®å¯¹è¯æ¡†"""
        config_window = tk.Toplevel(self.master)
        config_window.title("ç½‘é¡µç«¯é…ç½®ï¼ˆç”¨äºé‡åˆ›æ¨¡å¼ï¼‰")
        config_window.geometry("750x500")
        
        # æ ‡é¢˜
        ttk.Label(config_window, text="é‡åˆ›æ¨¡å¼ - ç½‘é¡µç«¯APIé…ç½®", font=("", 12, "bold")).pack(pady=10)
        
        # é…ç½®çŠ¶æ€
        status_frame = ttk.LabelFrame(config_window, text="å½“å‰é…ç½®çŠ¶æ€")
        status_frame.pack(fill="x", padx=20, pady=10)
        
        cookie_status = "âœ… å·²é…ç½®" if DOUYIN_WEB_COOKIE else "âŒ æœªé…ç½®"
        csrf_status = "âœ… å·²é…ç½®" if DOUYIN_WEB_CSRF_TOKEN else "âŒ æœªé…ç½®"
        
        ttk.Label(status_frame, text=f"Cookie: {cookie_status}", font=("", 10)).pack(anchor="w", padx=10, pady=5)
        ttk.Label(status_frame, text=f"CSRF Token: {csrf_status}", font=("", 10)).pack(anchor="w", padx=10, pady=5)
        ttk.Label(status_frame, text=f"Cookieæ¥æº: cookie.txt æ–‡ä»¶", font=("", 9), foreground="gray").pack(anchor="w", padx=10, pady=2)
        ttk.Label(status_frame, text=f"CSRF Token: ç¡¬ç¼–ç ", font=("", 9), foreground="gray").pack(anchor="w", padx=10, pady=2)
        
        # è¯´æ˜æ–‡å­—
        info_frame = ttk.LabelFrame(config_window, text="é…ç½®è¯´æ˜")
        info_frame.pack(fill="x", padx=20, pady=10)
        
        help_text = """
Cookieé…ç½®ï¼š
â€¢ Cookieå·²ä»ç¨‹åºæ ¹ç›®å½•çš„ cookie.txt æ–‡ä»¶è‡ªåŠ¨åŠ è½½
â€¢ å¦‚éœ€æ›´æ–°Cookieï¼Œè¯·ç¼–è¾‘ cookie.txt æ–‡ä»¶ï¼Œç„¶åç‚¹å‡»ä¸‹æ–¹"é‡æ–°åŠ è½½Cookie"æŒ‰é’®

CSRF Tokené…ç½®ï¼š
â€¢ CSRF Tokenå·²ç¡¬ç¼–ç åœ¨ç¨‹åºä¸­
â€¢ å½“å‰å€¼: 000100000001ae8a406b9344d0cc4e30ceaf542c505dbbabca5a3842c450a93e0787a4d2f8991880c8ea9d2d1372

å¦‚ä½•è·å–æ–°çš„Cookieï¼š
1. æ‰“å¼€æµè§ˆå™¨ï¼Œç™»å½• https://life.douyin.com
2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œåˆ‡æ¢åˆ°Networkæ ‡ç­¾
3. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°ä»»æ„è¯·æ±‚
4. åœ¨Request Headersä¸­å¤åˆ¶å®Œæ•´çš„Cookieå€¼
5. å°†Cookieç²˜è´´åˆ°ç¨‹åºæ ¹ç›®å½•çš„ cookie.txt æ–‡ä»¶ä¸­
        """
        ttk.Label(info_frame, text=help_text, justify="left", foreground="#333").pack(anchor="w", padx=10, pady=10)
        
        # æŒ‰é’®åŒºåŸŸ
        button_frame = ttk.Frame(config_window)
        button_frame.pack(pady=10)
        
        def reload_cookie():
            global DOUYIN_WEB_COOKIE
            DOUYIN_WEB_COOKIE = load_cookie_from_file()
            if DOUYIN_WEB_COOKIE:
                messagebox.showinfo("é‡æ–°åŠ è½½æˆåŠŸ", "Cookieå·²ä» cookie.txt æ–‡ä»¶é‡æ–°åŠ è½½ï¼")
                config_window.destroy()
                self.open_web_config()  # é‡æ–°æ‰“å¼€çª—å£æ˜¾ç¤ºæ–°çŠ¶æ€
            else:
                messagebox.showerror("åŠ è½½å¤±è´¥", "æ— æ³•ä» cookie.txt æ–‡ä»¶åŠ è½½Cookieï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        
        def open_cookie_file():
            cookie_file = os.path.join(os.path.dirname(__file__), 'cookie.txt')
            if os.path.exists(cookie_file):
                os.startfile(cookie_file)
            else:
                messagebox.showwarning("æ–‡ä»¶ä¸å­˜åœ¨", f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}\n\nè¯·åˆ›å»ºè¯¥æ–‡ä»¶å¹¶ç²˜è´´Cookieå†…å®¹ã€‚")
        
        ttk.Button(button_frame, text="ğŸ”„ é‡æ–°åŠ è½½Cookie", command=reload_cookie).pack(side="left", padx=5)
        ttk.Button(button_frame, text="ğŸ“ æ‰“å¼€cookie.txt", command=open_cookie_file).pack(side="left", padx=5)
        ttk.Button(button_frame, text="å…³é—­", command=config_window.destroy).pack(side="left", padx=5)

    def init_backend(self):
        self.set_ui_state(True)
        threading.Thread(target=self._init_backend_thread, daemon=True).start()

    def _init_backend_thread(self):
        self.douyin_access_token = get_douyin_access_token(self.log)
        if not self.douyin_access_token: messagebox.showerror("é”™è¯¯", "è·å–æŠ–éŸ³Access Tokenå¤±è´¥ã€‚")
        feishu_token = get_feishu_tenant_access_token(self.log)
        if feishu_token:
            self.store_data = get_feishu_bitable_records(feishu_token, self.log)
            if self.store_data: self.master.after(0, self.update_store_combobox)
        self.master.after(0, lambda: self.set_ui_state(False))

    def select_image_dir(self):
        dir_path = filedialog.askdirectory(title="é€‰æ‹©åŒ…å«å¤´å›¾çš„æ–‡ä»¶å¤¹")
        if dir_path:
            self.image_dir = dir_path
            self.image_dir_label.config(text=f"å·²é€‰: {os.path.basename(dir_path)}")
            self.log(f"æ–°å¢å¥—é¤çš„å›¾ç‰‡æºæ–‡ä»¶å¤¹å·²è®¾ç½®ä¸º: {dir_path}")

    def update_store_combobox(self):
        self.all_store_names = sorted(list(self.store_data.keys()))
        self.store_name_combobox['values'] = self.all_store_names
        if self.all_store_names: self.store_name_combobox.set(self.all_store_names[0])
        self.log("é—¨åº—ä¸‹æ‹‰åˆ—è¡¨å·²æ›´æ–°ã€‚")

    def filter_combobox_list(self, event):
        typed_text = self.store_name_combobox.get().lower()
        self.store_name_combobox['values'] = [name for name in self.all_store_names if typed_text in name.lower()] if typed_text else self.all_store_names

    def start_query_douyin(self):
        store_name = self.store_name_combobox.get().strip()
        if not store_name: return messagebox.showerror("é”™è¯¯", "è¯·é€‰æ‹©ä¸€ä¸ªé—¨åº—ã€‚")
        poi_id = self.store_data.get(store_name)
        if not poi_id: return messagebox.showerror("é”™è¯¯", f"æœªæ‰¾åˆ°é—¨åº— '{store_name}' çš„IDã€‚")
        self.set_ui_state(True)
        threading.Thread(target=self._query_douyin_thread, args=(poi_id,), daemon=True).start()
    
    def _query_douyin_thread(self, poi_id):
        self.current_poi_id = poi_id
        self.product_details_cache.clear()
        all_products = get_douyin_products_by_store(self.douyin_access_token, poi_id, self.log)
        
        exclude_string = self.exclude_keyword_var.get().strip()
        if exclude_string:
            # Use regex to split by spaces, commas (English/Chinese), and semicolons
            exclude_keywords = [kw for kw in re.split(r'[ ,;ï¼Œï¼›]+', exclude_string) if kw]
            if exclude_keywords:
                self.log(f"å¼€å§‹æ ¹æ®æ’é™¤å…³é”®è¯ {exclude_keywords} è¿‡æ»¤å•†å“...")
                # Filter products: exclude if ANY keyword is present in the name
                self.douyin_products = [
                    p for p in all_products
                    if not any(keyword in p['name'] for keyword in exclude_keywords)
                ]
                self.log(f"è¿‡æ»¤åå‰©ä½™ {len(self.douyin_products)} ä¸ªå•†å“ã€‚")
            else:
                self.douyin_products = all_products
        else:
            self.douyin_products = all_products

        self.master.after(0, self.filter_and_populate_products)
        self.master.after(0, lambda: self.set_ui_state(False))

    def populate_product_list(self, products_to_show):
        self.product_tree.delete(*self.product_tree.get_children())
        for pkg in products_to_show:
            # columns = ("douyin_name", "douyin_price", "douyin_origin_price", "match_status", "action_mode", "excel_title", "excel_price", "excel_origin_price", "excel_area", "excel_limit", "excel_validity", "id")
            values = (
                pkg['name'], pkg['price'], pkg.get('origin_price', '0.00'),
                "æœªåŒ¹é…", "ä¿®æ”¹", "",
                "", "", "", "", "", "",
                pkg['id']
            )
            self.product_tree.insert("", "end", values=values)
        self.log(f"æŠ–éŸ³å•†å“åˆ—è¡¨å·²æ›´æ–°ï¼Œå…± {len(products_to_show)} é¡¹ã€‚")
    
    def filter_and_populate_products(self):
        self.set_ui_state(True)
        self.log("æ­£åœ¨åº”ç”¨ç­›é€‰æ¡ä»¶...")
        threading.Thread(target=self._filter_worker_thread, daemon=True).start()

    def _filter_worker_thread(self):
        hide_live_only = self.hide_live_only_var.get()
        if not hide_live_only:
            self.master.after(0, lambda: self.populate_product_list(self.douyin_products))
            self.master.after(0, lambda: self.set_ui_state(False))
            return

        self.log("ç­›é€‰å¼€å¯ï¼šéšè—ä»…ç›´æ’­é—´å¯è§å•†å“ã€‚è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
        filtered_products = []
        total = len(self.douyin_products)
        for i, p in enumerate(self.douyin_products):
            product_id = p['id']
            
            if product_id not in self.product_details_cache:
                self.log(f"æ­£åœ¨è·å–å•†å“è¯¦æƒ… ({i+1}/{total}): {p['name'][:20]}...")
                details = get_douyin_product_details(self.douyin_access_token, product_id, self.log)
                self.product_details_cache[product_id] = details
            else:
                details = self.product_details_cache[product_id]
            
            if not details or 'product' not in details:
                self.log(f"[Warning] æ— æ³•è·å– {product_id} çš„è¯¦æƒ…ï¼Œå°†éšè—è¯¥å•†å“ã€‚")
                continue

            attr_map = details.get('product', {}).get('attr_key_value_map', {})
            show_channel = str(attr_map.get('show_channel', '1'))
            
            if show_channel == '2':
                self.log(f" -> å·²éšè— (ä»…ç›´æ’­é—´): {p['name']}")
                continue
            
            filtered_products.append(p)

        self.master.after(0, lambda: self.populate_product_list(filtered_products))
        self.master.after(0, lambda: self.set_ui_state(False))

    def start_load_excel(self):
        file_path = filedialog.askopenfilename(title="é€‰æ‹©Excelæ–‡ä»¶", filetypes=(("Excel files", "*.xlsx"), ("All files", "*.*")))
        if not file_path: return
        self.excel_file_path = file_path; self.set_ui_state(True)
        threading.Thread(target=self._load_excel_thread, args=(file_path,), daemon=True).start()

    def _load_excel_thread(self, file_path):
        self.excel_data = load_excel_data(file_path, self.log)
        if self.excel_data is not None: self.master.after(0, lambda: self.excel_path_label.config(text=os.path.basename(file_path)))
        else: messagebox.showerror("é”™è¯¯", "åŠ è½½Excelæ–‡ä»¶å¤±è´¥ã€‚")
        self.master.after(0, lambda: self.set_ui_state(False))

    def select_image_dir(self):
        dir_path = filedialog.askdirectory(title="é€‰æ‹©åŒ…å«å¤´å›¾çš„æ–‡ä»¶å¤¹")
        if dir_path:
            self.image_dir = dir_path
            self.image_dir_label.config(text=f"å·²é€‰: {os.path.basename(dir_path)}")
            self.log(f"æ–°å¢å¥—é¤çš„å›¾ç‰‡æºæ–‡ä»¶å¤¹å·²è®¾ç½®ä¸º: {dir_path}")

    def start_match_products(self):
        if not self.excel_data:
            messagebox.showinfo("æç¤º", "è¯·å…ˆåŠ è½½Excelæ–‡ä»¶ã€‚")
            return

        current_products_in_tree = []
        for item_id in self.product_tree.get_children():
            douyin_name = self.product_tree.set(item_id, "douyin_name")
            # åªå¯¹å®é™…çš„æŠ–éŸ³å•†å“è¿›è¡ŒåŒ¹é…ï¼Œå¿½ç•¥å¾…åˆ›å»ºçš„è¡Œ
            if douyin_name and douyin_name != "<å¾…åˆ›å»º>":
                current_products_in_tree.append({
                    "name": douyin_name,
                    "price": self.product_tree.set(item_id, "douyin_price"),
                    "origin_price": self.product_tree.set(item_id, "douyin_origin_price"),
                    "id": self.product_tree.set(item_id, "id")
                })
        
        if not current_products_in_tree:
            messagebox.showinfo("æç¤º", "æŠ–éŸ³å•†å“åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…ã€‚")
            return

        self.set_ui_state(True)
        threading.Thread(target=self._match_products_thread, args=(current_products_in_tree,), daemon=True).start()

    def _match_products_thread(self, current_douyin_products):
        match_result = match_products_with_llm(current_douyin_products, self.excel_data, self.log, self.llm_cache)
        if match_result: self.master.after(0, lambda: self.update_matches_in_tree(match_result))
        self.master.after(0, lambda: self.set_ui_state(False))

    def update_matches_in_tree(self, match_result):
        excel_map = {item['å›¢è´­æ ‡é¢˜']: item for item in self.excel_data}
        for item_id in self.product_tree.get_children():
            douyin_name = self.product_tree.set(item_id, "douyin_name")
            matched_excel_title = match_result.get(douyin_name)
            if matched_excel_title in excel_map:
                excel_item = excel_map[matched_excel_title]
                self.product_tree.set(item_id, "match_status", "åŒ¹é…æˆåŠŸ")
                self.product_tree.set(item_id, "excel_title", matched_excel_title)
                self.product_tree.set(item_id, "excel_price", excel_item.get('å”®ä»·', ''))
                self.product_tree.set(item_id, "excel_area", excel_item.get('å¯ç”¨åŒºåŸŸ', ''))
                self.product_tree.set(item_id, "excel_limit", excel_item.get('é™è´­', ''))
                self.product_tree.set(item_id, "excel_validity", excel_item.get('æœ‰æ•ˆæœŸ', ''))
                self.product_tree.set(item_id, "action_mode", "é‡åˆ›" if "é‡ç½®æ¬¡æ•°" in str(excel_item.get('å¯ç”¨åŒºåŸŸ', '')) else "ä¿®æ”¹")
            else:
                self.product_tree.set(item_id, "match_status", "åŒ¹é…å¤±è´¥")
                self.product_tree.set(item_id, "action_mode", "-")
                for col in ("excel_title", "excel_price", "excel_area", "excel_limit", "excel_validity"): self.product_tree.set(item_id, col, "")
        self.log("LLMåŒ¹é…ç»“æœå·²æ›´æ–°åˆ°ç•Œé¢ã€‚")

    def edit_cell(self, event):
        if hasattr(self, 'edit_entry') and self.edit_entry:
            self.edit_entry.destroy()

        item_id = self.product_tree.focus()
        if not item_id: return

        column_id = self.product_tree.identify_column(event.x)
        column_name = self.product_tree.column(column_id, "id")

        if column_name == "action_mode":
            current_mode = self.product_tree.set(item_id, "action_mode")
            modes = ["ä¿®æ”¹", "é‡åˆ›", "ä¸‹æ¶", "-"]
            try:
                new_mode = modes[(modes.index(current_mode) + 1) % len(modes)]
            except ValueError:
                new_mode = "ä¿®æ”¹"
            self.product_tree.set(item_id, "action_mode", new_mode)
            return

        editable_columns = ["excel_title", "excel_price", "excel_origin_price", "commodity_type", "applicable_location", "excel_area", "excel_limit", "excel_validity", "matched_image"]
        if column_name not in editable_columns:
            return

        x, y, width, height = self.product_tree.bbox(item_id, column_id)
        value = self.product_tree.set(item_id, column_name)
        
        entry_var = tk.StringVar(value=value)
        self.edit_entry = ttk.Entry(self.product_tree, textvariable=entry_var)
        self.edit_entry.place(x=x, y=y, width=width, height=height)
        self.edit_entry.focus_force()
        self.edit_entry.selection_range(0, tk.END)

        def on_edit_done(event=None):
            new_value = entry_var.get()
            self.product_tree.set(item_id, column_name, new_value)
            if hasattr(self, 'edit_entry') and self.edit_entry:
                self.edit_entry.destroy()
                self.edit_entry = None
        
        self.edit_entry.bind("<Return>", on_edit_done)
        self.edit_entry.bind("<FocusOut>", on_edit_done)
        self.edit_entry.bind("<Escape>", lambda e: self.edit_entry.destroy() if hasattr(self, 'edit_entry') and self.edit_entry else None)

    def delete_selected_rows(self):
        selected_items = self.product_tree.selection()
        if not selected_items: return messagebox.showinfo("æç¤º", "è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„è¡Œã€‚")
        if messagebox.askyesno("ç¡®è®¤åˆ é™¤", f"ç¡®å®šè¦åˆ é™¤é€‰ä¸­çš„ {len(selected_items)} è¡Œå—ï¼Ÿæ­¤æ“ä½œä»…åœ¨ç•Œé¢ä¸Šç§»é™¤ï¼Œä¸å½±å“çº¿ä¸Šå•†å“ã€‚"):
            for item in selected_items: self.product_tree.delete(item)
            self.log(f"å·²ä»ç•Œé¢åˆ é™¤ {len(selected_items)} è¡Œã€‚")

    def start_sync_meituan(self):
        """å¼€å§‹ç¾å›¢åŒæ­¥"""
        if not self.douyin_products:
            messagebox.showerror("é”™è¯¯", "è¯·å…ˆæŸ¥è¯¢æŠ–éŸ³å•†å“åˆ—è¡¨")
            return
        
        store_name = self.store_name_combobox.get().strip()
        if not store_name:
            messagebox.showerror("é”™è¯¯", "è¯·é€‰æ‹©é—¨åº—")
            return
        
        # ä½¿ç”¨simpledialogè¯¢é—®åŸå¸‚æ‹¼éŸ³
        from tkinter import simpledialog
        city = simpledialog.askstring(
            "è¾“å…¥åŸå¸‚ä¿¡æ¯",
            "è¯·è¾“å…¥åŸå¸‚æ‹¼éŸ³ï¼ˆå¦‚ï¼štaiyuanï¼‰:",
            initialvalue="taiyuan",
            parent=self.master
        )
        
        if not city or not city.strip():
            return
        
        self.set_ui_state(True)
        threading.Thread(target=self._sync_meituan_thread, args=(store_name, city.strip()), daemon=True).start()
    
    def _sync_meituan_thread(self, store_name, city):
        """ç¾å›¢åŒæ­¥çº¿ç¨‹"""
        self.log("========== å¼€å§‹ç¾å›¢åŒæ­¥ ==========")
        
        # 1. å¤„ç†åº—å
        cleaned_store_name = process_store_name_for_meituan(store_name, self.log)
        
        # 2. è·å–ç¾å›¢å¥—é¤
        meituan_packages = get_meituan_packages(cleaned_store_name, city, self.log)
        
        if not meituan_packages:
            self.log("[Error] æœªèƒ½è·å–ç¾å›¢å¥—é¤ï¼ŒåŒæ­¥ç»ˆæ­¢")
            self.master.after(0, lambda: messagebox.showerror("é”™è¯¯", "æœªèƒ½è·å–ç¾å›¢å¥—é¤ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œä»£ç†è®¾ç½®"))
            self.master.after(0, lambda: self.set_ui_state(False))
            return
        
        # 3. æ™ºèƒ½åŒ¹é…
        match_result = match_packages_smart(self.douyin_products, meituan_packages, self.log)
        
        # è·å–ç”¨æˆ·é€‰é¡¹ï¼šæ˜¯å¦è·³è¿‡ä»·æ ¼æ›´æ–°
        skip_price_update = self.meituan_skip_update_var.get()
        if skip_price_update:
            self.log("\n[ç”¨æˆ·é€‰é¡¹] å·²å¯ç”¨'ä»…æ–°å¢/ä¸‹æ¶'æ¨¡å¼ï¼Œå°†è·³è¿‡æ‰€æœ‰ä»·æ ¼æ›´æ–°æ“ä½œ")
        
        # 4. æ„å»ºæ“ä½œåˆ—è¡¨
        operations = []
        
        # 4.1 åŒ¹é…çš„å¥—é¤ - æ ¹æ®actionå’Œç”¨æˆ·é€‰é¡¹å†³å®šæ˜¯å¦æ›´æ–°
        for match in match_result["matches"]:
            dy_pkg = match["douyin"]
            mt_pkg = match["meituan"]
            action = match["action"]
            
            if action == "update":
                if skip_price_update:
                    # ç”¨æˆ·é€‰æ‹©è·³è¿‡ä»·æ ¼æ›´æ–°
                    self.log(f"è·³è¿‡ä»·æ ¼æ›´æ–°: {dy_pkg['name']} (ç”¨æˆ·é€‰æ‹©ä»…æ–°å¢/ä¸‹æ¶)")
                else:
                    # ä»·æ ¼ä¸åŒï¼Œéœ€è¦æ›´æ–°
                    operations.append({
                        "action": "update",
                        "product_id": dy_pkg['id'],
                        "douyin_name": dy_pkg['name'],
                        "new_data": {
                            "å›¢è´­æ ‡é¢˜": dy_pkg['name'],  # ä¿æŒåŸåç§°
                            "å”®ä»·": mt_pkg['price'],
                            "åŸä»·": mt_pkg['original_price'],
                            "å¯ç”¨åŒºåŸŸ": "",
                            "é™è´­": "",
                            "æœ‰æ•ˆæœŸ": "",
                            "å›¢å•å¤‡æ³¨": ""
                        }
                    })
            elif action == "keep":
                # ä»·æ ¼ç›¸åŒï¼Œä¿æŒåŸæ ·ï¼Œä¸æ·»åŠ åˆ°æ“ä½œåˆ—è¡¨
                self.log(f"ä¿æŒåŸæ ·: {dy_pkg['name']} (ä»·æ ¼å·²åŒæ­¥)")
                pass
        
        # 4.2 ç¾å›¢ç‹¬æœ‰ - æ–°å»º
        for mt_pkg in match_result["meituan_only"]:
            operations.append({
                "action": "add",
                "product_id": None,
                "douyin_name": "<å¾…åˆ›å»º>",
                "new_data": {
                    "å›¢è´­æ ‡é¢˜": mt_pkg['title'],
                    "å”®ä»·": mt_pkg['price'],
                    "åŸä»·": mt_pkg['original_price'],
                    "member_type": "ä¸é™åˆ¶",
                    "commodity_type": "ç½‘è´¹" if "ç½‘è´¹" in mt_pkg['title'] else "åŒ…æ—¶",
                    "applicable_location": "å¤§å…",
                    "å¯ç”¨åŒºåŸŸ": "",
                    "é™è´­": "",
                    "æœ‰æ•ˆæœŸ": "30",
                    "å›¢å•å¤‡æ³¨": ""
                }
            })
        
        # 4.3 æŠ–éŸ³ç‹¬æœ‰ - ä¸‹æ¶
        for dy_pkg in match_result["douyin_only"]:
            operations.append({
                "action": "delete",
                "product_id": dy_pkg['id'],
                "douyin_name": dy_pkg['name'],
                "new_data": {"å›¢è´­æ ‡é¢˜": f"ä¸‹æ¶-{dy_pkg['name']}"}
            })
        
        # 5. ç»Ÿè®¡æ“ä½œç±»å‹
        update_count = sum(1 for op in operations if op["action"] == "update")
        add_count = sum(1 for op in operations if op["action"] == "add")
        delete_count = sum(1 for op in operations if op["action"] == "delete")
        
        # 6. æ›´æ–°UIæ˜¾ç¤º
        self.master.after(0, lambda: self._populate_meituan_sync_result(operations))
        self.master.after(0, lambda: self.set_ui_state(False))
        
        # 7. è¾“å‡ºæ±‡æ€»ä¿¡æ¯
        self.log(f"\n========== ç¾å›¢åŒæ­¥åˆ†æå®Œæˆ ==========")
        self.log(f"æ€»æ“ä½œæ•°: {len(operations)} ä¸ª")
        self.log(f"  - ä»·æ ¼æ›´æ–°: {update_count} ä¸ª")
        self.log(f"  - æ–°å¢å¥—é¤: {add_count} ä¸ª")
        self.log(f"  - ä¸‹æ¶å¥—é¤: {delete_count} ä¸ª")
        if skip_price_update and update_count == 0:
            skipped_updates = sum(1 for m in match_result["matches"] if m["action"] == "update")
            if skipped_updates > 0:
                self.log(f"  - å·²è·³è¿‡ä»·æ ¼æ›´æ–°: {skipped_updates} ä¸ªï¼ˆç”¨æˆ·é€‰æ‹©ï¼‰")
        self.log(f"========================================\n")
    
    def _populate_meituan_sync_result(self, operations):
        """å°†ç¾å›¢åŒæ­¥ç»“æœå¡«å……åˆ°è¡¨æ ¼"""
        self.product_tree.delete(*self.product_tree.get_children())
        self.excel_data = []
        
        for op in operations:
            action = op["action"]
            product_id = op.get("product_id", "")
            douyin_name = op.get("douyin_name", "")
            new_data = op["new_data"]
            
            self.excel_data.append(new_data)
            
            if action == "update":
                action_mode = "ä¿®æ”¹"
                douyin_price = next((p['price'] for p in self.douyin_products if p['id'] == product_id), "")
                douyin_origin_price = next((p['origin_price'] for p in self.douyin_products if p['id'] == product_id), "")
            elif action == "add":
                action_mode = "é‡åˆ›"
                douyin_price = ""
                douyin_origin_price = ""
            else:  # delete
                action_mode = "ä¸‹æ¶"
                douyin_price = next((p['price'] for p in self.douyin_products if p['id'] == product_id), "")
                douyin_origin_price = next((p['origin_price'] for p in self.douyin_products if p['id'] == product_id), "")
            
            values = (
                douyin_name, douyin_price, douyin_origin_price,
                "åŒ¹é…æˆåŠŸ", action_mode, "",
                new_data.get('å›¢è´­æ ‡é¢˜'), new_data.get('å”®ä»·'), new_data.get('åŸä»·'),
                new_data.get('commodity_type', ''), new_data.get('applicable_location', ''),
                new_data.get('å¯ç”¨åŒºåŸŸ'), new_data.get('é™è´­'), new_data.get('æœ‰æ•ˆæœŸ'),
                product_id
            )
            
            tag = 'update' if action == "update" else ('add' if action == "add" else 'delete')
            self.product_tree.insert("", "end", values=values, tags=(tag,))
        
        self.product_tree.tag_configure('add', background='#D4EDDA')
        self.product_tree.tag_configure('update', background='#FFF3CD')
        self.product_tree.tag_configure('delete', background='#F8D7DA')
        
        self.log("ç¾å›¢åŒæ­¥ç»“æœå·²æ›´æ–°åˆ°ç•Œé¢ï¼Œè¯·æ£€æŸ¥åæ‰§è¡Œ'ä¸€é”®æ‰¹é‡æ“ä½œ'")
        self.update_btn.config(state="normal")

    def start_text_analysis(self):
        if not self.douyin_products:
            if not messagebox.askyesno("ç¡®è®¤æ“ä½œ", "å½“å‰é—¨åº—æ²¡æœ‰çº¿ä¸Šå•†å“æˆ–æœªæŸ¥è¯¢ã€‚\n\nè¿™ä¼šå¯¼è‡´AIæ— æ³•è¿›è¡Œ'ä¿®æ”¹'æˆ–'ä¸‹æ¶'çš„åˆ¤æ–­ã€‚\n\næ˜¯å¦ç»§ç»­ï¼Œåªæ‰§è¡Œçº¯'æ–°å¢'æ“ä½œï¼Ÿ"):
                return
        
        text_to_analyze = self.analysis_text.get("1.0", tk.END).strip()
        if not text_to_analyze:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ–‡æœ¬å†…å®¹ã€‚")
            return
            
        if not messagebox.askyesno("ç¡®è®¤æ“ä½œ", "è¿™å°†ä½¿ç”¨LLMåˆ†ææ–‡æœ¬å¹¶è¦†ç›–å½“å‰è¡¨æ ¼å†…å®¹ï¼Œç¡®å®šè¦ç»§ç»­å—ï¼Ÿ"):
            return

        self.set_ui_state(True)
        threading.Thread(target=self._text_analysis_thread, args=(text_to_analyze,), daemon=True).start()

    def _text_analysis_thread(self, text_to_analyze):
        self.log("--- å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬æ™ºèƒ½åˆ†æ ---")
        
        # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ç®€ç•¥å•†å“åˆ—è¡¨ï¼Œæ— éœ€é‡æ–°è·å–è¯¦æƒ…
        simple_product_list = self.douyin_products
        self.log(f"å·²åŠ è½½ {len(simple_product_list)} ä¸ªçº¿ä¸Šå•†å“ç”¨äºåˆ†æã€‚")

        analysis_result = analyze_text_for_actions(text_to_analyze, simple_product_list, self.log, self.llm_cache)
        
        if analysis_result:
            self.master.after(0, lambda: self.populate_tree_from_analysis(analysis_result))
        else:
            self.log("[Error] æ–‡æœ¬åˆ†æå¤±è´¥ï¼Œæœªèƒ½è·å–æœ‰æ•ˆç»“æœã€‚")
            
        self.master.after(0, lambda: self.set_ui_state(False))

    def populate_tree_from_analysis(self, analysis_result):
        self.log("--- æ­£åœ¨æ ¹æ®æ–‡æœ¬åˆ†æç»“æœæ›´æ–°åˆ—è¡¨ ---")
        self.product_tree.delete(*self.product_tree.get_children())
        
        douyin_products_map = {p['name']: p for p in self.douyin_products}
        self.excel_data = []
        processed_douyin_products = set()

        # columns = ("douyin_name", "douyin_price", "douyin_origin_price", "match_status", "action_mode", "excel_title", "excel_price", "excel_origin_price", "excel_area", "excel_limit", "excel_validity", "id")
        for new_data in analysis_result.get('add', []):
            self.excel_data.append(new_data)
            values = (
                "<å¾…åˆ›å»º>", "", "",
                "åŒ¹é…æˆåŠŸ", "é‡åˆ›", "",
                new_data.get('å›¢è´­æ ‡é¢˜'), new_data.get('å”®ä»·'), new_data.get('åŸä»·'),
                new_data.get('commodity_type'), new_data.get('applicable_location', 'å¤§å…'), new_data.get('å¯ç”¨åŒºåŸŸ'), new_data.get('é™è´­'), new_data.get('æœ‰æ•ˆæœŸ'), ""
            )
            self.product_tree.insert("", "end", values=values, tags=('add',))

        for update_item in analysis_result.get('update', []):
            from_name = update_item.get('from_name')
            new_data = update_item.get('new_data')
            if from_name in douyin_products_map and new_data:
                product = douyin_products_map[from_name]
                self.excel_data.append(new_data)
                values = (
                    product['name'], product['price'], product.get('origin_price', '0.00'),
                    "åŒ¹é…æˆåŠŸ", "ä¿®æ”¹", "",
                    new_data.get('å›¢è´­æ ‡é¢˜'), new_data.get('å”®ä»·'), new_data.get('åŸä»·'),
                    new_data.get('commodity_type', ''), new_data.get('applicable_location', ''),
                    new_data.get('å¯ç”¨åŒºåŸŸ'), new_data.get('é™è´­'), new_data.get('æœ‰æ•ˆæœŸ'), product['id']
                )
                self.product_tree.insert("", "end", values=values, tags=('update',))
                processed_douyin_products.add(from_name)

        for delete_item in analysis_result.get('delete', []):
            name = delete_item.get('name')
            if name in douyin_products_map:
                product = douyin_products_map[name]
                new_data = {"å›¢è´­æ ‡é¢˜": f"ä¸‹æ¶-{name}"}
                self.excel_data.append(new_data)
                values = (
                    product['name'], product['price'], product.get('origin_price', '0.00'),
                    "åŒ¹é…æˆåŠŸ", "ä¸‹æ¶", "",
                    f"ä¸‹æ¶-{name}", "", "", "", "", "", product['id']
                )
                self.product_tree.insert("", "end", values=values, tags=('delete',))
                processed_douyin_products.add(name)

        for name, product in douyin_products_map.items():
            if name not in processed_douyin_products:
                values = (
                    product['name'], product['price'], product.get('origin_price', '0.00'),
                    "æ— æ“ä½œ", "-", "", "", "", "", "", "", "", product['id']
                )
                self.product_tree.insert("", "end", values=values, tags=('keep',))
        
        self.product_tree.tag_configure('add', background='#D4EDDA')
        self.product_tree.tag_configure('update', background='#FFF3CD')
        self.product_tree.tag_configure('delete', background='#F8D7DA')

        self.log("åˆ—è¡¨å·²æ ¹æ®åˆ†æç»“æœæ›´æ–°ã€‚è¯·æ£€æŸ¥å¹¶æ‰§è¡Œâ€œä¸€é”®æ‰¹é‡æ“ä½œâ€ã€‚")
        self.update_btn.config(state="normal")

    def start_batch_update(self):
        if hasattr(self, 'edit_entry') and self.edit_entry:
            self.edit_entry.destroy()
            self.edit_entry = None
            
        items_to_process = []
        for item_id in self.product_tree.get_children():
            action_mode = self.product_tree.set(item_id, "action_mode")
            if action_mode == "-" or action_mode == "æ— æ“ä½œ":
                continue

            try:
                # ä»Treeviewä¸­ç›´æ¥è¯»å–æœ€ç»ˆç¡®è®¤çš„æ•°æ®
                excel_title_from_tree = self.product_tree.set(item_id, "excel_title")
                full_data_from_excel = next((d for d in self.excel_data if d.get("å›¢è´­æ ‡é¢˜") == excel_title_from_tree), {})

                new_data = {
                    "å›¢è´­æ ‡é¢˜": excel_title_from_tree,
                    "å”®ä»·": float(self.product_tree.set(item_id, "excel_price") or 0),
                    "åŸä»·": float(self.product_tree.set(item_id, "excel_origin_price") or 0),
                    "å¯ç”¨åŒºåŸŸ": self.product_tree.set(item_id, "excel_area"),
                    "é™è´­": self.product_tree.set(item_id, "excel_limit"),
                    "æœ‰æ•ˆæœŸ": self.product_tree.set(item_id, "excel_validity"),
                    "å›¢å•å¤‡æ³¨": "", # å¤‡æ³¨å­—æ®µç›®å‰ä¸åœ¨è¡¨æ ¼ä¸­ï¼Œé»˜è®¤ä¸ºç©º
                    "matched_image": self.product_tree.set(item_id, "matched_image"),
                    "member_type": full_data_from_excel.get("member_type"),
                    "commodity_type": self.product_tree.set(item_id, "commodity_type"),
                    "applicable_location": self.product_tree.set(item_id, "applicable_location")
                }
                
                # å¯¹â€œä¸‹æ¶â€æ“ä½œè¿›è¡Œç‰¹æ®Šå¤„ç†
                if action_mode == "ä¸‹æ¶":
                    original_name = self.product_tree.set(item_id, "douyin_name")
                    new_data["å›¢è´­æ ‡é¢˜"] = f"ä¸‹æ¶-{original_name}"

                item_to_add = {
                    "product_id": self.product_tree.set(item_id, "id"),
                    "new_data": new_data,
                    "action_mode": action_mode
                }
                items_to_process.append(item_to_add)
            except ValueError:
                messagebox.showerror("æ•°æ®é”™è¯¯", f"å•†å“ '{self.product_tree.set(item_id, 'excel_title')}' çš„ä»·æ ¼æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿ä¸ºæ•°å­—ã€‚")
                return
            except Exception as e:
                messagebox.showerror("æœªçŸ¥é”™è¯¯", f"å¤„ç†è¡Œæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return

        if not items_to_process: return messagebox.showinfo("æç¤º", "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦æ“ä½œçš„å•†å“ï¼ˆæ“ä½œæ¨¡å¼ä¸ä¸º'-'æˆ–'æ— æ“ä½œ'ï¼‰ã€‚")
        if not messagebox.askyesno("ç¡®è®¤æ“ä½œ", f"å³å°†å¤„ç† {len(items_to_process)} ä¸ªå•†å“ã€‚æ­¤æ“ä½œä¸å¯é€†ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ"): return
        self.set_ui_state(True)
        threading.Thread(target=self._batch_process_thread, args=(items_to_process,), daemon=True).start()

    def start_auto_match_images(self):
        if not self.image_dir:
            messagebox.showerror("é”™è¯¯", "è¯·å…ˆé€‰æ‹©ä¸€ä¸ªå›¾ç‰‡æ–‡ä»¶å¤¹ã€‚")
            return
        
        add_items = []
        for item_id in self.product_tree.get_children():
            if self.product_tree.set(item_id, "action_mode") == "é‡åˆ›":
                add_items.append(self.product_tree.set(item_id, "excel_title"))

        if not add_items:
            messagebox.showinfo("æç¤º", "è¡¨æ ¼ä¸­æ²¡æœ‰æ‰¾åˆ°éœ€è¦'é‡åˆ›'çš„æ–°å¢å¥—é¤ã€‚")
            return

        self.set_ui_state(True)
        threading.Thread(target=self._auto_match_images_thread, args=(add_items,), daemon=True).start()

    def _auto_match_images_thread(self, add_items):
        self.log("--- å¼€å§‹æ™ºèƒ½åŒ¹é…å¥—é¤å’Œå›¾ç‰‡ ---")
        if not llm_client:
            self.log("[Error] LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åŒ¹é…ã€‚")
            self.master.after(0, lambda: self.set_ui_state(False))
            return
        
        # 1. AIåˆ†æå›¾ç‰‡æ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡
        self.log("æ­¥éª¤ 1/2: æ­£åœ¨ä½¿ç”¨AIåˆ†æå›¾ç‰‡å†…å®¹...")
        image_summaries = []
        supported_formats = (".jpg", ".jpeg", ".png", ".bmp")
        model_index = 0
        try:
            image_files = [f for f in os.listdir(self.image_dir) if f.lower().endswith(supported_formats)]
            for filename in image_files:
                try:
                    full_path = os.path.join(self.image_dir, filename)
                    with open(full_path, "rb") as image_file:
                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    
                    image_url = f"data:image/jpeg;base64,{base64_image}"
                    
                    selected_vision_model = VISION_MODEL_IDS[model_index % len(VISION_MODEL_IDS)]
                    self.log(f"ä½¿ç”¨è§†è§‰æ¨¡å‹: {selected_vision_model}")
                    model_index += 1
                    
                    response = llm_client.chat.completions.create(
                        model=selected_vision_model,
                        messages=[{
                            'role': 'user',
                            'content': [{
                                'type': 'text',
                                'text': 'æ ¹æ®å›¾ç‰‡å†…å®¹ï¼Œä¸ºå…¶ç”Ÿæˆä¸€ä¸ªç®€çŸ­ä¸”æè¿°æ€§çš„ä¸­æ–‡åï¼ˆä¸è¦å¸¦æ‰©å±•åï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå›¾ç‰‡æ˜¯å…³äº500å…ƒç½‘è´¹å¥—é¤ï¼Œå°±è¿”å›"500å…ƒç½‘è´¹"ã€‚',
                            }, {
                                'type': 'image_url',
                                'image_url': { 'url': image_url },
                            }],
                        }]
                    )
                    summary = response.choices[0].message.content.strip()
                    self.log(f"--- [Vision LLM Raw Response for Image Summary] ---\n{summary}\n" + "-"*30)
                    image_summaries.append({"original_filename": filename, "summary": summary})
                    self.log(f"åˆ†æå›¾ç‰‡ '{filename}' -> AIæ‘˜è¦: '{summary}'")
                except Exception as e:
                    self.log(f"[Error] åˆ†æå›¾ç‰‡ {filename} æ—¶å‡ºé”™: {e}")
        except Exception as e:
            self.log(f"[Error] éå†å›¾ç‰‡æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
            self.master.after(0, lambda: self.set_ui_state(False))
            return

        if not image_summaries:
            self.log("[Error] æœªèƒ½æˆåŠŸåˆ†æä»»ä½•å›¾ç‰‡ã€‚")
            self.master.after(0, lambda: self.set_ui_state(False))
            return

        # 2. AIåŒ¹é…æ–‡æœ¬
        self.log("æ­¥éª¤ 2/2: æ­£åœ¨ä½¿ç”¨AIåŒ¹é…å›¾ç‰‡æ‘˜è¦å’Œå¥—é¤æ ‡é¢˜...")
        image_summary_list = [item['summary'] for item in image_summaries]
        
        multi_mode = self.multi_match_mode_var.get()
        if multi_mode:
            self.log("--- å½“å‰ä¸º [å›¾ç‰‡åŒ¹é…å¤šå¥—é¤] æ¨¡å¼ ---")
            prompt = f"""
            ç°æœ‰ä»¥ä¸‹éœ€è¦åˆ›å»ºçš„å¥—é¤åˆ—è¡¨ï¼š
            {json.dumps(add_items, ensure_ascii=False)}

            ä»¥åŠä»¥ä¸‹ä»å›¾ç‰‡ä¸­åˆ†æå‡ºçš„æ‘˜è¦åˆ—è¡¨ï¼š
            {json.dumps(image_summary_list, ensure_ascii=False)}

            ä»»åŠ¡ï¼šå¯¹äºæ¯ä¸€ä¸ªâ€œå›¾ç‰‡æ‘˜è¦â€ï¼Œåˆ¤æ–­å®ƒå¯ä»¥åŒ¹é…åˆ°å“ªäº›â€œå¥—é¤åˆ—è¡¨â€ä¸­çš„é¡¹ç›®ã€‚åŒ¹é…åº”è¯¥æ˜¯åŸºäºæ ¸å¿ƒå…³é”®è¯çš„åŒ…å«å…³ç³»ã€‚
            ä¾‹å¦‚ï¼Œæ‘˜è¦â€œç½‘è´¹â€å¯ä»¥åŒ¹é…æ‰€æœ‰æ ‡é¢˜ä¸­åŒ…å«â€œç½‘è´¹â€çš„å¥—é¤ã€‚
            
            è¿”å›ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼Œå…¶ä¸­é”®æ˜¯å›¾ç‰‡æ‘˜è¦ï¼Œå€¼æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰åŒ¹é…çš„å¥—é¤æ ‡é¢˜çš„**åˆ—è¡¨**ã€‚å¦‚æœä¸€ä¸ªæ‘˜è¦æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…é¡¹ï¼Œå€¼åº”ä¸ºç©ºåˆ—è¡¨ `[]`ã€‚
            ä¾‹å¦‚: {{ "é€šç”¨ç½‘è´¹å›¾": ["ã€æ–°å®¢ã€‘50å…ƒç½‘è´¹", "ã€è€å®¢ã€‘100å…ƒç½‘è´¹"], "åŒ…æ—¶å¥—é¤å›¾": [] }}
            """
        else:
            self.log("--- å½“å‰ä¸º [ä¸€å¯¹ä¸€ç²¾å‡†åŒ¹é…] æ¨¡å¼ ---")
            prompt = f"""
            ç°æœ‰ä»¥ä¸‹éœ€è¦åˆ›å»ºçš„å¥—é¤åˆ—è¡¨ï¼š
            {json.dumps(add_items, ensure_ascii=False)}

            ä»¥åŠä»¥ä¸‹ä»å›¾ç‰‡ä¸­åˆ†æå‡ºçš„æ‘˜è¦åˆ—è¡¨ï¼š
            {json.dumps(image_summary_list, ensure_ascii=False)}

            è¯·ä¸ºæ¯ä¸ªâ€œå¥—é¤åˆ—è¡¨â€ä¸­çš„é¡¹ç›®ï¼Œåœ¨â€œå›¾ç‰‡æ‘˜è¦åˆ—è¡¨â€ä¸­æ‰¾åˆ°**æœ€åŒ¹é…**çš„ä¸€é¡¹ã€‚
            è¿”å›ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼Œå…¶ä¸­é”®æ˜¯å¥—é¤æ ‡é¢˜ï¼Œå€¼æ˜¯åŒ¹é…ä¸Šçš„å›¾ç‰‡æ‘˜è¦ã€‚å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œè¯·å°†å€¼è®¾ä¸º nullã€‚
            ä¾‹å¦‚: {{ "ã€æ–°å®¢ã€‘50å…ƒç½‘è´¹": "50å…ƒç½‘è´¹", "ã€ä¸“äº«ã€‘300å…ƒåŒ…æ—¶": null }}
            """

        try:
            response = llm_client.chat.completions.create(
                model=LLM_MODEL_ID, # ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
                messages=[
                    {'role': 'system', 'content': 'You are a helpful assistant that only returns JSON.'},
                    {'role': 'user', 'content': prompt}
                ]
            )
            
            match_result_str = response.choices[0].message.content
            log_func(f"--- [LLM Raw Response for Image-Text Matching] ---\n{match_result_str}\n" + "-"*30)
            json_match = re.search(r'\{[\s\S]*\}', match_result_str)
            if json_match:
                cleaned_response = json_match.group(0)
                match_result = json.loads(cleaned_response)
            else:
                raise json.JSONDecodeError("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONå¯¹è±¡", match_result_str, 0)

            self.log("æ™ºèƒ½åŒ¹é…APIè°ƒç”¨æˆåŠŸï¼Œæ­£åœ¨æ›´æ–°UI...")

            summary_to_filename = {item['summary']: item['original_filename'] for item in image_summaries}

            # åœ¨UIçº¿ç¨‹ä¸­æ›´æ–°Treeview
            def _update_ui():
                title_to_image_map = {}
                if multi_mode:
                    self.log("[Debug] è¿›å…¥å¤šå¯¹å¤šåŒ¹é…UIæ›´æ–°é€»è¾‘ã€‚")
                    # "ä¸€å¯¹å¤š"é€»è¾‘: åè½¬å­—å…¸
                    for summary, titles in match_result.items():
                        if summary in summary_to_filename:
                            filename = summary_to_filename[summary]
                            for title in titles:
                                title_to_image_map[title] = filename
                else:
                    self.log("[Debug] è¿›å…¥ä¸€å¯¹ä¸€åŒ¹é…UIæ›´æ–°é€»è¾‘ã€‚")
                    # "ä¸€å¯¹ä¸€"é€»è¾‘
                    for title, summary in match_result.items():
                        if summary in summary_to_filename:
                            title_to_image_map[title] = summary_to_filename[summary]
                
                self.log(f"[Debug] æœ€ç»ˆæ„å»ºçš„ 'å¥—é¤æ ‡é¢˜ -> å›¾ç‰‡' æ˜ å°„: {json.dumps(title_to_image_map, ensure_ascii=False, indent=2)}")

                for item_id in self.product_tree.get_children():
                    if self.product_tree.set(item_id, "action_mode") == "é‡åˆ›":
                        title = self.product_tree.set(item_id, "excel_title")
                        if title in title_to_image_map:
                            filename = title_to_image_map[title]
                            self.product_tree.set(item_id, "matched_image", filename)
                            self.log(f"UIæ›´æ–°: å¥—é¤ '{title}' -> å›¾ç‰‡ '{filename}'")
                        else:
                            self.log(f"[Debug] å¥—é¤ '{title}' åœ¨æ˜ å°„ä¸­æœªæ‰¾åˆ°åŒ¹é…å›¾ç‰‡ã€‚")
                
                self.log("--- æ™ºèƒ½åŒ¹é…å®Œæˆ ---")
                self.set_ui_state(False)

            self.master.after(0, _update_ui)

        except json.JSONDecodeError:
            self.log(f"[Error] è§£æLLMè¿”å›çš„JSONå¤±è´¥ã€‚è¿”å›å†…å®¹: {match_result_str}")
            self.master.after(0, lambda: messagebox.showerror("AIåŒ¹é…é”™è¯¯", "AIæœåŠ¡è¿”å›äº†æ— æ•ˆçš„æ•°æ®æ ¼å¼ã€‚"))
            self.master.after(0, lambda: self.set_ui_state(False))
        except Exception as e:
            self.log(f"[Error] æ™ºèƒ½åŒ¹é…è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self.master.after(0, lambda: messagebox.showerror("AIåŒ¹é…é”™è¯¯", f"è¯·æ±‚AIæœåŠ¡æ—¶å‡ºé”™: {e}"))
            self.master.after(0, lambda: self.set_ui_state(False))

    def _batch_process_thread(self, items_to_process):
        success_count, failed_items = 0, []
        items_to_process.sort(key=lambda x: 1 if x["action_mode"] != "ä¿®æ”¹" else 0)
        
        # ä¸º"é‡åˆ›"æ¨¡å¼å‡†å¤‡æ¨¡æ¿IDï¼š
        # 1. ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ª"ä¿®æ”¹"æ“ä½œçš„å•†å“ID
        # 2. å¦åˆ™ä½¿ç”¨å½“å‰é—¨åº—å•†å“åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªï¼ˆself.douyin_productsæ˜¯å½“å‰é—¨åº—çš„å¥—é¤åˆ—è¡¨ï¼‰
        template_id_for_recreate = next(
            (item['product_id'] for item in items_to_process if item['action_mode'] == "ä¿®æ”¹"), 
            self.douyin_products[0]['id'] if self.douyin_products else None
        )
        
        if not template_id_for_recreate and any(item['action_mode'] == "é‡åˆ›" for item in items_to_process):
            self.log("[Error] é‡åˆ›æ¨¡å¼éœ€è¦æ¨¡æ¿å•†å“ï¼Œä½†å½“å‰é—¨åº—æ²¡æœ‰å¯ç”¨çš„å•†å“ã€‚")
            self.master.after(0, lambda: messagebox.showerror("é”™è¯¯", "é‡åˆ›æ¨¡å¼éœ€è¦æ¨¡æ¿å•†å“ï¼Œä½†å½“å‰é—¨åº—æ²¡æœ‰å¯ç”¨çš„å•†å“ã€‚\nè¯·å…ˆæŸ¥è¯¢é—¨åº—å•†å“åˆ—è¡¨ã€‚"))
            self.master.after(0, lambda: self.set_ui_state(False))
            return

        # è®°å½•æ¨¡æ¿å•†å“ä¿¡æ¯ï¼ˆç”¨äºé‡åˆ›æ¨¡å¼ï¼‰
        if template_id_for_recreate and any(item['action_mode'] == "é‡åˆ›" for item in items_to_process):
            template_product = next((p for p in self.douyin_products if p['id'] == template_id_for_recreate), None)
            if template_product:
                self.log(f"--- é‡åˆ›æ¨¡å¼å°†ä½¿ç”¨æ¨¡æ¿å•†å“: {template_product['name']} (ID: {template_id_for_recreate}) ---")
            else:
                self.log(f"--- é‡åˆ›æ¨¡å¼å°†ä½¿ç”¨æ¨¡æ¿å•†å“ID: {template_id_for_recreate} ---")

        for item in items_to_process:
            mode = item["action_mode"]
            product_id = item["product_id"]
            if mode == "ä¸‹æ¶":
                success, reason = operate_douyin_product(self.douyin_access_token, product_id, self.log, offline=True)
            elif mode == "ä¿®æ”¹":
                # ä¿®æ”¹æ¨¡å¼ï¼šä½¿ç”¨å½“å‰å•†å“è‡ªå·±çš„IDä½œä¸ºæ¨¡æ¿
                success, reason = update_douyin_product(self.douyin_access_token, product_id, item["new_data"], self.log, mode, image_dir=self.image_dir, target_poi_id=self.current_poi_id)
            else:  # é‡åˆ›æ¨¡å¼ - ä½¿ç”¨ç½‘é¡µç«¯API
                # ä½¿ç”¨ç½‘é¡µç«¯APIåˆ›å»ºå•†å“ï¼ˆå¤ç”¨æ¨¡æ¿å›¾ç‰‡ï¼Œåˆ›å»ºåè‡ªåŠ¨ä¿®æ”¹POI IDï¼‰
                # æ¨¡æ¿æ¥æºï¼šå½“å‰é—¨åº—çš„å•†å“åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ª
                product_id_created, reason = create_product_via_web(
                    DOUYIN_WEB_COOKIE,
                    DOUYIN_WEB_CSRF_TOKEN,
                    DOUYIN_ROOT_LIFE_ACCOUNT_ID,
                    template_id_for_recreate,  # æ¨¡æ¿å•†å“IDï¼ˆæ¥è‡ªå½“å‰é—¨åº—ï¼‰
                    item["new_data"],
                    self.current_poi_id,  # ç›®æ ‡é—¨åº—POI ID
                    self.douyin_access_token,  # ç”¨äºåç»­ä¿®æ”¹POI ID
                    self.log
                )
                success = product_id_created is not None
            
            if success: success_count += 1
            else: failed_items.append(f"ID {product_id}: {reason}")
            time.sleep(1)

        summary_message = f"æ‰¹é‡æ“ä½œå®Œæˆï¼\n\næˆåŠŸ: {success_count} ä¸ª\nå¤±è´¥: {len(failed_items)} ä¸ª"
        if failed_items:
            self.log("--- æ“ä½œå¤±è´¥è¯¦æƒ… ---"); [self.log(f) for f in failed_items]
            summary_message += "\n\nå¤±è´¥è¯¦æƒ…è¯·æŸ¥çœ‹æ—¥å¿—ã€‚"
        self.master.after(0, lambda: messagebox.showinfo("æ“ä½œå®Œæˆ", summary_message))
        self.master.after(0, self.start_query_douyin)

if __name__ == "__main__":
    root = tk.Tk()
    app = App(master=root)
    root.mainloop()
